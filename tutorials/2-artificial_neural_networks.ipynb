{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_7PSrzbdQbT"
   },
   "source": [
    "# Tutorial 2 - Artificial Neural Networks \n",
    "\n",
    "## Before you start, you'll need to know:\n",
    "* Artificial Neurons Networks (ANNs) \n",
    "* Forward Pass\n",
    "* Activation Functions\n",
    "* Mean Squared Error and Cross-Entropy\n",
    "* Gradient Descent and Backpropagation\n",
    "* Nonlinearity\n",
    "\n",
    "\n",
    "**If you want a headstart on these topics before we go over them in meeting, here are some videos you can consult that I enjoyed when I was learning:**\n",
    "* Intro to Artificial Neural Networks: https://www.youtube.com/watch?v=aircAruvnKk\n",
    "* Gradient Descent: https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "* Back propagation: https://www.youtube.com/watch?v=Ilg3gGewQ5U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiwXNn3SNZfu"
   },
   "source": [
    "### What is PyTorch?\n",
    "\n",
    "PyTorch is a scientific computing package that builds on the NumPy library to allow for the use of GPUs. It incorporates deep learning capabilities while maximizing flexibility and speed.\n",
    "\n",
    "### PyTorch Basics\n",
    "\n",
    "To use PyTorch you must first import the library\n",
    "\n",
    "**Like in tutorial 1, if this throws an error, you'll have to install the library using \"pip install torch\" in your terminal**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TU7EUSNcVN9x"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bTAo6LzVJwg"
   },
   "source": [
    "#### Tensors\n",
    "Tensors are n-dimensional arrays that allow that can be used with a GPU to accelerate computing. There are several ways to work with Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MuUNQZ3UksM"
   },
   "outputs": [],
   "source": [
    "# initialize a random tensor\n",
    "x = torch.rand(4, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmcpRB48VVFe"
   },
   "outputs": [],
   "source": [
    "# initialize a tensor loaded with zeros\n",
    "x = torch.zeros(4, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LI84uJM-VjyL"
   },
   "outputs": [],
   "source": [
    "# initialized with data entered manually\n",
    "x = torch.tensor([2.1, 4.0, -5.2])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "421on8nnWE3e"
   },
   "outputs": [],
   "source": [
    "# initialized with data from numPy\n",
    "import numpy as np\n",
    "data = np.array([2.1, 4.0, -5.2])\n",
    "print(data)\n",
    "x = torch.tensor(data)\n",
    "print(x)\n",
    "\n",
    "# note you can easily convert fron tensor to numpy\n",
    "x_np = x.numpy()\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_I8gINJWy1g"
   },
   "source": [
    "### Tensor size and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUCzrV1nW5XA"
   },
   "outputs": [],
   "source": [
    "# obtain size of tensor data structure\n",
    "x = torch.zeros(4, 3)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdPIVeeEXFO5"
   },
   "source": [
    "### Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5AltppKXUZv"
   },
   "outputs": [],
   "source": [
    "# tensor addition\n",
    "x = torch.rand(4, 3)\n",
    "y = torch.ones(4, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UsB6YlwX_p2"
   },
   "outputs": [],
   "source": [
    "# tensor multiplication\n",
    "x = torch.rand(4, 3)\n",
    "y = torch.ones(4, 3)\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8W5LZlbXnMu"
   },
   "outputs": [],
   "source": [
    "# Provide output tensor as argument\n",
    "result = torch.ones(4,3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxDlMlhnYWAN"
   },
   "outputs": [],
   "source": [
    "# resize and reshape tensors\n",
    "x = torch.randn(4, 3)\n",
    "y = x.view(12)\n",
    "z = x.view(-1, 4)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBcAFWNSZnPE"
   },
   "outputs": [],
   "source": [
    "# convert one element tensor to a Python number\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhBQXibrZy0Z"
   },
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "The PyTorch autograd package allows for easy computation of derivative. This is handled automatically using a define-by-run framework, which works as you write your code. \n",
    "\n",
    "To enable this feature you need to set the Tensor attribute .required_grat to True, at which point it begins to track all operations performed on it. After your computation have been completed you can call .backward() and all the gradients will be computed for you. The gradient for each tensor will be stored in the tensor attribute .grad.\n",
    "\n",
    "Each tensor also has a .grad_fn attribute which references Function that has created it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKqBTFK2cRZW"
   },
   "outputs": [],
   "source": [
    "# Example computation of gradients\n",
    "x = torch.rand(4, 3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#perform some operations\n",
    "y = x + 10\n",
    "z = y*y\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JDr3PgVdGMp"
   },
   "outputs": [],
   "source": [
    "# compute the gradient with respect to the output\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ntVfJhxUjtN"
   },
   "source": [
    "## Artificial Neural Networks in PyTorch\n",
    "In this example we will train an \"artificial pigeon\" to perform a digit recognition\n",
    "task. That is, we will use the MNIST dataset of hand-written digits, and train\n",
    "the pigeon to **recognize a small digit, namely a digit that is less than 3**.\n",
    "This problem is a **binary classification problem** we want to predict\n",
    "which of two classes an input image is a part of.\n",
    "\n",
    "### 1) Load MNIST Data\n",
    "The MNIST dataset contains hand-written digits that are 28x28=784 pixels large.\n",
    "Here are a few digits in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOu8rvDVMpSq"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# load the data\n",
    "mnist_train = datasets.MNIST('data', train=True, download=True)\n",
    "mnist_train = list(mnist_train)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3xCgqcwMvCl"
   },
   "outputs": [],
   "source": [
    "# plot the first 18 images in the training data\n",
    "for k, (image, label) in enumerate(mnist_train[:18]):\n",
    "    plt.subplot(3, 6, k+1)\n",
    "    plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-DcwbkZ3Nlx"
   },
   "source": [
    "### 2) Defining the ANN Forward Pass\n",
    "Here is an implementation of the artificial pigeon brain in PyTorch.\n",
    "Don't worry if this code or the explanations don't make sense yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-ueh5jh3I5p"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1) # set the random seed\n",
    "\n",
    "class Pigeon(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pigeon, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 30) # 784 = 28x28 every pixel is a feature\n",
    "        \n",
    "        self.layer2 = nn.Linear(30, 1)\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(-1, 28 * 28)\n",
    "        activation1 = self.layer1(flattened)\n",
    "        activation1 = F.relu(activation1)\n",
    "        activation2 = self.layer2(activation1)\n",
    "        return activation2\n",
    "\n",
    "pigeon = Pigeon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi9lSSOtKhEE"
   },
   "source": [
    "In this network, there are 28x28 = 784 input neurons, to work with our 28x28 pixel images. We have a single output neuron and a hidden layer of 30 neurons.\n",
    "\n",
    "The variable `pigeon.layer1` contains information about the connectivity\n",
    "between the input layer and the hidden layer (stored as a matrix), and the\n",
    "biases (stored as a vector).\n",
    "\n",
    "Similarly, the variable `pigeon.layer2` contains information about the weights\n",
    "between the hidden layer and the output layer, and the bias.\n",
    "\n",
    "The weights and biases adjust during training, so they are called the model's\n",
    "**parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82KQZkDlLx7j"
   },
   "outputs": [],
   "source": [
    "# view parameters\n",
    "for w in pigeon.layer1.parameters():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86ubAvO03eMp"
   },
   "source": [
    "### 3) Test Network Forward Pass\n",
    "Here is an example of using the network to classify whether the\n",
    "image contains a small digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtiBP83X247A"
   },
   "outputs": [],
   "source": [
    "# make predictions for the first 10 images in mnist_train\n",
    "img_to_tensor = transforms.ToTensor() #transform the image data into a 28x28 matrix of numbers\n",
    "\n",
    "for k, (image, label) in enumerate(mnist_train[:10]):\n",
    "    inval = img_to_tensor(image)\n",
    "    outval = pigeon(inval)       # find the output activation given input\n",
    "    prob = torch.sigmoid(outval) # turn the activation into a probability\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHkHYl0aO7ot"
   },
   "source": [
    "Since we haven't trained the network\n",
    "yet, the predicted probability of images containing a small digit \n",
    "is close to half. The \"pigeon\" is unsure.\n",
    "\n",
    "In order for the network to be useful, we need to actually train it, so\n",
    "that the weights are actually meaningful, non-random values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiUzssKS3ifv"
   },
   "source": [
    "### 4) Update Parameters using Gradient Descent with Cross-Entropy\n",
    "To update the parameters (weights) we will first use the network to make predictions, then compare the predictions\n",
    "agains the ground truth. To compare the predictions against actual values we'll compute a classification error using the Cross-Entropy equation.\n",
    "\n",
    "The classification error, that is how good or bad the prediction was compared to the actual values is more commonly referred to as the **loss** and Cross-Entropy is the **loss function**. The introduction of a loss function makes our problem a **optimization** problem: what set of parameters minimizes the loss across the training examples?\n",
    "\n",
    "Turning a learning problem into an optimization problem\n",
    "is actually a very subtle but important step in many machine learning tools,\n",
    "because it allows us to use tools from mathematical optimization.\n",
    "\n",
    "That there are **optimizers** that can tune the network parameters for\n",
    "us is also really, really cool. The gradient descent algorithm we derived in the last lecture is one example of an optimizer.\n",
    "\n",
    "For now, we will choose a standard loss function for a binary classification\n",
    "problem: the **binary cross-entropy loss**. We'll also choose\n",
    "a **stochastic gradient descent** optimizer. We'll talk about\n",
    "what these mean later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZhbr5Bw270k"
   },
   "outputs": [],
   "source": [
    "# simplified training code to train `pigeon` on the \"small digit recognition\" task\n",
    "import torch.optim as optim\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(pigeon.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtHoiE6AYadE"
   },
   "source": [
    "Now, we can start to train the pigeon network, similar to the way we would train\n",
    "a real pigeon:\n",
    "\n",
    "1. We'll show the network pictures of digits, one by one\n",
    "2. We'll see what the network predicts\n",
    "3. We'll check the loss function for that example digit, comparing the network prediction against the ground truth\n",
    "4. We'll make a small update to the parameters to try and improve the loss for that digit\n",
    "5. We'll continue doing this many times -- let's say 1000 times\n",
    "\n",
    "For simplicity, we'll use 1000 images, and show the network each image only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHmT89U0RM3C"
   },
   "outputs": [],
   "source": [
    "for (image, label) in mnist_train[:1000]:\n",
    "    # actual ground truth: is the digit less than 3?\n",
    "    actual = torch.tensor(label < 3).reshape([1,1]).type(torch.FloatTensor)\n",
    "    # pigeon prediction\n",
    "    out = pigeon(img_to_tensor(image)) # step 1-2\n",
    "    # update the parameters based on the loss\n",
    "    loss = criterion(out, actual)      # step 3\n",
    "    loss.backward()                    # step 4 (compute the updates for each parameter)\n",
    "    optimizer.step()                   # step 4 (make the updates for each parameter)\n",
    "    optimizer.zero_grad()              # a clean up step for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfzAfZiuRQWx"
   },
   "source": [
    "It is very common to run into errors with changing different data types to tensors with the correct shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9Gieq973sDh"
   },
   "source": [
    "### 5) Test Updated Network Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-RgzimS3FEW"
   },
   "outputs": [],
   "source": [
    "# make predictions for the first 10 images in mnist_train\n",
    "for k, (image, label) in enumerate(mnist_train[:10]):\n",
    "    print(label, torch.sigmoid(pigeon(img_to_tensor(image))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otc8_sYjOk1I"
   },
   "source": [
    "Not bad! We'll use the probability 50% as the cutoff for making a \n",
    "discrete prediction. Then, we can compute the accuracy on the 1000\n",
    "images we used to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pluL_aN30CC"
   },
   "source": [
    "### 6)a) Validation of Network on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-Qq5Bgh4Hx9"
   },
   "outputs": [],
   "source": [
    "# computing the error and accuracy on the training set\n",
    "error = 0\n",
    "for (image, label) in mnist_train[:1000]:\n",
    "    prob = torch.sigmoid(pigeon(img_to_tensor(image)))\n",
    "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):\n",
    "        error += 1\n",
    "print(\"Training Error Rate:\", error/1000)\n",
    "print(\"Training Accuracy:\", 1 - error/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNozPD5XOk1L"
   },
   "source": [
    "The accuracy on those 1000 images is 96%, which is really good considering\n",
    "that we only showed the network each image only once.\n",
    "\n",
    "However, this accuracy is not representative of how well the network is doing,\n",
    "because the network was *trained* on the data. The network had a chance to\n",
    "see the actual answer, and learn from that answer. To get a better sense of\n",
    "the network's predictive accuracy, we should compute accuracy numbers on\n",
    "a **test set**: a set of images that were not seen in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VplQogR24Aue"
   },
   "source": [
    "### 6)b) Validation of Network on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blrK9sxS4IX5"
   },
   "outputs": [],
   "source": [
    "# computing the error and accuracy on a test set\n",
    "error = 0\n",
    "for (image, label) in mnist_train[1000:2000]:\n",
    "    prob = torch.sigmoid(pigeon(img_to_tensor(image)))\n",
    "    if (prob < 0.5 and label < 3) or (prob >= 0.5 and label >= 3):\n",
    "        error += 1\n",
    "print(\"Test Error Rate:\", error/1000)\n",
    "print(\"Test Accuracy:\", 1 - error/1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dd90ed194a3ae38b0eff763817ca93fe2bbd06cf5e4f855518f48eed923881c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

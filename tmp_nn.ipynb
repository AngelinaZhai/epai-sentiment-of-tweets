{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchtext\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from langdetect import detect_langs\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pandarallel import pandarallel \n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  use_cuda = True\n",
    "else:  \n",
    "  use_cuda = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading GLOVE embeddings\n",
    "GLOVE = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=10000)  # use 10k most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-c32713cabe528196\n",
      "Found cached dataset parquet (/Users/angelinazhai/.cache/huggingface/datasets/ucberkeley-dlab___parquet/ucberkeley-dlab--measuring-hate-speech-c32713cabe528196/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0a7cfb526d43c9a8c11398a11b42e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>infitms</th>\n",
       "      <th>outfitms</th>\n",
       "      <th>annotator_severity</th>\n",
       "      <th>std_err</th>\n",
       "      <th>annotator_infitms</th>\n",
       "      <th>annotator_outfitms</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>annotator_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.00000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23530.416138</td>\n",
       "      <td>5567.097812</td>\n",
       "      <td>1.281352</td>\n",
       "      <td>2.954307</td>\n",
       "      <td>2.828875</td>\n",
       "      <td>2.56331</td>\n",
       "      <td>2.278638</td>\n",
       "      <td>2.698575</td>\n",
       "      <td>1.846211</td>\n",
       "      <td>1.052045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744733</td>\n",
       "      <td>-0.567428</td>\n",
       "      <td>1.034322</td>\n",
       "      <td>1.001052</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>0.300588</td>\n",
       "      <td>1.007158</td>\n",
       "      <td>1.011841</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>37.910772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12387.194125</td>\n",
       "      <td>3230.508937</td>\n",
       "      <td>1.023542</td>\n",
       "      <td>1.231552</td>\n",
       "      <td>1.309548</td>\n",
       "      <td>1.38983</td>\n",
       "      <td>1.370876</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>1.402372</td>\n",
       "      <td>1.345706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932260</td>\n",
       "      <td>2.380003</td>\n",
       "      <td>0.496867</td>\n",
       "      <td>0.791943</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>0.236380</td>\n",
       "      <td>0.269876</td>\n",
       "      <td>0.675863</td>\n",
       "      <td>0.613006</td>\n",
       "      <td>11.641276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.340000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-1.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-1.578693</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18148.000000</td>\n",
       "      <td>2719.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.330000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-0.341008</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20052.000000</td>\n",
       "      <td>5602.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32038.250000</td>\n",
       "      <td>8363.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.449555</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50070.000000</td>\n",
       "      <td>11142.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          comment_id   annotator_id       platform      sentiment  \\\n",
       "count  135556.000000  135556.000000  135556.000000  135556.000000   \n",
       "mean    23530.416138    5567.097812       1.281352       2.954307   \n",
       "std     12387.194125    3230.508937       1.023542       1.231552   \n",
       "min         1.000000       1.000000       0.000000       0.000000   \n",
       "25%     18148.000000    2719.000000       0.000000       2.000000   \n",
       "50%     20052.000000    5602.500000       1.000000       3.000000   \n",
       "75%     32038.250000    8363.000000       2.000000       4.000000   \n",
       "max     50070.000000   11142.000000       3.000000       4.000000   \n",
       "\n",
       "             respect        insult      humiliate         status  \\\n",
       "count  135556.000000  135556.00000  135556.000000  135556.000000   \n",
       "mean        2.828875       2.56331       2.278638       2.698575   \n",
       "std         1.309548       1.38983       1.370876       0.898500   \n",
       "min         0.000000       0.00000       0.000000       0.000000   \n",
       "25%         2.000000       2.00000       1.000000       2.000000   \n",
       "50%         3.000000       3.00000       3.000000       3.000000   \n",
       "75%         4.000000       4.00000       3.000000       3.000000   \n",
       "max         4.000000       4.00000       4.000000       4.000000   \n",
       "\n",
       "          dehumanize       violence  ...     hatespeech  hate_speech_score  \\\n",
       "count  135556.000000  135556.000000  ...  135556.000000      135556.000000   \n",
       "mean        1.846211       1.052045  ...       0.744733          -0.567428   \n",
       "std         1.402372       1.345706  ...       0.932260           2.380003   \n",
       "min         0.000000       0.000000  ...       0.000000          -8.340000   \n",
       "25%         1.000000       0.000000  ...       0.000000          -2.330000   \n",
       "50%         2.000000       0.000000  ...       0.000000          -0.340000   \n",
       "75%         3.000000       2.000000  ...       2.000000           1.410000   \n",
       "max         4.000000       4.000000  ...       2.000000           6.300000   \n",
       "\n",
       "             infitms       outfitms  annotator_severity        std_err  \\\n",
       "count  135556.000000  135556.000000       135556.000000  135556.000000   \n",
       "mean        1.034322       1.001052           -0.018817       0.300588   \n",
       "std         0.496867       0.791943            0.487261       0.236380   \n",
       "min         0.100000       0.070000           -1.820000       0.020000   \n",
       "25%         0.710000       0.560000           -0.380000       0.030000   \n",
       "50%         0.960000       0.830000           -0.020000       0.340000   \n",
       "75%         1.300000       1.220000            0.350000       0.420000   \n",
       "max         5.900000       9.000000            1.360000       1.900000   \n",
       "\n",
       "       annotator_infitms  annotator_outfitms     hypothesis  annotator_age  \n",
       "count      135556.000000       135556.000000  135556.000000  135451.000000  \n",
       "mean            1.007158            1.011841       0.014589      37.910772  \n",
       "std             0.269876            0.675863       0.613006      11.641276  \n",
       "min             0.390000            0.280000      -1.578693      18.000000  \n",
       "25%             0.810000            0.670000      -0.341008      29.000000  \n",
       "50%             0.970000            0.850000       0.110405      35.000000  \n",
       "75%             1.170000            1.130000       0.449555      45.000000  \n",
       "max             2.010000            9.000000       0.987511      81.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load numpy array from file\n",
    "tmp_np_arr = np.load('hate_speech.npy', allow_pickle=True)\n",
    "\n",
    "#convert to pandas dataframe\n",
    "df.drop(df.iloc[:, 15:131], inplace=True, axis=1)\n",
    "df_tmp = df.drop([\"annotator_id\"], axis=1)\n",
    "df_norm = pd.DataFrame(tmp_np_arr, columns=df_tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spliced = df_norm.drop('comment_id', axis=1)\n",
    "df_spliced = df_spliced.drop('platform', axis=1)\n",
    "df_spliced = df_spliced.drop('sentiment', axis=1)\n",
    "df_spliced = df_spliced.drop('hatespeech', axis=1)\n",
    "df_spliced = df_spliced.drop('hate_speech_score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_spliced.iloc[:,-1:]\n",
    "labels = df_spliced.iloc[:,:-1]\n",
    "labels = labels.to_numpy()\n",
    "label_names = list(df_spliced.iloc[:,:-1].columns)\n",
    "train_size = int(0.7*len(df_spliced))\n",
    "val_size = int((len(df_spliced) - train_size)/2)\n",
    "test_size = len(df_spliced) - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tweet(tweet):\n",
    "    # separate punctuations\n",
    "    tweet = tweet.replace(\".\", \" . \") \\\n",
    "                 .replace(\",\", \" , \") \\\n",
    "                 .replace(\";\", \" ; \") \\\n",
    "                 .replace(\"?\", \" ? \")\n",
    "    return tweet.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_words(glove_vector):\n",
    "    train, valid, test = [], [], []\n",
    "    for index, row in df_spliced.iterrows():\n",
    "        try:\n",
    "            tweet = row[-1]\n",
    "            idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
    "                    for w in split_tweet(tweet)\n",
    "                    if w in glove_vector.stoi] # keep words that has an embedding\n",
    "            if not idxs: # ignore tweets without any word with an embedding\n",
    "                continue\n",
    "            idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
    "            label = np.array(row[:-1].values).astype(np.float32) \n",
    "            label = torch.tensor(label) #storing label information to tensor\n",
    "            #adding tweet to corresponding train/val/test set\n",
    "            if index < train_size:\n",
    "                train.append((idxs, label))\n",
    "            elif index < train_size+val_size:\n",
    "                valid.append((idxs, label))\n",
    "            else:\n",
    "                test.append((idxs, label))\n",
    "        except:\n",
    "            print(\"Error at index: \", index)\n",
    "            continue\n",
    "    return train, valid, test\n",
    "\n",
    "train, valid, test = get_tweet_words(GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "  (xx, yy) = zip(*batch)\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  y_lens = [len(y) for y in yy]\n",
    "\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "  return xx_pad, yy_pad\n",
    "  \n",
    "train_loader = torch.utils.data.DataLoader(train[:10], batch_size=10, shuffle=True, collate_fn=pad_collate)\n",
    "valid_loader = torch.utils.data.DataLoader(valid[:10], batch_size=10, shuffle=True, collate_fn=pad_collate)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1024)\n",
    "        self.fc6 = nn.Linear(1024, 512)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # embedded = self.dropout(self.embedding(text))\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        hidden = self.relu(self.fc(hidden))\n",
    "        hidden = self.relu(self.fc6(hidden))\n",
    "        hidden = self.relu(self.fc7(hidden))\n",
    "        hidden = self.relu(self.fc1(hidden))\n",
    "        hidden = self.relu(self.fc2(hidden))\n",
    "        hidden = self.relu(self.fc3(hidden))\n",
    "        hidden = self.relu(self.fc4(hidden))\n",
    "        return self.fc5(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training loop\n",
    "# def train(model, iterator, optimizer, criterion):\n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0\n",
    "#     model.train()\n",
    "#     for text, label in \n",
    "#         # epoch_acc += acc.item()\n",
    "#     return epoch_loss / len(iterator), 0\n",
    "\n",
    "def get_accuracy(model, training_mode, size):\n",
    "    if training_mode == True:\n",
    "        data = train\n",
    "    else:\n",
    "        data = valid\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(data[:10000], batch_size=size, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "    hamming = []\n",
    "    for text, label in loader:\n",
    "        predictions = model(text)\n",
    "        # f1_scores.append(f1_score(label, predictions, average='macro'))\n",
    "        #detatching the tensor from the graph\n",
    "        predictions = predictions.detach().numpy()\n",
    "        label = label.squeeze().detach().numpy()\n",
    "\n",
    "        # predictions[predictions >= 0.5] = 1\n",
    "        # predictions[predictions < 0.5] = 0\n",
    "\n",
    "        predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "        label = np.where(label >= 0.5, 1, 0)\n",
    "\n",
    "        # print(predictions.shape)\n",
    "        # print(label.shape)\n",
    "        \n",
    "        hamming.append(f1_score(label, predictions, average='macro'))\n",
    "\n",
    "    return np.mean(hamming)\n",
    "    \n",
    "\n",
    "\n",
    "# def train_net(net, batch_size, train_loader, valid_loader, optimizer, criterion, epochs=10):\n",
    "def train_net(net, batch_size, learning_rate, epochs=10):\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train[:10000], batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid[:10000], batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "    # optimizer = torch.optim.SDG(net.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    iters = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    n = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1} of {epochs}')\n",
    "        for text, labels in iter(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = net(text)\n",
    "            loss = criterion(output, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #save training information\n",
    "            iters.append(n)\n",
    "            train_loss.append(float(loss)) \n",
    "            train_acc.append(get_accuracy(net, training_mode=True, size=batch_size)) # compute training accuracy \n",
    "            val_acc.append(get_accuracy(net, training_mode=False, size = batch_size))  # compute validation accuracy\n",
    "            # calculate for validation loss\n",
    "            \n",
    "            n += 1\n",
    "        print((\"Epoch {}: loss: {}, Training Accuracy: {}, Validation Accuracy: {}\").format(\n",
    "            epoch + 1, train_loss[epoch], train_acc[epoch], val_acc[epoch]\n",
    "        ))\n",
    "\n",
    "     # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, train_loss, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, train_acc, label=\"Train\")\n",
    "    plt.plot(iters, val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93511"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 24\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# #only use 10 datapoints from trainloader for testing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# train_loader = torch.utils.data.DataLoader(train[:10], batch_size=128, shuffle=True, collate_fn=pad_collate)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# valid_loader = torch.utils.data.DataLoader(valid[:10], batch_size=128, shuffle=True, collate_fn=pad_collate)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# train = train[:10]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True, collate_fn=pad_collate)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model \u001b[39m=\u001b[39m RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n\u001b[0;32m---> 24\u001b[0m train_net(model, BATCH_SIZE, LEARNING_RATE, EPOCHS)\n",
      "Cell \u001b[0;32mIn[93], line 71\u001b[0m, in \u001b[0;36mtrain_net\u001b[0;34m(net, batch_size, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     69\u001b[0m iters\u001b[39m.\u001b[39mappend(n)\n\u001b[1;32m     70\u001b[0m train_loss\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(loss)) \n\u001b[0;32m---> 71\u001b[0m train_acc\u001b[39m.\u001b[39mappend(get_accuracy(net, training_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, size\u001b[39m=\u001b[39;49mbatch_size)) \u001b[39m# compute training accuracy \u001b[39;00m\n\u001b[1;32m     72\u001b[0m val_acc\u001b[39m.\u001b[39mappend(get_accuracy(net, training_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, size \u001b[39m=\u001b[39m batch_size))  \u001b[39m# compute validation accuracy\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# calculate for validation loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[93], line 20\u001b[0m, in \u001b[0;36mget_accuracy\u001b[0;34m(model, training_mode, size)\u001b[0m\n\u001b[1;32m     18\u001b[0m hamming \u001b[39m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m text, label \u001b[39min\u001b[39;00m loader:\n\u001b[0;32m---> 20\u001b[0m     predictions \u001b[39m=\u001b[39m model(text)\n\u001b[1;32m     21\u001b[0m     \u001b[39m# f1_scores.append(f1_score(label, predictions, average='macro'))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m#detatching the tensor from the graph\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[92], line 20\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m     18\u001b[0m     \u001b[39m# embedded = self.dropout(self.embedding(text))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(text)\n\u001b[0;32m---> 20\u001b[0m     output, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(embedded)\n\u001b[1;32m     21\u001b[0m     \u001b[39m# hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     hidden \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((hidden[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,:,:], hidden[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,:]), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#train model\n",
    "# INPUT_DIM = 10000\n",
    "INPUT_DIM=10000\n",
    "EMBEDDING_DIM = 126\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 8\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.6\n",
    "# LEARNING_RATE = 0.00075\n",
    "LEARNING_RATE = 0.00075\n",
    "EPOCHS = 36\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# #only use 10 datapoints from trainloader for testing\n",
    "# train_loader = torch.utils.data.DataLoader(train[:10], batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid[:10], batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "#only take the first 10 datapoints from trainloader for testing\n",
    "# train = train[:10]\n",
    "# train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "train_net(model, BATCH_SIZE, LEARNING_RATE, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "600ea93296a0ae4f33cffb606658847eff10124d9f52ef16042deecb5dd32ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

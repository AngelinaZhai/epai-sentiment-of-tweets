{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchtext\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from langdetect import detect_langs\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pandarallel import pandarallel \n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  use_cuda = True\n",
    "else:  \n",
    "  use_cuda = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading GLOVE embeddings\n",
    "GLOVE = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=10000)  # use 10k most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-c32713cabe528196\n",
      "Found cached dataset parquet (/Users/angelinazhai/.cache/huggingface/datasets/ucberkeley-dlab___parquet/ucberkeley-dlab--measuring-hate-speech-c32713cabe528196/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67825581388644f7a1d84160e804d725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>infitms</th>\n",
       "      <th>outfitms</th>\n",
       "      <th>annotator_severity</th>\n",
       "      <th>std_err</th>\n",
       "      <th>annotator_infitms</th>\n",
       "      <th>annotator_outfitms</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>annotator_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.00000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23530.416138</td>\n",
       "      <td>5567.097812</td>\n",
       "      <td>1.281352</td>\n",
       "      <td>2.954307</td>\n",
       "      <td>2.828875</td>\n",
       "      <td>2.56331</td>\n",
       "      <td>2.278638</td>\n",
       "      <td>2.698575</td>\n",
       "      <td>1.846211</td>\n",
       "      <td>1.052045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744733</td>\n",
       "      <td>-0.567428</td>\n",
       "      <td>1.034322</td>\n",
       "      <td>1.001052</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>0.300588</td>\n",
       "      <td>1.007158</td>\n",
       "      <td>1.011841</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>37.910772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12387.194125</td>\n",
       "      <td>3230.508937</td>\n",
       "      <td>1.023542</td>\n",
       "      <td>1.231552</td>\n",
       "      <td>1.309548</td>\n",
       "      <td>1.38983</td>\n",
       "      <td>1.370876</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>1.402372</td>\n",
       "      <td>1.345706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932260</td>\n",
       "      <td>2.380003</td>\n",
       "      <td>0.496867</td>\n",
       "      <td>0.791943</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>0.236380</td>\n",
       "      <td>0.269876</td>\n",
       "      <td>0.675863</td>\n",
       "      <td>0.613006</td>\n",
       "      <td>11.641276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.340000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-1.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-1.578693</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18148.000000</td>\n",
       "      <td>2719.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.330000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-0.341008</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20052.000000</td>\n",
       "      <td>5602.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32038.250000</td>\n",
       "      <td>8363.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.449555</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50070.000000</td>\n",
       "      <td>11142.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          comment_id   annotator_id       platform      sentiment  \\\n",
       "count  135556.000000  135556.000000  135556.000000  135556.000000   \n",
       "mean    23530.416138    5567.097812       1.281352       2.954307   \n",
       "std     12387.194125    3230.508937       1.023542       1.231552   \n",
       "min         1.000000       1.000000       0.000000       0.000000   \n",
       "25%     18148.000000    2719.000000       0.000000       2.000000   \n",
       "50%     20052.000000    5602.500000       1.000000       3.000000   \n",
       "75%     32038.250000    8363.000000       2.000000       4.000000   \n",
       "max     50070.000000   11142.000000       3.000000       4.000000   \n",
       "\n",
       "             respect        insult      humiliate         status  \\\n",
       "count  135556.000000  135556.00000  135556.000000  135556.000000   \n",
       "mean        2.828875       2.56331       2.278638       2.698575   \n",
       "std         1.309548       1.38983       1.370876       0.898500   \n",
       "min         0.000000       0.00000       0.000000       0.000000   \n",
       "25%         2.000000       2.00000       1.000000       2.000000   \n",
       "50%         3.000000       3.00000       3.000000       3.000000   \n",
       "75%         4.000000       4.00000       3.000000       3.000000   \n",
       "max         4.000000       4.00000       4.000000       4.000000   \n",
       "\n",
       "          dehumanize       violence  ...     hatespeech  hate_speech_score  \\\n",
       "count  135556.000000  135556.000000  ...  135556.000000      135556.000000   \n",
       "mean        1.846211       1.052045  ...       0.744733          -0.567428   \n",
       "std         1.402372       1.345706  ...       0.932260           2.380003   \n",
       "min         0.000000       0.000000  ...       0.000000          -8.340000   \n",
       "25%         1.000000       0.000000  ...       0.000000          -2.330000   \n",
       "50%         2.000000       0.000000  ...       0.000000          -0.340000   \n",
       "75%         3.000000       2.000000  ...       2.000000           1.410000   \n",
       "max         4.000000       4.000000  ...       2.000000           6.300000   \n",
       "\n",
       "             infitms       outfitms  annotator_severity        std_err  \\\n",
       "count  135556.000000  135556.000000       135556.000000  135556.000000   \n",
       "mean        1.034322       1.001052           -0.018817       0.300588   \n",
       "std         0.496867       0.791943            0.487261       0.236380   \n",
       "min         0.100000       0.070000           -1.820000       0.020000   \n",
       "25%         0.710000       0.560000           -0.380000       0.030000   \n",
       "50%         0.960000       0.830000           -0.020000       0.340000   \n",
       "75%         1.300000       1.220000            0.350000       0.420000   \n",
       "max         5.900000       9.000000            1.360000       1.900000   \n",
       "\n",
       "       annotator_infitms  annotator_outfitms     hypothesis  annotator_age  \n",
       "count      135556.000000       135556.000000  135556.000000  135451.000000  \n",
       "mean            1.007158            1.011841       0.014589      37.910772  \n",
       "std             0.269876            0.675863       0.613006      11.641276  \n",
       "min             0.390000            0.280000      -1.578693      18.000000  \n",
       "25%             0.810000            0.670000      -0.341008      29.000000  \n",
       "50%             0.970000            0.850000       0.110405      35.000000  \n",
       "75%             1.170000            1.130000       0.449555      45.000000  \n",
       "max             2.010000            9.000000       0.987511      81.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load numpy array from file\n",
    "tmp_np_arr = np.load('hate_speech.npy', allow_pickle=True)\n",
    "\n",
    "#convert to pandas dataframe\n",
    "df.drop(df.iloc[:, 15:131], inplace=True, axis=1)\n",
    "df_tmp = df.drop([\"annotator_id\"], axis=1)\n",
    "df_norm = pd.DataFrame(tmp_np_arr, columns=df_tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spliced = df_norm.drop('comment_id', axis=1)\n",
    "df_spliced = df_spliced.drop('platform', axis=1)\n",
    "df_spliced = df_spliced.drop('sentiment', axis=1)\n",
    "df_spliced = df_spliced.drop('hatespeech', axis=1)\n",
    "df_spliced = df_spliced.drop('hate_speech_score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_spliced.iloc[:,-1:]\n",
    "labels = df_spliced.iloc[:,:-1]\n",
    "labels = labels.to_numpy()\n",
    "label_names = list(df_spliced.iloc[:,:-1].columns)\n",
    "train_size = int(0.7*len(df_spliced))\n",
    "val_size = int((len(df_spliced) - train_size)/2)\n",
    "test_size = len(df_spliced) - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tweet(tweet):\n",
    "    # separate punctuations\n",
    "    tweet = tweet.replace(\".\", \" . \") \\\n",
    "                 .replace(\",\", \" , \") \\\n",
    "                 .replace(\";\", \" ; \") \\\n",
    "                 .replace(\"?\", \" ? \")\n",
    "    return tweet.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_words(glove_vector):\n",
    "    train, valid, test = [], [], []\n",
    "    for index, row in df_spliced.iterrows():\n",
    "        try:\n",
    "            tweet = row[-1]\n",
    "            idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
    "                    for w in split_tweet(tweet)\n",
    "                    if w in glove_vector.stoi] # keep words that has an embedding\n",
    "            if not idxs: # ignore tweets without any word with an embedding\n",
    "                continue\n",
    "            idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
    "            label = np.array(row[:-1].values).astype(np.float32) \n",
    "            label = torch.tensor(label) #storing label information to tensor\n",
    "            #adding tweet to corresponding train/val/test set\n",
    "            if index < train_size:\n",
    "                train.append((idxs, label))\n",
    "            elif index < train_size+val_size:\n",
    "                valid.append((idxs, label))\n",
    "            else:\n",
    "                test.append((idxs, label))\n",
    "        except:\n",
    "            print(\"Error at index: \", index)\n",
    "            continue\n",
    "    return train, valid, test\n",
    "\n",
    "train, valid, test = get_tweet_words(GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "  (xx, yy) = zip(*batch)\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  y_lens = [len(y) for y in yy]\n",
    "\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "  return xx_pad, yy_pad\n",
    "  \n",
    "train_loader = torch.utils.data.DataLoader(train[:10], batch_size=10, shuffle=True, collate_fn=pad_collate)\n",
    "valid_loader = torch.utils.data.DataLoader(valid[:10], batch_size=10, shuffle=True, collate_fn=pad_collate)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1024)\n",
    "        self.fc6 = nn.Linear(1024, 512)\n",
    "        self.fc7 = nn.Linear(512, 256)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # embedded = self.dropout(self.embedding(text))\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        hidden = self.relu(self.fc(hidden))\n",
    "        hidden = self.relu(self.fc6(hidden))\n",
    "        hidden = self.relu(self.fc7(hidden))\n",
    "        hidden = self.relu(self.fc1(hidden))\n",
    "        hidden = self.relu(self.fc2(hidden))\n",
    "        hidden = self.relu(self.fc3(hidden))\n",
    "        hidden = self.relu(self.fc4(hidden))\n",
    "        return self.fc5(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training loop\n",
    "# def train(model, iterator, optimizer, criterion):\n",
    "#     epoch_loss = 0\n",
    "#     epoch_acc = 0\n",
    "#     model.train()\n",
    "#     for text, label in \n",
    "#         # epoch_acc += acc.item()\n",
    "#     return epoch_loss / len(iterator), 0\n",
    "\n",
    "def train_net(net, batch_size, train_loader, valid_loader, optimizer, criterion, epochs=10):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    #add in batch size\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1} of {epochs}')\n",
    "        for text, labels in iter(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = net(text)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        print(f'Train Loss: {loss.item()}')\n",
    "\n",
    "    #plot loss\n",
    "    plt.plot(train_loss, label='Training loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2000\n",
      "Train Loss: 0.7070940136909485\n",
      "Epoch 2 of 2000\n",
      "Train Loss: 0.706669270992279\n",
      "Epoch 3 of 2000\n",
      "Train Loss: 0.7062322497367859\n",
      "Epoch 4 of 2000\n",
      "Train Loss: 0.7057236433029175\n",
      "Epoch 5 of 2000\n",
      "Train Loss: 0.7051432728767395\n",
      "Epoch 6 of 2000\n",
      "Train Loss: 0.7042081356048584\n",
      "Epoch 7 of 2000\n",
      "Train Loss: 0.702329695224762\n",
      "Epoch 8 of 2000\n",
      "Train Loss: 0.6989606618881226\n",
      "Epoch 9 of 2000\n",
      "Train Loss: 0.6941251158714294\n",
      "Epoch 10 of 2000\n",
      "Train Loss: 0.6878819465637207\n",
      "Epoch 11 of 2000\n",
      "Train Loss: 0.6836713552474976\n",
      "Epoch 12 of 2000\n",
      "Train Loss: 0.6844479441642761\n",
      "Epoch 13 of 2000\n",
      "Train Loss: 0.6757516860961914\n",
      "Epoch 14 of 2000\n",
      "Train Loss: 0.6717613935470581\n",
      "Epoch 15 of 2000\n",
      "Train Loss: 0.6698784232139587\n",
      "Epoch 16 of 2000\n",
      "Train Loss: 0.6669338941574097\n",
      "Epoch 17 of 2000\n",
      "Train Loss: 0.6626197695732117\n",
      "Epoch 18 of 2000\n",
      "Train Loss: 0.6585315465927124\n",
      "Epoch 19 of 2000\n",
      "Train Loss: 0.6557502746582031\n",
      "Epoch 20 of 2000\n",
      "Train Loss: 0.6526929140090942\n",
      "Epoch 21 of 2000\n",
      "Train Loss: 0.6467798352241516\n",
      "Epoch 22 of 2000\n",
      "Train Loss: 0.6457935571670532\n",
      "Epoch 23 of 2000\n",
      "Train Loss: 0.6437581777572632\n",
      "Epoch 24 of 2000\n",
      "Train Loss: 0.6410637497901917\n",
      "Epoch 25 of 2000\n",
      "Train Loss: 0.6385106444358826\n",
      "Epoch 26 of 2000\n",
      "Train Loss: 0.635811448097229\n",
      "Epoch 27 of 2000\n",
      "Train Loss: 0.6324125528335571\n",
      "Epoch 28 of 2000\n",
      "Train Loss: 0.629398763179779\n",
      "Epoch 29 of 2000\n",
      "Train Loss: 0.6249958872795105\n",
      "Epoch 30 of 2000\n",
      "Train Loss: 0.6205912232398987\n",
      "Epoch 31 of 2000\n",
      "Train Loss: 0.6166990995407104\n",
      "Epoch 32 of 2000\n",
      "Train Loss: 0.6132761240005493\n",
      "Epoch 33 of 2000\n",
      "Train Loss: 0.6113778352737427\n",
      "Epoch 34 of 2000\n",
      "Train Loss: 0.6094481945037842\n",
      "Epoch 35 of 2000\n",
      "Train Loss: 0.6037461757659912\n",
      "Epoch 36 of 2000\n",
      "Train Loss: 0.5995384454727173\n",
      "Epoch 37 of 2000\n",
      "Train Loss: 0.5959327816963196\n",
      "Epoch 38 of 2000\n",
      "Train Loss: 0.5911551117897034\n",
      "Epoch 39 of 2000\n",
      "Train Loss: 0.5865811705589294\n",
      "Epoch 40 of 2000\n",
      "Train Loss: 0.5836122632026672\n",
      "Epoch 41 of 2000\n",
      "Train Loss: 0.5804811716079712\n",
      "Epoch 42 of 2000\n",
      "Train Loss: 0.5759010314941406\n",
      "Epoch 43 of 2000\n",
      "Train Loss: 0.5713087320327759\n",
      "Epoch 44 of 2000\n",
      "Train Loss: 0.5672223567962646\n",
      "Epoch 45 of 2000\n",
      "Train Loss: 0.5583928823471069\n",
      "Epoch 46 of 2000\n",
      "Train Loss: 0.5457628965377808\n",
      "Epoch 47 of 2000\n",
      "Train Loss: 0.5369938015937805\n",
      "Epoch 48 of 2000\n",
      "Train Loss: 0.5327237844467163\n",
      "Epoch 49 of 2000\n",
      "Train Loss: 0.5241862535476685\n",
      "Epoch 50 of 2000\n",
      "Train Loss: 0.5146253705024719\n",
      "Epoch 51 of 2000\n",
      "Train Loss: 0.5057080388069153\n",
      "Epoch 52 of 2000\n",
      "Train Loss: 0.527047336101532\n",
      "Epoch 53 of 2000\n",
      "Train Loss: 0.5035364627838135\n",
      "Epoch 54 of 2000\n",
      "Train Loss: 0.5064641237258911\n",
      "Epoch 55 of 2000\n",
      "Train Loss: 0.4898245334625244\n",
      "Epoch 56 of 2000\n",
      "Train Loss: 0.4891810417175293\n",
      "Epoch 57 of 2000\n",
      "Train Loss: 0.49189481139183044\n",
      "Epoch 58 of 2000\n",
      "Train Loss: 0.4890303611755371\n",
      "Epoch 59 of 2000\n",
      "Train Loss: 0.48499003052711487\n",
      "Epoch 60 of 2000\n",
      "Train Loss: 0.5513292551040649\n",
      "Epoch 61 of 2000\n",
      "Train Loss: 0.5167899131774902\n",
      "Epoch 62 of 2000\n",
      "Train Loss: 0.4888319969177246\n",
      "Epoch 63 of 2000\n",
      "Train Loss: 0.4890057444572449\n",
      "Epoch 64 of 2000\n",
      "Train Loss: 0.4927501082420349\n",
      "Epoch 65 of 2000\n",
      "Train Loss: 0.49472713470458984\n",
      "Epoch 66 of 2000\n",
      "Train Loss: 0.4910605549812317\n",
      "Epoch 67 of 2000\n",
      "Train Loss: 0.49062013626098633\n",
      "Epoch 68 of 2000\n",
      "Train Loss: 0.4883735775947571\n",
      "Epoch 69 of 2000\n",
      "Train Loss: 0.48685222864151\n",
      "Epoch 70 of 2000\n",
      "Train Loss: 0.4840153753757477\n",
      "Epoch 71 of 2000\n",
      "Train Loss: 0.4957304000854492\n",
      "Epoch 72 of 2000\n",
      "Train Loss: 0.5423496961593628\n",
      "Epoch 73 of 2000\n",
      "Train Loss: 0.5267534852027893\n",
      "Epoch 74 of 2000\n",
      "Train Loss: 0.528956413269043\n",
      "Epoch 75 of 2000\n",
      "Train Loss: 0.5169278979301453\n",
      "Epoch 76 of 2000\n",
      "Train Loss: 0.5129916071891785\n",
      "Epoch 77 of 2000\n",
      "Train Loss: 0.5115489959716797\n",
      "Epoch 78 of 2000\n",
      "Train Loss: 0.5071336030960083\n",
      "Epoch 79 of 2000\n",
      "Train Loss: 0.5041574239730835\n",
      "Epoch 80 of 2000\n",
      "Train Loss: 0.5039576292037964\n",
      "Epoch 81 of 2000\n",
      "Train Loss: 0.5041742324829102\n",
      "Epoch 82 of 2000\n",
      "Train Loss: 0.5024782419204712\n",
      "Epoch 83 of 2000\n",
      "Train Loss: 0.5000643730163574\n",
      "Epoch 84 of 2000\n",
      "Train Loss: 0.49805527925491333\n",
      "Epoch 85 of 2000\n",
      "Train Loss: 0.4960598051548004\n",
      "Epoch 86 of 2000\n",
      "Train Loss: 0.4942587912082672\n",
      "Epoch 87 of 2000\n",
      "Train Loss: 0.49245113134384155\n",
      "Epoch 88 of 2000\n",
      "Train Loss: 0.49105191230773926\n",
      "Epoch 89 of 2000\n",
      "Train Loss: 0.49023208022117615\n",
      "Epoch 90 of 2000\n",
      "Train Loss: 0.489751398563385\n",
      "Epoch 91 of 2000\n",
      "Train Loss: 0.4885428547859192\n",
      "Epoch 92 of 2000\n",
      "Train Loss: 0.4896000027656555\n",
      "Epoch 93 of 2000\n",
      "Train Loss: 0.4899848997592926\n",
      "Epoch 94 of 2000\n",
      "Train Loss: 0.48785337805747986\n",
      "Epoch 95 of 2000\n",
      "Train Loss: 0.48760300874710083\n",
      "Epoch 96 of 2000\n",
      "Train Loss: 0.48728498816490173\n",
      "Epoch 97 of 2000\n",
      "Train Loss: 0.4850866198539734\n",
      "Epoch 98 of 2000\n",
      "Train Loss: 0.48470157384872437\n",
      "Epoch 99 of 2000\n",
      "Train Loss: 0.48458439111709595\n",
      "Epoch 100 of 2000\n",
      "Train Loss: 0.48378124833106995\n",
      "Epoch 101 of 2000\n",
      "Train Loss: 0.48255103826522827\n",
      "Epoch 102 of 2000\n",
      "Train Loss: 0.4820040166378021\n",
      "Epoch 103 of 2000\n",
      "Train Loss: 0.4813787341117859\n",
      "Epoch 104 of 2000\n",
      "Train Loss: 0.48141536116600037\n",
      "Epoch 105 of 2000\n",
      "Train Loss: 0.4805498719215393\n",
      "Epoch 106 of 2000\n",
      "Train Loss: 0.47774869203567505\n",
      "Epoch 107 of 2000\n",
      "Train Loss: 0.4776211380958557\n",
      "Epoch 108 of 2000\n",
      "Train Loss: 0.47512632608413696\n",
      "Epoch 109 of 2000\n",
      "Train Loss: 0.4737189710140228\n",
      "Epoch 110 of 2000\n",
      "Train Loss: 0.47221946716308594\n",
      "Epoch 111 of 2000\n",
      "Train Loss: 0.46947503089904785\n",
      "Epoch 112 of 2000\n",
      "Train Loss: 0.4683149755001068\n",
      "Epoch 113 of 2000\n",
      "Train Loss: 0.4650118947029114\n",
      "Epoch 114 of 2000\n",
      "Train Loss: 0.46244263648986816\n",
      "Epoch 115 of 2000\n",
      "Train Loss: 0.4591384530067444\n",
      "Epoch 116 of 2000\n",
      "Train Loss: 0.45647677779197693\n",
      "Epoch 117 of 2000\n",
      "Train Loss: 0.45432037115097046\n",
      "Epoch 118 of 2000\n",
      "Train Loss: 0.45093899965286255\n",
      "Epoch 119 of 2000\n",
      "Train Loss: 0.4473852515220642\n",
      "Epoch 120 of 2000\n",
      "Train Loss: 0.4437049329280853\n",
      "Epoch 121 of 2000\n",
      "Train Loss: 0.4404243528842926\n",
      "Epoch 122 of 2000\n",
      "Train Loss: 0.4371641278266907\n",
      "Epoch 123 of 2000\n",
      "Train Loss: 0.4350817799568176\n",
      "Epoch 124 of 2000\n",
      "Train Loss: 0.43302494287490845\n",
      "Epoch 125 of 2000\n",
      "Train Loss: 0.4293057322502136\n",
      "Epoch 126 of 2000\n",
      "Train Loss: 0.4267420172691345\n",
      "Epoch 127 of 2000\n",
      "Train Loss: 0.4244801998138428\n",
      "Epoch 128 of 2000\n",
      "Train Loss: 0.4212316572666168\n",
      "Epoch 129 of 2000\n",
      "Train Loss: 0.4179016053676605\n",
      "Epoch 130 of 2000\n",
      "Train Loss: 0.416829913854599\n",
      "Epoch 131 of 2000\n",
      "Train Loss: 0.4127945303916931\n",
      "Epoch 132 of 2000\n",
      "Train Loss: 0.41157644987106323\n",
      "Epoch 133 of 2000\n",
      "Train Loss: 0.40831518173217773\n",
      "Epoch 134 of 2000\n",
      "Train Loss: 0.40506601333618164\n",
      "Epoch 135 of 2000\n",
      "Train Loss: 0.4021734297275543\n",
      "Epoch 136 of 2000\n",
      "Train Loss: 0.3988345265388489\n",
      "Epoch 137 of 2000\n",
      "Train Loss: 0.395918607711792\n",
      "Epoch 138 of 2000\n",
      "Train Loss: 0.3932233452796936\n",
      "Epoch 139 of 2000\n",
      "Train Loss: 0.38949957489967346\n",
      "Epoch 140 of 2000\n",
      "Train Loss: 0.3866133689880371\n",
      "Epoch 141 of 2000\n",
      "Train Loss: 0.38175833225250244\n",
      "Epoch 142 of 2000\n",
      "Train Loss: 0.37663188576698303\n",
      "Epoch 143 of 2000\n",
      "Train Loss: 0.3722132742404938\n",
      "Epoch 144 of 2000\n",
      "Train Loss: 0.3689422011375427\n",
      "Epoch 145 of 2000\n",
      "Train Loss: 0.3648248612880707\n",
      "Epoch 146 of 2000\n",
      "Train Loss: 0.36113375425338745\n",
      "Epoch 147 of 2000\n",
      "Train Loss: 0.3594643175601959\n",
      "Epoch 148 of 2000\n",
      "Train Loss: 0.3549202084541321\n",
      "Epoch 149 of 2000\n",
      "Train Loss: 0.35455381870269775\n",
      "Epoch 150 of 2000\n",
      "Train Loss: 0.3507639765739441\n",
      "Epoch 151 of 2000\n",
      "Train Loss: 0.3440283238887787\n",
      "Epoch 152 of 2000\n",
      "Train Loss: 0.3456547260284424\n",
      "Epoch 153 of 2000\n",
      "Train Loss: 0.3361406624317169\n",
      "Epoch 154 of 2000\n",
      "Train Loss: 0.3330525755882263\n",
      "Epoch 155 of 2000\n",
      "Train Loss: 0.3234778046607971\n",
      "Epoch 156 of 2000\n",
      "Train Loss: 0.32350271940231323\n",
      "Epoch 157 of 2000\n",
      "Train Loss: 0.30490249395370483\n",
      "Epoch 158 of 2000\n",
      "Train Loss: 0.3061511218547821\n",
      "Epoch 159 of 2000\n",
      "Train Loss: 0.29565930366516113\n",
      "Epoch 160 of 2000\n",
      "Train Loss: 0.2937884032726288\n",
      "Epoch 161 of 2000\n",
      "Train Loss: 0.2757621705532074\n",
      "Epoch 162 of 2000\n",
      "Train Loss: 0.285396009683609\n",
      "Epoch 163 of 2000\n",
      "Train Loss: 0.25754159688949585\n",
      "Epoch 164 of 2000\n",
      "Train Loss: 0.2600427269935608\n",
      "Epoch 165 of 2000\n",
      "Train Loss: 0.24561119079589844\n",
      "Epoch 166 of 2000\n",
      "Train Loss: 0.24608302116394043\n",
      "Epoch 167 of 2000\n",
      "Train Loss: 0.25675293803215027\n",
      "Epoch 168 of 2000\n",
      "Train Loss: 0.2406732141971588\n",
      "Epoch 169 of 2000\n",
      "Train Loss: 0.22712771594524384\n",
      "Epoch 170 of 2000\n",
      "Train Loss: 0.23410609364509583\n",
      "Epoch 171 of 2000\n",
      "Train Loss: 0.22332540154457092\n",
      "Epoch 172 of 2000\n",
      "Train Loss: 0.22016167640686035\n",
      "Epoch 173 of 2000\n",
      "Train Loss: 0.22446027398109436\n",
      "Epoch 174 of 2000\n",
      "Train Loss: 0.2121267020702362\n",
      "Epoch 175 of 2000\n",
      "Train Loss: 0.21549972891807556\n",
      "Epoch 176 of 2000\n",
      "Train Loss: 0.21948376297950745\n",
      "Epoch 177 of 2000\n",
      "Train Loss: 0.21782740950584412\n",
      "Epoch 178 of 2000\n",
      "Train Loss: 0.2209789752960205\n",
      "Epoch 179 of 2000\n",
      "Train Loss: 0.21674709022045135\n",
      "Epoch 180 of 2000\n",
      "Train Loss: 0.2133592665195465\n",
      "Epoch 181 of 2000\n",
      "Train Loss: 0.21070823073387146\n",
      "Epoch 182 of 2000\n",
      "Train Loss: 0.21089358627796173\n",
      "Epoch 183 of 2000\n",
      "Train Loss: 0.20320048928260803\n",
      "Epoch 184 of 2000\n",
      "Train Loss: 0.21035948395729065\n",
      "Epoch 185 of 2000\n",
      "Train Loss: 0.2053786814212799\n",
      "Epoch 186 of 2000\n",
      "Train Loss: 0.2032856047153473\n",
      "Epoch 187 of 2000\n",
      "Train Loss: 0.20268650352954865\n",
      "Epoch 188 of 2000\n",
      "Train Loss: 0.1991070806980133\n",
      "Epoch 189 of 2000\n",
      "Train Loss: 0.19933879375457764\n",
      "Epoch 190 of 2000\n",
      "Train Loss: 0.19773156940937042\n",
      "Epoch 191 of 2000\n",
      "Train Loss: 0.198068305850029\n",
      "Epoch 192 of 2000\n",
      "Train Loss: 0.1952134221792221\n",
      "Epoch 193 of 2000\n",
      "Train Loss: 0.194905087351799\n",
      "Epoch 194 of 2000\n",
      "Train Loss: 0.20305368304252625\n",
      "Epoch 195 of 2000\n",
      "Train Loss: 0.20733590424060822\n",
      "Epoch 196 of 2000\n",
      "Train Loss: 0.2048099935054779\n",
      "Epoch 197 of 2000\n",
      "Train Loss: 0.20629891753196716\n",
      "Epoch 198 of 2000\n",
      "Train Loss: 0.2339850664138794\n",
      "Epoch 199 of 2000\n",
      "Train Loss: 0.192601278424263\n",
      "Epoch 200 of 2000\n",
      "Train Loss: 0.20607057213783264\n",
      "Epoch 201 of 2000\n",
      "Train Loss: 0.1997799575328827\n",
      "Epoch 202 of 2000\n",
      "Train Loss: 0.20509502291679382\n",
      "Epoch 203 of 2000\n",
      "Train Loss: 0.19350333511829376\n",
      "Epoch 204 of 2000\n",
      "Train Loss: 0.20102202892303467\n",
      "Epoch 205 of 2000\n",
      "Train Loss: 0.233045294880867\n",
      "Epoch 206 of 2000\n",
      "Train Loss: 0.19039800763130188\n",
      "Epoch 207 of 2000\n",
      "Train Loss: 0.20444278419017792\n",
      "Epoch 208 of 2000\n",
      "Train Loss: 0.19998905062675476\n",
      "Epoch 209 of 2000\n",
      "Train Loss: 0.19462251663208008\n",
      "Epoch 210 of 2000\n",
      "Train Loss: 0.1936713457107544\n",
      "Epoch 211 of 2000\n",
      "Train Loss: 0.2017836570739746\n",
      "Epoch 212 of 2000\n",
      "Train Loss: 0.19428622722625732\n",
      "Epoch 213 of 2000\n",
      "Train Loss: 0.23379603028297424\n",
      "Epoch 214 of 2000\n",
      "Train Loss: 0.19154411554336548\n",
      "Epoch 215 of 2000\n",
      "Train Loss: 0.1920659989118576\n",
      "Epoch 216 of 2000\n",
      "Train Loss: 0.1943327635526657\n",
      "Epoch 217 of 2000\n",
      "Train Loss: 0.19296976923942566\n",
      "Epoch 218 of 2000\n",
      "Train Loss: 0.19617268443107605\n",
      "Epoch 219 of 2000\n",
      "Train Loss: 0.19989845156669617\n",
      "Epoch 220 of 2000\n",
      "Train Loss: 0.19100168347358704\n",
      "Epoch 221 of 2000\n",
      "Train Loss: 0.19141022861003876\n",
      "Epoch 222 of 2000\n",
      "Train Loss: 0.18658845126628876\n",
      "Epoch 223 of 2000\n",
      "Train Loss: 0.1856554001569748\n",
      "Epoch 224 of 2000\n",
      "Train Loss: 0.18566326797008514\n",
      "Epoch 225 of 2000\n",
      "Train Loss: 0.18721477687358856\n",
      "Epoch 226 of 2000\n",
      "Train Loss: 0.18672724068164825\n",
      "Epoch 227 of 2000\n",
      "Train Loss: 0.18725040555000305\n",
      "Epoch 228 of 2000\n",
      "Train Loss: 0.18374939262866974\n",
      "Epoch 229 of 2000\n",
      "Train Loss: 0.18297132849693298\n",
      "Epoch 230 of 2000\n",
      "Train Loss: 0.18114490807056427\n",
      "Epoch 231 of 2000\n",
      "Train Loss: 0.18376536667346954\n",
      "Epoch 232 of 2000\n",
      "Train Loss: 0.18335378170013428\n",
      "Epoch 233 of 2000\n",
      "Train Loss: 0.18060943484306335\n",
      "Epoch 234 of 2000\n",
      "Train Loss: 0.1805359423160553\n",
      "Epoch 235 of 2000\n",
      "Train Loss: 0.18035192787647247\n",
      "Epoch 236 of 2000\n",
      "Train Loss: 0.17977961897850037\n",
      "Epoch 237 of 2000\n",
      "Train Loss: 0.18138226866722107\n",
      "Epoch 238 of 2000\n",
      "Train Loss: 0.18017253279685974\n",
      "Epoch 239 of 2000\n",
      "Train Loss: 0.1789795309305191\n",
      "Epoch 240 of 2000\n",
      "Train Loss: 0.1782589554786682\n",
      "Epoch 241 of 2000\n",
      "Train Loss: 0.1787225902080536\n",
      "Epoch 242 of 2000\n",
      "Train Loss: 0.1791955977678299\n",
      "Epoch 243 of 2000\n",
      "Train Loss: 0.17738373577594757\n",
      "Epoch 244 of 2000\n",
      "Train Loss: 0.17862637341022491\n",
      "Epoch 245 of 2000\n",
      "Train Loss: 0.17654268443584442\n",
      "Epoch 246 of 2000\n",
      "Train Loss: 0.17770616710186005\n",
      "Epoch 247 of 2000\n",
      "Train Loss: 0.1777218133211136\n",
      "Epoch 248 of 2000\n",
      "Train Loss: 0.17652170360088348\n",
      "Epoch 249 of 2000\n",
      "Train Loss: 0.17586567997932434\n",
      "Epoch 250 of 2000\n",
      "Train Loss: 0.1761126071214676\n",
      "Epoch 251 of 2000\n",
      "Train Loss: 0.17642971873283386\n",
      "Epoch 252 of 2000\n",
      "Train Loss: 0.17652392387390137\n",
      "Epoch 253 of 2000\n",
      "Train Loss: 0.17463307082653046\n",
      "Epoch 254 of 2000\n",
      "Train Loss: 0.17500530183315277\n",
      "Epoch 255 of 2000\n",
      "Train Loss: 0.17464172840118408\n",
      "Epoch 256 of 2000\n",
      "Train Loss: 0.17443637549877167\n",
      "Epoch 257 of 2000\n",
      "Train Loss: 0.17365926504135132\n",
      "Epoch 258 of 2000\n",
      "Train Loss: 0.17351582646369934\n",
      "Epoch 259 of 2000\n",
      "Train Loss: 0.17300812900066376\n",
      "Epoch 260 of 2000\n",
      "Train Loss: 0.1738569587469101\n",
      "Epoch 261 of 2000\n",
      "Train Loss: 0.17445340752601624\n",
      "Epoch 262 of 2000\n",
      "Train Loss: 0.17349421977996826\n",
      "Epoch 263 of 2000\n",
      "Train Loss: 0.173588827252388\n",
      "Epoch 264 of 2000\n",
      "Train Loss: 0.17293448746204376\n",
      "Epoch 265 of 2000\n",
      "Train Loss: 0.17410404980182648\n",
      "Epoch 266 of 2000\n",
      "Train Loss: 0.17254379391670227\n",
      "Epoch 267 of 2000\n",
      "Train Loss: 0.17262080311775208\n",
      "Epoch 268 of 2000\n",
      "Train Loss: 0.17366759479045868\n",
      "Epoch 269 of 2000\n",
      "Train Loss: 0.1718638837337494\n",
      "Epoch 270 of 2000\n",
      "Train Loss: 0.1724151372909546\n",
      "Epoch 271 of 2000\n",
      "Train Loss: 0.1722666323184967\n",
      "Epoch 272 of 2000\n",
      "Train Loss: 0.1720205545425415\n",
      "Epoch 273 of 2000\n",
      "Train Loss: 0.17197363078594208\n",
      "Epoch 274 of 2000\n",
      "Train Loss: 0.17426343262195587\n",
      "Epoch 275 of 2000\n",
      "Train Loss: 0.1732814759016037\n",
      "Epoch 276 of 2000\n",
      "Train Loss: 0.17360565066337585\n",
      "Epoch 277 of 2000\n",
      "Train Loss: 0.17379291355609894\n",
      "Epoch 278 of 2000\n",
      "Train Loss: 0.1708245724439621\n",
      "Epoch 279 of 2000\n",
      "Train Loss: 0.17305438220500946\n",
      "Epoch 280 of 2000\n",
      "Train Loss: 0.1724107563495636\n",
      "Epoch 281 of 2000\n",
      "Train Loss: 0.1714411973953247\n",
      "Epoch 282 of 2000\n",
      "Train Loss: 0.17049089074134827\n",
      "Epoch 283 of 2000\n",
      "Train Loss: 0.17117106914520264\n",
      "Epoch 284 of 2000\n",
      "Train Loss: 0.17176608741283417\n",
      "Epoch 285 of 2000\n",
      "Train Loss: 0.17081114649772644\n",
      "Epoch 286 of 2000\n",
      "Train Loss: 0.171454519033432\n",
      "Epoch 287 of 2000\n",
      "Train Loss: 0.17023655772209167\n",
      "Epoch 288 of 2000\n",
      "Train Loss: 0.17124266922473907\n",
      "Epoch 289 of 2000\n",
      "Train Loss: 0.17038264870643616\n",
      "Epoch 290 of 2000\n",
      "Train Loss: 0.17013520002365112\n",
      "Epoch 291 of 2000\n",
      "Train Loss: 0.17098219692707062\n",
      "Epoch 292 of 2000\n",
      "Train Loss: 0.17006555199623108\n",
      "Epoch 293 of 2000\n",
      "Train Loss: 0.17046329379081726\n",
      "Epoch 294 of 2000\n",
      "Train Loss: 0.16979345679283142\n",
      "Epoch 295 of 2000\n",
      "Train Loss: 0.16976314783096313\n",
      "Epoch 296 of 2000\n",
      "Train Loss: 0.17003948986530304\n",
      "Epoch 297 of 2000\n",
      "Train Loss: 0.16964158415794373\n",
      "Epoch 298 of 2000\n",
      "Train Loss: 0.16989116370677948\n",
      "Epoch 299 of 2000\n",
      "Train Loss: 0.1712760180234909\n",
      "Epoch 300 of 2000\n",
      "Train Loss: 0.1701745092868805\n",
      "Epoch 301 of 2000\n",
      "Train Loss: 0.1703099012374878\n",
      "Epoch 302 of 2000\n",
      "Train Loss: 0.16909149289131165\n",
      "Epoch 303 of 2000\n",
      "Train Loss: 0.16998031735420227\n",
      "Epoch 304 of 2000\n",
      "Train Loss: 0.17085939645767212\n",
      "Epoch 305 of 2000\n",
      "Train Loss: 0.1710955798625946\n",
      "Epoch 306 of 2000\n",
      "Train Loss: 0.16961193084716797\n",
      "Epoch 307 of 2000\n",
      "Train Loss: 0.17065641283988953\n",
      "Epoch 308 of 2000\n",
      "Train Loss: 0.169870525598526\n",
      "Epoch 309 of 2000\n",
      "Train Loss: 0.16933748126029968\n",
      "Epoch 310 of 2000\n",
      "Train Loss: 0.17027875781059265\n",
      "Epoch 311 of 2000\n",
      "Train Loss: 0.1690535992383957\n",
      "Epoch 312 of 2000\n",
      "Train Loss: 0.1694699376821518\n",
      "Epoch 313 of 2000\n",
      "Train Loss: 0.16937896609306335\n",
      "Epoch 314 of 2000\n",
      "Train Loss: 0.1688702553510666\n",
      "Epoch 315 of 2000\n",
      "Train Loss: 0.16951438784599304\n",
      "Epoch 316 of 2000\n",
      "Train Loss: 0.16965197026729584\n",
      "Epoch 317 of 2000\n",
      "Train Loss: 0.1686427891254425\n",
      "Epoch 318 of 2000\n",
      "Train Loss: 0.16884899139404297\n",
      "Epoch 319 of 2000\n",
      "Train Loss: 0.16893906891345978\n",
      "Epoch 320 of 2000\n",
      "Train Loss: 0.16897393763065338\n",
      "Epoch 321 of 2000\n",
      "Train Loss: 0.1691669076681137\n",
      "Epoch 322 of 2000\n",
      "Train Loss: 0.16843396425247192\n",
      "Epoch 323 of 2000\n",
      "Train Loss: 0.16918055713176727\n",
      "Epoch 324 of 2000\n",
      "Train Loss: 0.16837458312511444\n",
      "Epoch 325 of 2000\n",
      "Train Loss: 0.16821806132793427\n",
      "Epoch 326 of 2000\n",
      "Train Loss: 0.16891835629940033\n",
      "Epoch 327 of 2000\n",
      "Train Loss: 0.1683407425880432\n",
      "Epoch 328 of 2000\n",
      "Train Loss: 0.16898317635059357\n",
      "Epoch 329 of 2000\n",
      "Train Loss: 0.17244505882263184\n",
      "Epoch 330 of 2000\n",
      "Train Loss: 0.16833381354808807\n",
      "Epoch 331 of 2000\n",
      "Train Loss: 0.16868877410888672\n",
      "Epoch 332 of 2000\n",
      "Train Loss: 0.16923578083515167\n",
      "Epoch 333 of 2000\n",
      "Train Loss: 0.1683875322341919\n",
      "Epoch 334 of 2000\n",
      "Train Loss: 0.16869547963142395\n",
      "Epoch 335 of 2000\n",
      "Train Loss: 0.16932052373886108\n",
      "Epoch 336 of 2000\n",
      "Train Loss: 0.1685067117214203\n",
      "Epoch 337 of 2000\n",
      "Train Loss: 0.17191462218761444\n",
      "Epoch 338 of 2000\n",
      "Train Loss: 0.16849014163017273\n",
      "Epoch 339 of 2000\n",
      "Train Loss: 0.16946490108966827\n",
      "Epoch 340 of 2000\n",
      "Train Loss: 0.1694379299879074\n",
      "Epoch 341 of 2000\n",
      "Train Loss: 0.16870996356010437\n",
      "Epoch 342 of 2000\n",
      "Train Loss: 0.16834530234336853\n",
      "Epoch 343 of 2000\n",
      "Train Loss: 0.16870851814746857\n",
      "Epoch 344 of 2000\n",
      "Train Loss: 0.16981303691864014\n",
      "Epoch 345 of 2000\n",
      "Train Loss: 0.1678386926651001\n",
      "Epoch 346 of 2000\n",
      "Train Loss: 0.1694401204586029\n",
      "Epoch 347 of 2000\n",
      "Train Loss: 0.1694011688232422\n",
      "Epoch 348 of 2000\n",
      "Train Loss: 0.1678684502840042\n",
      "Epoch 349 of 2000\n",
      "Train Loss: 0.16999439895153046\n",
      "Epoch 350 of 2000\n",
      "Train Loss: 0.1685400754213333\n",
      "Epoch 351 of 2000\n",
      "Train Loss: 0.16928724944591522\n",
      "Epoch 352 of 2000\n",
      "Train Loss: 0.16994543373584747\n",
      "Epoch 353 of 2000\n",
      "Train Loss: 0.16820064187049866\n",
      "Epoch 354 of 2000\n",
      "Train Loss: 0.16877691447734833\n",
      "Epoch 355 of 2000\n",
      "Train Loss: 0.16850045323371887\n",
      "Epoch 356 of 2000\n",
      "Train Loss: 0.16829673945903778\n",
      "Epoch 357 of 2000\n",
      "Train Loss: 0.16877757012844086\n",
      "Epoch 358 of 2000\n",
      "Train Loss: 0.1682858169078827\n",
      "Epoch 359 of 2000\n",
      "Train Loss: 0.16850732266902924\n",
      "Epoch 360 of 2000\n",
      "Train Loss: 0.16922776401042938\n",
      "Epoch 361 of 2000\n",
      "Train Loss: 0.1692449152469635\n",
      "Epoch 362 of 2000\n",
      "Train Loss: 0.1696902960538864\n",
      "Epoch 363 of 2000\n",
      "Train Loss: 0.1675553023815155\n",
      "Epoch 364 of 2000\n",
      "Train Loss: 0.16763636469841003\n",
      "Epoch 365 of 2000\n",
      "Train Loss: 0.16829076409339905\n",
      "Epoch 366 of 2000\n",
      "Train Loss: 0.16800424456596375\n",
      "Epoch 367 of 2000\n",
      "Train Loss: 0.16752317547798157\n",
      "Epoch 368 of 2000\n",
      "Train Loss: 0.16797956824302673\n",
      "Epoch 369 of 2000\n",
      "Train Loss: 0.16830670833587646\n",
      "Epoch 370 of 2000\n",
      "Train Loss: 0.16761472821235657\n",
      "Epoch 371 of 2000\n",
      "Train Loss: 0.1692320853471756\n",
      "Epoch 372 of 2000\n",
      "Train Loss: 0.16813454031944275\n",
      "Epoch 373 of 2000\n",
      "Train Loss: 0.16900932788848877\n",
      "Epoch 374 of 2000\n",
      "Train Loss: 0.1672683209180832\n",
      "Epoch 375 of 2000\n",
      "Train Loss: 0.16783031821250916\n",
      "Epoch 376 of 2000\n",
      "Train Loss: 0.16790488362312317\n",
      "Epoch 377 of 2000\n",
      "Train Loss: 0.1678771674633026\n",
      "Epoch 378 of 2000\n",
      "Train Loss: 0.16810965538024902\n",
      "Epoch 379 of 2000\n",
      "Train Loss: 0.16824807226657867\n",
      "Epoch 380 of 2000\n",
      "Train Loss: 0.16845059394836426\n",
      "Epoch 381 of 2000\n",
      "Train Loss: 0.167633056640625\n",
      "Epoch 382 of 2000\n",
      "Train Loss: 0.16787968575954437\n",
      "Epoch 383 of 2000\n",
      "Train Loss: 0.16850917041301727\n",
      "Epoch 384 of 2000\n",
      "Train Loss: 0.1674908697605133\n",
      "Epoch 385 of 2000\n",
      "Train Loss: 0.16746434569358826\n",
      "Epoch 386 of 2000\n",
      "Train Loss: 0.1682220846414566\n",
      "Epoch 387 of 2000\n",
      "Train Loss: 0.16809909045696259\n",
      "Epoch 388 of 2000\n",
      "Train Loss: 0.16807779669761658\n",
      "Epoch 389 of 2000\n",
      "Train Loss: 0.16932258009910583\n",
      "Epoch 390 of 2000\n",
      "Train Loss: 0.16841325163841248\n",
      "Epoch 391 of 2000\n",
      "Train Loss: 0.1679643988609314\n",
      "Epoch 392 of 2000\n",
      "Train Loss: 0.16868260502815247\n",
      "Epoch 393 of 2000\n",
      "Train Loss: 0.16898414492607117\n",
      "Epoch 394 of 2000\n",
      "Train Loss: 0.168808251619339\n",
      "Epoch 395 of 2000\n",
      "Train Loss: 0.16955584287643433\n",
      "Epoch 396 of 2000\n",
      "Train Loss: 0.16931399703025818\n",
      "Epoch 397 of 2000\n",
      "Train Loss: 0.16891276836395264\n",
      "Epoch 398 of 2000\n",
      "Train Loss: 0.16892248392105103\n",
      "Epoch 399 of 2000\n",
      "Train Loss: 0.1685905158519745\n",
      "Epoch 400 of 2000\n",
      "Train Loss: 0.1679810881614685\n",
      "Epoch 401 of 2000\n",
      "Train Loss: 0.16827742755413055\n",
      "Epoch 402 of 2000\n",
      "Train Loss: 0.16880011558532715\n",
      "Epoch 403 of 2000\n",
      "Train Loss: 0.16772082448005676\n",
      "Epoch 404 of 2000\n",
      "Train Loss: 0.16782724857330322\n",
      "Epoch 405 of 2000\n",
      "Train Loss: 0.16844132542610168\n",
      "Epoch 406 of 2000\n",
      "Train Loss: 0.16741585731506348\n",
      "Epoch 407 of 2000\n",
      "Train Loss: 0.16734731197357178\n",
      "Epoch 408 of 2000\n",
      "Train Loss: 0.16784434020519257\n",
      "Epoch 409 of 2000\n",
      "Train Loss: 0.1675024926662445\n",
      "Epoch 410 of 2000\n",
      "Train Loss: 0.16753268241882324\n",
      "Epoch 411 of 2000\n",
      "Train Loss: 0.16771498322486877\n",
      "Epoch 412 of 2000\n",
      "Train Loss: 0.168197900056839\n",
      "Epoch 413 of 2000\n",
      "Train Loss: 0.16748245060443878\n",
      "Epoch 414 of 2000\n",
      "Train Loss: 0.16763350367546082\n",
      "Epoch 415 of 2000\n",
      "Train Loss: 0.16730928421020508\n",
      "Epoch 416 of 2000\n",
      "Train Loss: 0.1675695925951004\n",
      "Epoch 417 of 2000\n",
      "Train Loss: 0.16747376322746277\n",
      "Epoch 418 of 2000\n",
      "Train Loss: 0.16732415556907654\n",
      "Epoch 419 of 2000\n",
      "Train Loss: 0.1670810431241989\n",
      "Epoch 420 of 2000\n",
      "Train Loss: 0.16734586656093597\n",
      "Epoch 421 of 2000\n",
      "Train Loss: 0.16739997267723083\n",
      "Epoch 422 of 2000\n",
      "Train Loss: 0.16684147715568542\n",
      "Epoch 423 of 2000\n",
      "Train Loss: 0.16731172800064087\n",
      "Epoch 424 of 2000\n",
      "Train Loss: 0.16723518073558807\n",
      "Epoch 425 of 2000\n",
      "Train Loss: 0.16691669821739197\n",
      "Epoch 426 of 2000\n",
      "Train Loss: 0.1672617644071579\n",
      "Epoch 427 of 2000\n",
      "Train Loss: 0.1671801060438156\n",
      "Epoch 428 of 2000\n",
      "Train Loss: 0.1673496514558792\n",
      "Epoch 429 of 2000\n",
      "Train Loss: 0.1675461381673813\n",
      "Epoch 430 of 2000\n",
      "Train Loss: 0.1675095558166504\n",
      "Epoch 431 of 2000\n",
      "Train Loss: 0.1680516004562378\n",
      "Epoch 432 of 2000\n",
      "Train Loss: 0.16721253097057343\n",
      "Epoch 433 of 2000\n",
      "Train Loss: 0.1680326610803604\n",
      "Epoch 434 of 2000\n",
      "Train Loss: 0.16755317151546478\n",
      "Epoch 435 of 2000\n",
      "Train Loss: 0.16713690757751465\n",
      "Epoch 436 of 2000\n",
      "Train Loss: 0.16711479425430298\n",
      "Epoch 437 of 2000\n",
      "Train Loss: 0.16724443435668945\n",
      "Epoch 438 of 2000\n",
      "Train Loss: 0.16712979972362518\n",
      "Epoch 439 of 2000\n",
      "Train Loss: 0.16735653579235077\n",
      "Epoch 440 of 2000\n",
      "Train Loss: 0.16718706488609314\n",
      "Epoch 441 of 2000\n",
      "Train Loss: 0.16724833846092224\n",
      "Epoch 442 of 2000\n",
      "Train Loss: 0.16740010678768158\n",
      "Epoch 443 of 2000\n",
      "Train Loss: 0.16764526069164276\n",
      "Epoch 444 of 2000\n",
      "Train Loss: 0.1673404425382614\n",
      "Epoch 445 of 2000\n",
      "Train Loss: 0.16710171103477478\n",
      "Epoch 446 of 2000\n",
      "Train Loss: 0.1672196090221405\n",
      "Epoch 447 of 2000\n",
      "Train Loss: 0.1668890416622162\n",
      "Epoch 448 of 2000\n",
      "Train Loss: 0.16695070266723633\n",
      "Epoch 449 of 2000\n",
      "Train Loss: 0.16716405749320984\n",
      "Epoch 450 of 2000\n",
      "Train Loss: 0.1668207347393036\n",
      "Epoch 451 of 2000\n",
      "Train Loss: 0.16670942306518555\n",
      "Epoch 452 of 2000\n",
      "Train Loss: 0.167135089635849\n",
      "Epoch 453 of 2000\n",
      "Train Loss: 0.1668950319290161\n",
      "Epoch 454 of 2000\n",
      "Train Loss: 0.16678395867347717\n",
      "Epoch 455 of 2000\n",
      "Train Loss: 0.16712523996829987\n",
      "Epoch 456 of 2000\n",
      "Train Loss: 0.16720835864543915\n",
      "Epoch 457 of 2000\n",
      "Train Loss: 0.16661925613880157\n",
      "Epoch 458 of 2000\n",
      "Train Loss: 0.16692829132080078\n",
      "Epoch 459 of 2000\n",
      "Train Loss: 0.16674216091632843\n",
      "Epoch 460 of 2000\n",
      "Train Loss: 0.16680331528186798\n",
      "Epoch 461 of 2000\n",
      "Train Loss: 0.16707465052604675\n",
      "Epoch 462 of 2000\n",
      "Train Loss: 0.1667969673871994\n",
      "Epoch 463 of 2000\n",
      "Train Loss: 0.16661235690116882\n",
      "Epoch 464 of 2000\n",
      "Train Loss: 0.16682203114032745\n",
      "Epoch 465 of 2000\n",
      "Train Loss: 0.16679984331130981\n",
      "Epoch 466 of 2000\n",
      "Train Loss: 0.16729262471199036\n",
      "Epoch 467 of 2000\n",
      "Train Loss: 0.1669391691684723\n",
      "Epoch 468 of 2000\n",
      "Train Loss: 0.16716311872005463\n",
      "Epoch 469 of 2000\n",
      "Train Loss: 0.16682258248329163\n",
      "Epoch 470 of 2000\n",
      "Train Loss: 0.1666664183139801\n",
      "Epoch 471 of 2000\n",
      "Train Loss: 0.16699948906898499\n",
      "Epoch 472 of 2000\n",
      "Train Loss: 0.1668149083852768\n",
      "Epoch 473 of 2000\n",
      "Train Loss: 0.16677263379096985\n",
      "Epoch 474 of 2000\n",
      "Train Loss: 0.16714799404144287\n",
      "Epoch 475 of 2000\n",
      "Train Loss: 0.16692323982715607\n",
      "Epoch 476 of 2000\n",
      "Train Loss: 0.1669672578573227\n",
      "Epoch 477 of 2000\n",
      "Train Loss: 0.16676846146583557\n",
      "Epoch 478 of 2000\n",
      "Train Loss: 0.16675610840320587\n",
      "Epoch 479 of 2000\n",
      "Train Loss: 0.16759710013866425\n",
      "Epoch 480 of 2000\n",
      "Train Loss: 0.1667664349079132\n",
      "Epoch 481 of 2000\n",
      "Train Loss: 0.16659119725227356\n",
      "Epoch 482 of 2000\n",
      "Train Loss: 0.16665524244308472\n",
      "Epoch 483 of 2000\n",
      "Train Loss: 0.16772720217704773\n",
      "Epoch 484 of 2000\n",
      "Train Loss: 0.16659648716449738\n",
      "Epoch 485 of 2000\n",
      "Train Loss: 0.16705046594142914\n",
      "Epoch 486 of 2000\n",
      "Train Loss: 0.16703924536705017\n",
      "Epoch 487 of 2000\n",
      "Train Loss: 0.16679459810256958\n",
      "Epoch 488 of 2000\n",
      "Train Loss: 0.16760502755641937\n",
      "Epoch 489 of 2000\n",
      "Train Loss: 0.16685965657234192\n",
      "Epoch 490 of 2000\n",
      "Train Loss: 0.16681312024593353\n",
      "Epoch 491 of 2000\n",
      "Train Loss: 0.16746844351291656\n",
      "Epoch 492 of 2000\n",
      "Train Loss: 0.1668783277273178\n",
      "Epoch 493 of 2000\n",
      "Train Loss: 0.16705451905727386\n",
      "Epoch 494 of 2000\n",
      "Train Loss: 0.16713711619377136\n",
      "Epoch 495 of 2000\n",
      "Train Loss: 0.16715916991233826\n",
      "Epoch 496 of 2000\n",
      "Train Loss: 0.16717001795768738\n",
      "Epoch 497 of 2000\n",
      "Train Loss: 0.1672285944223404\n",
      "Epoch 498 of 2000\n",
      "Train Loss: 0.16662904620170593\n",
      "Epoch 499 of 2000\n",
      "Train Loss: 0.166981503367424\n",
      "Epoch 500 of 2000\n",
      "Train Loss: 0.16667135059833527\n",
      "Epoch 501 of 2000\n",
      "Train Loss: 0.1668132245540619\n",
      "Epoch 502 of 2000\n",
      "Train Loss: 0.16664645075798035\n",
      "Epoch 503 of 2000\n",
      "Train Loss: 0.1668754518032074\n",
      "Epoch 504 of 2000\n",
      "Train Loss: 0.1669066995382309\n",
      "Epoch 505 of 2000\n",
      "Train Loss: 0.1669381558895111\n",
      "Epoch 506 of 2000\n",
      "Train Loss: 0.16672036051750183\n",
      "Epoch 507 of 2000\n",
      "Train Loss: 0.16686658561229706\n",
      "Epoch 508 of 2000\n",
      "Train Loss: 0.1666908711194992\n",
      "Epoch 509 of 2000\n",
      "Train Loss: 0.16678614914417267\n",
      "Epoch 510 of 2000\n",
      "Train Loss: 0.16654151678085327\n",
      "Epoch 511 of 2000\n",
      "Train Loss: 0.16692161560058594\n",
      "Epoch 512 of 2000\n",
      "Train Loss: 0.16710707545280457\n",
      "Epoch 513 of 2000\n",
      "Train Loss: 0.16694431006908417\n",
      "Epoch 514 of 2000\n",
      "Train Loss: 0.1667264699935913\n",
      "Epoch 515 of 2000\n",
      "Train Loss: 0.16691048443317413\n",
      "Epoch 516 of 2000\n",
      "Train Loss: 0.16741040349006653\n",
      "Epoch 517 of 2000\n",
      "Train Loss: 0.16784079372882843\n",
      "Epoch 518 of 2000\n",
      "Train Loss: 0.16706857085227966\n",
      "Epoch 519 of 2000\n",
      "Train Loss: 0.16695159673690796\n",
      "Epoch 520 of 2000\n",
      "Train Loss: 0.16754093766212463\n",
      "Epoch 521 of 2000\n",
      "Train Loss: 0.16726337373256683\n",
      "Epoch 522 of 2000\n",
      "Train Loss: 0.16677653789520264\n",
      "Epoch 523 of 2000\n",
      "Train Loss: 0.1670176088809967\n",
      "Epoch 524 of 2000\n",
      "Train Loss: 0.16711053252220154\n",
      "Epoch 525 of 2000\n",
      "Train Loss: 0.16678252816200256\n",
      "Epoch 526 of 2000\n",
      "Train Loss: 0.16715876758098602\n",
      "Epoch 527 of 2000\n",
      "Train Loss: 0.16676262021064758\n",
      "Epoch 528 of 2000\n",
      "Train Loss: 0.1668081134557724\n",
      "Epoch 529 of 2000\n",
      "Train Loss: 0.16725261509418488\n",
      "Epoch 530 of 2000\n",
      "Train Loss: 0.16681602597236633\n",
      "Epoch 531 of 2000\n",
      "Train Loss: 0.16653850674629211\n",
      "Epoch 532 of 2000\n",
      "Train Loss: 0.16668979823589325\n",
      "Epoch 533 of 2000\n",
      "Train Loss: 0.16683153808116913\n",
      "Epoch 534 of 2000\n",
      "Train Loss: 0.1668723076581955\n",
      "Epoch 535 of 2000\n",
      "Train Loss: 0.1666060835123062\n",
      "Epoch 536 of 2000\n",
      "Train Loss: 0.166945680975914\n",
      "Epoch 537 of 2000\n",
      "Train Loss: 0.16740097105503082\n",
      "Epoch 538 of 2000\n",
      "Train Loss: 0.16688841581344604\n",
      "Epoch 539 of 2000\n",
      "Train Loss: 0.16651999950408936\n",
      "Epoch 540 of 2000\n",
      "Train Loss: 0.16686482727527618\n",
      "Epoch 541 of 2000\n",
      "Train Loss: 0.16680894792079926\n",
      "Epoch 542 of 2000\n",
      "Train Loss: 0.1666661500930786\n",
      "Epoch 543 of 2000\n",
      "Train Loss: 0.16664375364780426\n",
      "Epoch 544 of 2000\n",
      "Train Loss: 0.16670894622802734\n",
      "Epoch 545 of 2000\n",
      "Train Loss: 0.16672217845916748\n",
      "Epoch 546 of 2000\n",
      "Train Loss: 0.1665441393852234\n",
      "Epoch 547 of 2000\n",
      "Train Loss: 0.1666882187128067\n",
      "Epoch 548 of 2000\n",
      "Train Loss: 0.16658726334571838\n",
      "Epoch 549 of 2000\n",
      "Train Loss: 0.16654928028583527\n",
      "Epoch 550 of 2000\n",
      "Train Loss: 0.16677017509937286\n",
      "Epoch 551 of 2000\n",
      "Train Loss: 0.16658970713615417\n",
      "Epoch 552 of 2000\n",
      "Train Loss: 0.16661112010478973\n",
      "Epoch 553 of 2000\n",
      "Train Loss: 0.16662366688251495\n",
      "Epoch 554 of 2000\n",
      "Train Loss: 0.16685211658477783\n",
      "Epoch 555 of 2000\n",
      "Train Loss: 0.16684703528881073\n",
      "Epoch 556 of 2000\n",
      "Train Loss: 0.16689209640026093\n",
      "Epoch 557 of 2000\n",
      "Train Loss: 0.1665208339691162\n",
      "Epoch 558 of 2000\n",
      "Train Loss: 0.16696521639823914\n",
      "Epoch 559 of 2000\n",
      "Train Loss: 0.1673581898212433\n",
      "Epoch 560 of 2000\n",
      "Train Loss: 0.1669880896806717\n",
      "Epoch 561 of 2000\n",
      "Train Loss: 0.16642972826957703\n",
      "Epoch 562 of 2000\n",
      "Train Loss: 0.1664859503507614\n",
      "Epoch 563 of 2000\n",
      "Train Loss: 0.1668182909488678\n",
      "Epoch 564 of 2000\n",
      "Train Loss: 0.16667473316192627\n",
      "Epoch 565 of 2000\n",
      "Train Loss: 0.16652454435825348\n",
      "Epoch 566 of 2000\n",
      "Train Loss: 0.16652439534664154\n",
      "Epoch 567 of 2000\n",
      "Train Loss: 0.1670028269290924\n",
      "Epoch 568 of 2000\n",
      "Train Loss: 0.16658850014209747\n",
      "Epoch 569 of 2000\n",
      "Train Loss: 0.16667000949382782\n",
      "Epoch 570 of 2000\n",
      "Train Loss: 0.16721732914447784\n",
      "Epoch 571 of 2000\n",
      "Train Loss: 0.16686931252479553\n",
      "Epoch 572 of 2000\n",
      "Train Loss: 0.16674742102622986\n",
      "Epoch 573 of 2000\n",
      "Train Loss: 0.1668461114168167\n",
      "Epoch 574 of 2000\n",
      "Train Loss: 0.1675768792629242\n",
      "Epoch 575 of 2000\n",
      "Train Loss: 0.16725385189056396\n",
      "Epoch 576 of 2000\n",
      "Train Loss: 0.16735246777534485\n",
      "Epoch 577 of 2000\n",
      "Train Loss: 0.16709792613983154\n",
      "Epoch 578 of 2000\n",
      "Train Loss: 0.16808676719665527\n",
      "Epoch 579 of 2000\n",
      "Train Loss: 0.16698244214057922\n",
      "Epoch 580 of 2000\n",
      "Train Loss: 0.16692429780960083\n",
      "Epoch 581 of 2000\n",
      "Train Loss: 0.16852958500385284\n",
      "Epoch 582 of 2000\n",
      "Train Loss: 0.16799740493297577\n",
      "Epoch 583 of 2000\n",
      "Train Loss: 0.1676497459411621\n",
      "Epoch 584 of 2000\n",
      "Train Loss: 0.16967085003852844\n",
      "Epoch 585 of 2000\n",
      "Train Loss: 0.16978004574775696\n",
      "Epoch 586 of 2000\n",
      "Train Loss: 0.16758571565151215\n",
      "Epoch 587 of 2000\n",
      "Train Loss: 0.16908957064151764\n",
      "Epoch 588 of 2000\n",
      "Train Loss: 0.16955086588859558\n",
      "Epoch 589 of 2000\n",
      "Train Loss: 0.1674448549747467\n",
      "Epoch 590 of 2000\n",
      "Train Loss: 0.16910557448863983\n",
      "Epoch 591 of 2000\n",
      "Train Loss: 0.1682438850402832\n",
      "Epoch 592 of 2000\n",
      "Train Loss: 0.16731873154640198\n",
      "Epoch 593 of 2000\n",
      "Train Loss: 0.16877755522727966\n",
      "Epoch 594 of 2000\n",
      "Train Loss: 0.16720475256443024\n",
      "Epoch 595 of 2000\n",
      "Train Loss: 0.1676788181066513\n",
      "Epoch 596 of 2000\n",
      "Train Loss: 0.1672188937664032\n",
      "Epoch 597 of 2000\n",
      "Train Loss: 0.16680335998535156\n",
      "Epoch 598 of 2000\n",
      "Train Loss: 0.1669447124004364\n",
      "Epoch 599 of 2000\n",
      "Train Loss: 0.1665104329586029\n",
      "Epoch 600 of 2000\n",
      "Train Loss: 0.16683900356292725\n",
      "Epoch 601 of 2000\n",
      "Train Loss: 0.1665392369031906\n",
      "Epoch 602 of 2000\n",
      "Train Loss: 0.1667373925447464\n",
      "Epoch 603 of 2000\n",
      "Train Loss: 0.1667824685573578\n",
      "Epoch 604 of 2000\n",
      "Train Loss: 0.16655504703521729\n",
      "Epoch 605 of 2000\n",
      "Train Loss: 0.1668667048215866\n",
      "Epoch 606 of 2000\n",
      "Train Loss: 0.16644124686717987\n",
      "Epoch 607 of 2000\n",
      "Train Loss: 0.1668587177991867\n",
      "Epoch 608 of 2000\n",
      "Train Loss: 0.1668510138988495\n",
      "Epoch 609 of 2000\n",
      "Train Loss: 0.166896253824234\n",
      "Epoch 610 of 2000\n",
      "Train Loss: 0.16745319962501526\n",
      "Epoch 611 of 2000\n",
      "Train Loss: 0.1667831689119339\n",
      "Epoch 612 of 2000\n",
      "Train Loss: 0.16687871515750885\n",
      "Epoch 613 of 2000\n",
      "Train Loss: 0.16738207638263702\n",
      "Epoch 614 of 2000\n",
      "Train Loss: 0.16662943363189697\n",
      "Epoch 615 of 2000\n",
      "Train Loss: 0.1675427258014679\n",
      "Epoch 616 of 2000\n",
      "Train Loss: 0.1668633669614792\n",
      "Epoch 617 of 2000\n",
      "Train Loss: 0.1668754518032074\n",
      "Epoch 618 of 2000\n",
      "Train Loss: 0.1671389788389206\n",
      "Epoch 619 of 2000\n",
      "Train Loss: 0.16693004965782166\n",
      "Epoch 620 of 2000\n",
      "Train Loss: 0.16685998439788818\n",
      "Epoch 621 of 2000\n",
      "Train Loss: 0.16717004776000977\n",
      "Epoch 622 of 2000\n",
      "Train Loss: 0.16657964885234833\n",
      "Epoch 623 of 2000\n",
      "Train Loss: 0.16670019924640656\n",
      "Epoch 624 of 2000\n",
      "Train Loss: 0.16667214035987854\n",
      "Epoch 625 of 2000\n",
      "Train Loss: 0.16642089188098907\n",
      "Epoch 626 of 2000\n",
      "Train Loss: 0.1668085753917694\n",
      "Epoch 627 of 2000\n",
      "Train Loss: 0.16681146621704102\n",
      "Epoch 628 of 2000\n",
      "Train Loss: 0.16668465733528137\n",
      "Epoch 629 of 2000\n",
      "Train Loss: 0.16667068004608154\n",
      "Epoch 630 of 2000\n",
      "Train Loss: 0.16676414012908936\n",
      "Epoch 631 of 2000\n",
      "Train Loss: 0.16688314080238342\n",
      "Epoch 632 of 2000\n",
      "Train Loss: 0.1665729284286499\n",
      "Epoch 633 of 2000\n",
      "Train Loss: 0.1668233573436737\n",
      "Epoch 634 of 2000\n",
      "Train Loss: 0.1666329801082611\n",
      "Epoch 635 of 2000\n",
      "Train Loss: 0.1665191948413849\n",
      "Epoch 636 of 2000\n",
      "Train Loss: 0.1668161153793335\n",
      "Epoch 637 of 2000\n",
      "Train Loss: 0.166500061750412\n",
      "Epoch 638 of 2000\n",
      "Train Loss: 0.166724294424057\n",
      "Epoch 639 of 2000\n",
      "Train Loss: 0.1667744666337967\n",
      "Epoch 640 of 2000\n",
      "Train Loss: 0.16651442646980286\n",
      "Epoch 641 of 2000\n",
      "Train Loss: 0.16706153750419617\n",
      "Epoch 642 of 2000\n",
      "Train Loss: 0.1666572540998459\n",
      "Epoch 643 of 2000\n",
      "Train Loss: 0.16679105162620544\n",
      "Epoch 644 of 2000\n",
      "Train Loss: 0.16683709621429443\n",
      "Epoch 645 of 2000\n",
      "Train Loss: 0.16636867821216583\n",
      "Epoch 646 of 2000\n",
      "Train Loss: 0.1668284833431244\n",
      "Epoch 647 of 2000\n",
      "Train Loss: 0.1668214052915573\n",
      "Epoch 648 of 2000\n",
      "Train Loss: 0.16659137606620789\n",
      "Epoch 649 of 2000\n",
      "Train Loss: 0.16686412692070007\n",
      "Epoch 650 of 2000\n",
      "Train Loss: 0.1664837896823883\n",
      "Epoch 651 of 2000\n",
      "Train Loss: 0.166473388671875\n",
      "Epoch 652 of 2000\n",
      "Train Loss: 0.16688260436058044\n",
      "Epoch 653 of 2000\n",
      "Train Loss: 0.16640882194042206\n",
      "Epoch 654 of 2000\n",
      "Train Loss: 0.16655230522155762\n",
      "Epoch 655 of 2000\n",
      "Train Loss: 0.16655364632606506\n",
      "Epoch 656 of 2000\n",
      "Train Loss: 0.1664554625749588\n",
      "Epoch 657 of 2000\n",
      "Train Loss: 0.166463702917099\n",
      "Epoch 658 of 2000\n",
      "Train Loss: 0.1664969027042389\n",
      "Epoch 659 of 2000\n",
      "Train Loss: 0.16667984426021576\n",
      "Epoch 660 of 2000\n",
      "Train Loss: 0.16664305329322815\n",
      "Epoch 661 of 2000\n",
      "Train Loss: 0.16676673293113708\n",
      "Epoch 662 of 2000\n",
      "Train Loss: 0.1663762778043747\n",
      "Epoch 663 of 2000\n",
      "Train Loss: 0.16655106842517853\n",
      "Epoch 664 of 2000\n",
      "Train Loss: 0.16672518849372864\n",
      "Epoch 665 of 2000\n",
      "Train Loss: 0.1663890779018402\n",
      "Epoch 666 of 2000\n",
      "Train Loss: 0.16692377626895905\n",
      "Epoch 667 of 2000\n",
      "Train Loss: 0.16662947833538055\n",
      "Epoch 668 of 2000\n",
      "Train Loss: 0.16659006476402283\n",
      "Epoch 669 of 2000\n",
      "Train Loss: 0.16663138568401337\n",
      "Epoch 670 of 2000\n",
      "Train Loss: 0.16647551953792572\n",
      "Epoch 671 of 2000\n",
      "Train Loss: 0.1669655591249466\n",
      "Epoch 672 of 2000\n",
      "Train Loss: 0.1665496826171875\n",
      "Epoch 673 of 2000\n",
      "Train Loss: 0.1665380597114563\n",
      "Epoch 674 of 2000\n",
      "Train Loss: 0.16660983860492706\n",
      "Epoch 675 of 2000\n",
      "Train Loss: 0.1663511097431183\n",
      "Epoch 676 of 2000\n",
      "Train Loss: 0.16644319891929626\n",
      "Epoch 677 of 2000\n",
      "Train Loss: 0.16646647453308105\n",
      "Epoch 678 of 2000\n",
      "Train Loss: 0.16641393303871155\n",
      "Epoch 679 of 2000\n",
      "Train Loss: 0.16655674576759338\n",
      "Epoch 680 of 2000\n",
      "Train Loss: 0.16645412147045135\n",
      "Epoch 681 of 2000\n",
      "Train Loss: 0.16637960076332092\n",
      "Epoch 682 of 2000\n",
      "Train Loss: 0.16660359501838684\n",
      "Epoch 683 of 2000\n",
      "Train Loss: 0.1664278507232666\n",
      "Epoch 684 of 2000\n",
      "Train Loss: 0.1664154976606369\n",
      "Epoch 685 of 2000\n",
      "Train Loss: 0.1664283275604248\n",
      "Epoch 686 of 2000\n",
      "Train Loss: 0.16635365784168243\n",
      "Epoch 687 of 2000\n",
      "Train Loss: 0.16635695099830627\n",
      "Epoch 688 of 2000\n",
      "Train Loss: 0.1663442850112915\n",
      "Epoch 689 of 2000\n",
      "Train Loss: 0.1663566529750824\n",
      "Epoch 690 of 2000\n",
      "Train Loss: 0.16632431745529175\n",
      "Epoch 691 of 2000\n",
      "Train Loss: 0.1662760078907013\n",
      "Epoch 692 of 2000\n",
      "Train Loss: 0.16628026962280273\n",
      "Epoch 693 of 2000\n",
      "Train Loss: 0.16647282242774963\n",
      "Epoch 694 of 2000\n",
      "Train Loss: 0.16640205681324005\n",
      "Epoch 695 of 2000\n",
      "Train Loss: 0.1663006842136383\n",
      "Epoch 696 of 2000\n",
      "Train Loss: 0.1663946807384491\n",
      "Epoch 697 of 2000\n",
      "Train Loss: 0.1663215458393097\n",
      "Epoch 698 of 2000\n",
      "Train Loss: 0.1662960648536682\n",
      "Epoch 699 of 2000\n",
      "Train Loss: 0.16632014513015747\n",
      "Epoch 700 of 2000\n",
      "Train Loss: 0.16634097695350647\n",
      "Epoch 701 of 2000\n",
      "Train Loss: 0.16633062064647675\n",
      "Epoch 702 of 2000\n",
      "Train Loss: 0.1665322184562683\n",
      "Epoch 703 of 2000\n",
      "Train Loss: 0.16632351279258728\n",
      "Epoch 704 of 2000\n",
      "Train Loss: 0.166537344455719\n",
      "Epoch 705 of 2000\n",
      "Train Loss: 0.16630591452121735\n",
      "Epoch 706 of 2000\n",
      "Train Loss: 0.1664922535419464\n",
      "Epoch 707 of 2000\n",
      "Train Loss: 0.16633054614067078\n",
      "Epoch 708 of 2000\n",
      "Train Loss: 0.16645032167434692\n",
      "Epoch 709 of 2000\n",
      "Train Loss: 0.16666512191295624\n",
      "Epoch 710 of 2000\n",
      "Train Loss: 0.16640208661556244\n",
      "Epoch 711 of 2000\n",
      "Train Loss: 0.16660423576831818\n",
      "Epoch 712 of 2000\n",
      "Train Loss: 0.16628722846508026\n",
      "Epoch 713 of 2000\n",
      "Train Loss: 0.16634277999401093\n",
      "Epoch 714 of 2000\n",
      "Train Loss: 0.16660471260547638\n",
      "Epoch 715 of 2000\n",
      "Train Loss: 0.16641482710838318\n",
      "Epoch 716 of 2000\n",
      "Train Loss: 0.16644051671028137\n",
      "Epoch 717 of 2000\n",
      "Train Loss: 0.16632883250713348\n",
      "Epoch 718 of 2000\n",
      "Train Loss: 0.16627824306488037\n",
      "Epoch 719 of 2000\n",
      "Train Loss: 0.16637080907821655\n",
      "Epoch 720 of 2000\n",
      "Train Loss: 0.1663166582584381\n",
      "Epoch 721 of 2000\n",
      "Train Loss: 0.1662815809249878\n",
      "Epoch 722 of 2000\n",
      "Train Loss: 0.16629807651042938\n",
      "Epoch 723 of 2000\n",
      "Train Loss: 0.16645768284797668\n",
      "Epoch 724 of 2000\n",
      "Train Loss: 0.16625957190990448\n",
      "Epoch 725 of 2000\n",
      "Train Loss: 0.16628894209861755\n",
      "Epoch 726 of 2000\n",
      "Train Loss: 0.16626064479351044\n",
      "Epoch 727 of 2000\n",
      "Train Loss: 0.16634033620357513\n",
      "Epoch 728 of 2000\n",
      "Train Loss: 0.16630777716636658\n",
      "Epoch 729 of 2000\n",
      "Train Loss: 0.1662846803665161\n",
      "Epoch 730 of 2000\n",
      "Train Loss: 0.16636285185813904\n",
      "Epoch 731 of 2000\n",
      "Train Loss: 0.16631300747394562\n",
      "Epoch 732 of 2000\n",
      "Train Loss: 0.1663701981306076\n",
      "Epoch 733 of 2000\n",
      "Train Loss: 0.16632983088493347\n",
      "Epoch 734 of 2000\n",
      "Train Loss: 0.1664353907108307\n",
      "Epoch 735 of 2000\n",
      "Train Loss: 0.16637955605983734\n",
      "Epoch 736 of 2000\n",
      "Train Loss: 0.16692698001861572\n",
      "Epoch 737 of 2000\n",
      "Train Loss: 0.16792738437652588\n",
      "Epoch 738 of 2000\n",
      "Train Loss: 0.16810771822929382\n",
      "Epoch 739 of 2000\n",
      "Train Loss: 0.16819128394126892\n",
      "Epoch 740 of 2000\n",
      "Train Loss: 0.16834358870983124\n",
      "Epoch 741 of 2000\n",
      "Train Loss: 0.16678749024868011\n",
      "Epoch 742 of 2000\n",
      "Train Loss: 0.16749553382396698\n",
      "Epoch 743 of 2000\n",
      "Train Loss: 0.1673842817544937\n",
      "Epoch 744 of 2000\n",
      "Train Loss: 0.16724520921707153\n",
      "Epoch 745 of 2000\n",
      "Train Loss: 0.16786137223243713\n",
      "Epoch 746 of 2000\n",
      "Train Loss: 0.16708669066429138\n",
      "Epoch 747 of 2000\n",
      "Train Loss: 0.16740962862968445\n",
      "Epoch 748 of 2000\n",
      "Train Loss: 0.16679565608501434\n",
      "Epoch 749 of 2000\n",
      "Train Loss: 0.16664192080497742\n",
      "Epoch 750 of 2000\n",
      "Train Loss: 0.16715946793556213\n",
      "Epoch 751 of 2000\n",
      "Train Loss: 0.16666707396507263\n",
      "Epoch 752 of 2000\n",
      "Train Loss: 0.16696706414222717\n",
      "Epoch 753 of 2000\n",
      "Train Loss: 0.1667206585407257\n",
      "Epoch 754 of 2000\n",
      "Train Loss: 0.1665641963481903\n",
      "Epoch 755 of 2000\n",
      "Train Loss: 0.1666071116924286\n",
      "Epoch 756 of 2000\n",
      "Train Loss: 0.16634711623191833\n",
      "Epoch 757 of 2000\n",
      "Train Loss: 0.1665526181459427\n",
      "Epoch 758 of 2000\n",
      "Train Loss: 0.16655075550079346\n",
      "Epoch 759 of 2000\n",
      "Train Loss: 0.166582852602005\n",
      "Epoch 760 of 2000\n",
      "Train Loss: 0.16641907393932343\n",
      "Epoch 761 of 2000\n",
      "Train Loss: 0.1663978099822998\n",
      "Epoch 762 of 2000\n",
      "Train Loss: 0.16646628081798553\n",
      "Epoch 763 of 2000\n",
      "Train Loss: 0.16635358333587646\n",
      "Epoch 764 of 2000\n",
      "Train Loss: 0.1663675606250763\n",
      "Epoch 765 of 2000\n",
      "Train Loss: 0.16638214886188507\n",
      "Epoch 766 of 2000\n",
      "Train Loss: 0.16634681820869446\n",
      "Epoch 767 of 2000\n",
      "Train Loss: 0.16629257798194885\n",
      "Epoch 768 of 2000\n",
      "Train Loss: 0.16647353768348694\n",
      "Epoch 769 of 2000\n",
      "Train Loss: 0.16646084189414978\n",
      "Epoch 770 of 2000\n",
      "Train Loss: 0.166366845369339\n",
      "Epoch 771 of 2000\n",
      "Train Loss: 0.16634783148765564\n",
      "Epoch 772 of 2000\n",
      "Train Loss: 0.16649129986763\n",
      "Epoch 773 of 2000\n",
      "Train Loss: 0.16635921597480774\n",
      "Epoch 774 of 2000\n",
      "Train Loss: 0.1664019674062729\n",
      "Epoch 775 of 2000\n",
      "Train Loss: 0.16630327701568604\n",
      "Epoch 776 of 2000\n",
      "Train Loss: 0.16627618670463562\n",
      "Epoch 777 of 2000\n",
      "Train Loss: 0.16638939082622528\n",
      "Epoch 778 of 2000\n",
      "Train Loss: 0.1664465218782425\n",
      "Epoch 779 of 2000\n",
      "Train Loss: 0.16627073287963867\n",
      "Epoch 780 of 2000\n",
      "Train Loss: 0.16639354825019836\n",
      "Epoch 781 of 2000\n",
      "Train Loss: 0.16641786694526672\n",
      "Epoch 782 of 2000\n",
      "Train Loss: 0.16633155941963196\n",
      "Epoch 783 of 2000\n",
      "Train Loss: 0.16633453965187073\n",
      "Epoch 784 of 2000\n",
      "Train Loss: 0.1664009988307953\n",
      "Epoch 785 of 2000\n",
      "Train Loss: 0.16635943949222565\n",
      "Epoch 786 of 2000\n",
      "Train Loss: 0.16633066534996033\n",
      "Epoch 787 of 2000\n",
      "Train Loss: 0.16642501950263977\n",
      "Epoch 788 of 2000\n",
      "Train Loss: 0.16639424860477448\n",
      "Epoch 789 of 2000\n",
      "Train Loss: 0.16646742820739746\n",
      "Epoch 790 of 2000\n",
      "Train Loss: 0.16626858711242676\n",
      "Epoch 791 of 2000\n",
      "Train Loss: 0.16631735861301422\n",
      "Epoch 792 of 2000\n",
      "Train Loss: 0.1663917750120163\n",
      "Epoch 793 of 2000\n",
      "Train Loss: 0.16633202135562897\n",
      "Epoch 794 of 2000\n",
      "Train Loss: 0.16631421446800232\n",
      "Epoch 795 of 2000\n",
      "Train Loss: 0.16663160920143127\n",
      "Epoch 796 of 2000\n",
      "Train Loss: 0.1663587987422943\n",
      "Epoch 797 of 2000\n",
      "Train Loss: 0.1663840264081955\n",
      "Epoch 798 of 2000\n",
      "Train Loss: 0.16663339734077454\n",
      "Epoch 799 of 2000\n",
      "Train Loss: 0.16674597561359406\n",
      "Epoch 800 of 2000\n",
      "Train Loss: 0.1665985882282257\n",
      "Epoch 801 of 2000\n",
      "Train Loss: 0.16629019379615784\n",
      "Epoch 802 of 2000\n",
      "Train Loss: 0.16641423106193542\n",
      "Epoch 803 of 2000\n",
      "Train Loss: 0.16640743613243103\n",
      "Epoch 804 of 2000\n",
      "Train Loss: 0.16634522378444672\n",
      "Epoch 805 of 2000\n",
      "Train Loss: 0.1664671003818512\n",
      "Epoch 806 of 2000\n",
      "Train Loss: 0.16668060421943665\n",
      "Epoch 807 of 2000\n",
      "Train Loss: 0.16656215488910675\n",
      "Epoch 808 of 2000\n",
      "Train Loss: 0.16634081304073334\n",
      "Epoch 809 of 2000\n",
      "Train Loss: 0.16635096073150635\n",
      "Epoch 810 of 2000\n",
      "Train Loss: 0.1666043996810913\n",
      "Epoch 811 of 2000\n",
      "Train Loss: 0.16640768945217133\n",
      "Epoch 812 of 2000\n",
      "Train Loss: 0.16643090546131134\n",
      "Epoch 813 of 2000\n",
      "Train Loss: 0.16647525131702423\n",
      "Epoch 814 of 2000\n",
      "Train Loss: 0.16662490367889404\n",
      "Epoch 815 of 2000\n",
      "Train Loss: 0.16638436913490295\n",
      "Epoch 816 of 2000\n",
      "Train Loss: 0.16633689403533936\n",
      "Epoch 817 of 2000\n",
      "Train Loss: 0.1664164513349533\n",
      "Epoch 818 of 2000\n",
      "Train Loss: 0.16629736125469208\n",
      "Epoch 819 of 2000\n",
      "Train Loss: 0.16642320156097412\n",
      "Epoch 820 of 2000\n",
      "Train Loss: 0.16636162996292114\n",
      "Epoch 821 of 2000\n",
      "Train Loss: 0.16649989783763885\n",
      "Epoch 822 of 2000\n",
      "Train Loss: 0.1664241999387741\n",
      "Epoch 823 of 2000\n",
      "Train Loss: 0.16627545654773712\n",
      "Epoch 824 of 2000\n",
      "Train Loss: 0.1663883775472641\n",
      "Epoch 825 of 2000\n",
      "Train Loss: 0.16653263568878174\n",
      "Epoch 826 of 2000\n",
      "Train Loss: 0.16630032658576965\n",
      "Epoch 827 of 2000\n",
      "Train Loss: 0.16650456190109253\n",
      "Epoch 828 of 2000\n",
      "Train Loss: 0.16640010476112366\n",
      "Epoch 829 of 2000\n",
      "Train Loss: 0.16627949476242065\n",
      "Epoch 830 of 2000\n",
      "Train Loss: 0.16638734936714172\n",
      "Epoch 831 of 2000\n",
      "Train Loss: 0.16633263230323792\n",
      "Epoch 832 of 2000\n",
      "Train Loss: 0.166360542178154\n",
      "Epoch 833 of 2000\n",
      "Train Loss: 0.1663837879896164\n",
      "Epoch 834 of 2000\n",
      "Train Loss: 0.16625860333442688\n",
      "Epoch 835 of 2000\n",
      "Train Loss: 0.1663585901260376\n",
      "Epoch 836 of 2000\n",
      "Train Loss: 0.16651847958564758\n",
      "Epoch 837 of 2000\n",
      "Train Loss: 0.16627034544944763\n",
      "Epoch 838 of 2000\n",
      "Train Loss: 0.16651040315628052\n",
      "Epoch 839 of 2000\n",
      "Train Loss: 0.1662556380033493\n",
      "Epoch 840 of 2000\n",
      "Train Loss: 0.1664322316646576\n",
      "Epoch 841 of 2000\n",
      "Train Loss: 0.1666194349527359\n",
      "Epoch 842 of 2000\n",
      "Train Loss: 0.16644136607646942\n",
      "Epoch 843 of 2000\n",
      "Train Loss: 0.16649091243743896\n",
      "Epoch 844 of 2000\n",
      "Train Loss: 0.16651970148086548\n",
      "Epoch 845 of 2000\n",
      "Train Loss: 0.1662643849849701\n",
      "Epoch 846 of 2000\n",
      "Train Loss: 0.16666972637176514\n",
      "Epoch 847 of 2000\n",
      "Train Loss: 0.16646170616149902\n",
      "Epoch 848 of 2000\n",
      "Train Loss: 0.16650433838367462\n",
      "Epoch 849 of 2000\n",
      "Train Loss: 0.1663217693567276\n",
      "Epoch 850 of 2000\n",
      "Train Loss: 0.1663678139448166\n",
      "Epoch 851 of 2000\n",
      "Train Loss: 0.16685977578163147\n",
      "Epoch 852 of 2000\n",
      "Train Loss: 0.16652287542819977\n",
      "Epoch 853 of 2000\n",
      "Train Loss: 0.166647270321846\n",
      "Epoch 854 of 2000\n",
      "Train Loss: 0.1664574146270752\n",
      "Epoch 855 of 2000\n",
      "Train Loss: 0.16627980768680573\n",
      "Epoch 856 of 2000\n",
      "Train Loss: 0.16649529337882996\n",
      "Epoch 857 of 2000\n",
      "Train Loss: 0.16642796993255615\n",
      "Epoch 858 of 2000\n",
      "Train Loss: 0.16635389626026154\n",
      "Epoch 859 of 2000\n",
      "Train Loss: 0.16641589999198914\n",
      "Epoch 860 of 2000\n",
      "Train Loss: 0.16652707755565643\n",
      "Epoch 861 of 2000\n",
      "Train Loss: 0.16648399829864502\n",
      "Epoch 862 of 2000\n",
      "Train Loss: 0.16633982956409454\n",
      "Epoch 863 of 2000\n",
      "Train Loss: 0.16632458567619324\n",
      "Epoch 864 of 2000\n",
      "Train Loss: 0.16625498235225677\n",
      "Epoch 865 of 2000\n",
      "Train Loss: 0.16627958416938782\n",
      "Epoch 866 of 2000\n",
      "Train Loss: 0.16634990274906158\n",
      "Epoch 867 of 2000\n",
      "Train Loss: 0.16632011532783508\n",
      "Epoch 868 of 2000\n",
      "Train Loss: 0.16630584001541138\n",
      "Epoch 869 of 2000\n",
      "Train Loss: 0.16628627479076385\n",
      "Epoch 870 of 2000\n",
      "Train Loss: 0.1664011925458908\n",
      "Epoch 871 of 2000\n",
      "Train Loss: 0.16649110615253448\n",
      "Epoch 872 of 2000\n",
      "Train Loss: 0.16638077795505524\n",
      "Epoch 873 of 2000\n",
      "Train Loss: 0.16626890003681183\n",
      "Epoch 874 of 2000\n",
      "Train Loss: 0.16625770926475525\n",
      "Epoch 875 of 2000\n",
      "Train Loss: 0.16631318628787994\n",
      "Epoch 876 of 2000\n",
      "Train Loss: 0.1663733720779419\n",
      "Epoch 877 of 2000\n",
      "Train Loss: 0.1665019690990448\n",
      "Epoch 878 of 2000\n",
      "Train Loss: 0.16633394360542297\n",
      "Epoch 879 of 2000\n",
      "Train Loss: 0.16654960811138153\n",
      "Epoch 880 of 2000\n",
      "Train Loss: 0.16654135286808014\n",
      "Epoch 881 of 2000\n",
      "Train Loss: 0.1667695939540863\n",
      "Epoch 882 of 2000\n",
      "Train Loss: 0.16653349995613098\n",
      "Epoch 883 of 2000\n",
      "Train Loss: 0.16632208228111267\n",
      "Epoch 884 of 2000\n",
      "Train Loss: 0.16643735766410828\n",
      "Epoch 885 of 2000\n",
      "Train Loss: 0.16639800369739532\n",
      "Epoch 886 of 2000\n",
      "Train Loss: 0.1665358543395996\n",
      "Epoch 887 of 2000\n",
      "Train Loss: 0.16625159978866577\n",
      "Epoch 888 of 2000\n",
      "Train Loss: 0.16652409732341766\n",
      "Epoch 889 of 2000\n",
      "Train Loss: 0.16681510210037231\n",
      "Epoch 890 of 2000\n",
      "Train Loss: 0.16690829396247864\n",
      "Epoch 891 of 2000\n",
      "Train Loss: 0.16646736860275269\n",
      "Epoch 892 of 2000\n",
      "Train Loss: 0.16629329323768616\n",
      "Epoch 893 of 2000\n",
      "Train Loss: 0.1664915680885315\n",
      "Epoch 894 of 2000\n",
      "Train Loss: 0.16657383739948273\n",
      "Epoch 895 of 2000\n",
      "Train Loss: 0.16635490953922272\n",
      "Epoch 896 of 2000\n",
      "Train Loss: 0.16623707115650177\n",
      "Epoch 897 of 2000\n",
      "Train Loss: 0.16648617386817932\n",
      "Epoch 898 of 2000\n",
      "Train Loss: 0.16665825247764587\n",
      "Epoch 899 of 2000\n",
      "Train Loss: 0.16678449511528015\n",
      "Epoch 900 of 2000\n",
      "Train Loss: 3.1329216957092285\n",
      "Epoch 901 of 2000\n",
      "Train Loss: 1.3263510465621948\n",
      "Epoch 902 of 2000\n",
      "Train Loss: 0.8197481036186218\n",
      "Epoch 903 of 2000\n",
      "Train Loss: 0.5419312715530396\n",
      "Epoch 904 of 2000\n",
      "Train Loss: 0.49015969038009644\n",
      "Epoch 905 of 2000\n",
      "Train Loss: 0.4338518977165222\n",
      "Epoch 906 of 2000\n",
      "Train Loss: 0.46200889348983765\n",
      "Epoch 907 of 2000\n",
      "Train Loss: 0.4336649477481842\n",
      "Epoch 908 of 2000\n",
      "Train Loss: 0.44430065155029297\n",
      "Epoch 909 of 2000\n",
      "Train Loss: 0.4465177655220032\n",
      "Epoch 910 of 2000\n",
      "Train Loss: 0.44400423765182495\n",
      "Epoch 911 of 2000\n",
      "Train Loss: 0.4392811357975006\n",
      "Epoch 912 of 2000\n",
      "Train Loss: 0.42883649468421936\n",
      "Epoch 913 of 2000\n",
      "Train Loss: 0.4186251163482666\n",
      "Epoch 914 of 2000\n",
      "Train Loss: 0.4078303277492523\n",
      "Epoch 915 of 2000\n",
      "Train Loss: 0.4012400507926941\n",
      "Epoch 916 of 2000\n",
      "Train Loss: 0.3950631022453308\n",
      "Epoch 917 of 2000\n",
      "Train Loss: 0.38667377829551697\n",
      "Epoch 918 of 2000\n",
      "Train Loss: 0.3834512233734131\n",
      "Epoch 919 of 2000\n",
      "Train Loss: 0.37416768074035645\n",
      "Epoch 920 of 2000\n",
      "Train Loss: 0.3756101727485657\n",
      "Epoch 921 of 2000\n",
      "Train Loss: 0.3659994602203369\n",
      "Epoch 922 of 2000\n",
      "Train Loss: 0.3613610565662384\n",
      "Epoch 923 of 2000\n",
      "Train Loss: 0.3577587306499481\n",
      "Epoch 924 of 2000\n",
      "Train Loss: 0.3430604636669159\n",
      "Epoch 925 of 2000\n",
      "Train Loss: 0.34045711159706116\n",
      "Epoch 926 of 2000\n",
      "Train Loss: 0.33151349425315857\n",
      "Epoch 927 of 2000\n",
      "Train Loss: 0.3222356140613556\n",
      "Epoch 928 of 2000\n",
      "Train Loss: 0.3256777822971344\n",
      "Epoch 929 of 2000\n",
      "Train Loss: 0.3106454908847809\n",
      "Epoch 930 of 2000\n",
      "Train Loss: 0.30247658491134644\n",
      "Epoch 931 of 2000\n",
      "Train Loss: 0.29887324571609497\n",
      "Epoch 932 of 2000\n",
      "Train Loss: 0.2712753117084503\n",
      "Epoch 933 of 2000\n",
      "Train Loss: 0.260491281747818\n",
      "Epoch 934 of 2000\n",
      "Train Loss: 0.2773726284503937\n",
      "Epoch 935 of 2000\n",
      "Train Loss: 0.2585154175758362\n",
      "Epoch 936 of 2000\n",
      "Train Loss: 0.4639975130558014\n",
      "Epoch 937 of 2000\n",
      "Train Loss: 0.25433188676834106\n",
      "Epoch 938 of 2000\n",
      "Train Loss: 0.3055611550807953\n",
      "Epoch 939 of 2000\n",
      "Train Loss: 0.30423155426979065\n",
      "Epoch 940 of 2000\n",
      "Train Loss: 0.2808452248573303\n",
      "Epoch 941 of 2000\n",
      "Train Loss: 0.2633681893348694\n",
      "Epoch 942 of 2000\n",
      "Train Loss: 0.25657886266708374\n",
      "Epoch 943 of 2000\n",
      "Train Loss: 0.2524833083152771\n",
      "Epoch 944 of 2000\n",
      "Train Loss: 0.2530899941921234\n",
      "Epoch 945 of 2000\n",
      "Train Loss: 0.24996308982372284\n",
      "Epoch 946 of 2000\n",
      "Train Loss: 0.253852903842926\n",
      "Epoch 947 of 2000\n",
      "Train Loss: 0.22480762004852295\n",
      "Epoch 948 of 2000\n",
      "Train Loss: 0.2422032654285431\n",
      "Epoch 949 of 2000\n",
      "Train Loss: 0.239291712641716\n",
      "Epoch 950 of 2000\n",
      "Train Loss: 0.2185421735048294\n",
      "Epoch 951 of 2000\n",
      "Train Loss: 0.2229098528623581\n",
      "Epoch 952 of 2000\n",
      "Train Loss: 0.2273779660463333\n",
      "Epoch 953 of 2000\n",
      "Train Loss: 0.2152075320482254\n",
      "Epoch 954 of 2000\n",
      "Train Loss: 0.2153739482164383\n",
      "Epoch 955 of 2000\n",
      "Train Loss: 0.21726688742637634\n",
      "Epoch 956 of 2000\n",
      "Train Loss: 0.21117940545082092\n",
      "Epoch 957 of 2000\n",
      "Train Loss: 0.21206839382648468\n",
      "Epoch 958 of 2000\n",
      "Train Loss: 0.20248885452747345\n",
      "Epoch 959 of 2000\n",
      "Train Loss: 0.20606212317943573\n",
      "Epoch 960 of 2000\n",
      "Train Loss: 0.20700936019420624\n",
      "Epoch 961 of 2000\n",
      "Train Loss: 0.19789692759513855\n",
      "Epoch 962 of 2000\n",
      "Train Loss: 0.1993236094713211\n",
      "Epoch 963 of 2000\n",
      "Train Loss: 0.19898469746112823\n",
      "Epoch 964 of 2000\n",
      "Train Loss: 0.19839362800121307\n",
      "Epoch 965 of 2000\n",
      "Train Loss: 0.19643858075141907\n",
      "Epoch 966 of 2000\n",
      "Train Loss: 0.19256281852722168\n",
      "Epoch 967 of 2000\n",
      "Train Loss: 0.19303160905838013\n",
      "Epoch 968 of 2000\n",
      "Train Loss: 0.19034099578857422\n",
      "Epoch 969 of 2000\n",
      "Train Loss: 0.19151076674461365\n",
      "Epoch 970 of 2000\n",
      "Train Loss: 0.19061394035816193\n",
      "Epoch 971 of 2000\n",
      "Train Loss: 0.19098712503910065\n",
      "Epoch 972 of 2000\n",
      "Train Loss: 0.18782171607017517\n",
      "Epoch 973 of 2000\n",
      "Train Loss: 0.189815491437912\n",
      "Epoch 974 of 2000\n",
      "Train Loss: 0.18819695711135864\n",
      "Epoch 975 of 2000\n",
      "Train Loss: 0.18361636996269226\n",
      "Epoch 976 of 2000\n",
      "Train Loss: 0.18626686930656433\n",
      "Epoch 977 of 2000\n",
      "Train Loss: 0.18936868011951447\n",
      "Epoch 978 of 2000\n",
      "Train Loss: 0.18234050273895264\n",
      "Epoch 979 of 2000\n",
      "Train Loss: 0.1826021373271942\n",
      "Epoch 980 of 2000\n",
      "Train Loss: 0.18319258093833923\n",
      "Epoch 981 of 2000\n",
      "Train Loss: 0.18358998000621796\n",
      "Epoch 982 of 2000\n",
      "Train Loss: 0.18031108379364014\n",
      "Epoch 983 of 2000\n",
      "Train Loss: 0.181233212351799\n",
      "Epoch 984 of 2000\n",
      "Train Loss: 0.18230193853378296\n",
      "Epoch 985 of 2000\n",
      "Train Loss: 0.17857417464256287\n",
      "Epoch 986 of 2000\n",
      "Train Loss: 0.17919203639030457\n",
      "Epoch 987 of 2000\n",
      "Train Loss: 0.17943966388702393\n",
      "Epoch 988 of 2000\n",
      "Train Loss: 0.17931893467903137\n",
      "Epoch 989 of 2000\n",
      "Train Loss: 0.17791447043418884\n",
      "Epoch 990 of 2000\n",
      "Train Loss: 0.17878547310829163\n",
      "Epoch 991 of 2000\n",
      "Train Loss: 0.1779787540435791\n",
      "Epoch 992 of 2000\n",
      "Train Loss: 0.17695382237434387\n",
      "Epoch 993 of 2000\n",
      "Train Loss: 0.17712804675102234\n",
      "Epoch 994 of 2000\n",
      "Train Loss: 0.17734013497829437\n",
      "Epoch 995 of 2000\n",
      "Train Loss: 0.1776239573955536\n",
      "Epoch 996 of 2000\n",
      "Train Loss: 0.17578692734241486\n",
      "Epoch 997 of 2000\n",
      "Train Loss: 0.1781717985868454\n",
      "Epoch 998 of 2000\n",
      "Train Loss: 0.17599092423915863\n",
      "Epoch 999 of 2000\n",
      "Train Loss: 0.17486782371997833\n",
      "Epoch 1000 of 2000\n",
      "Train Loss: 0.1749512255191803\n",
      "Epoch 1001 of 2000\n",
      "Train Loss: 0.1755780428647995\n",
      "Epoch 1002 of 2000\n",
      "Train Loss: 0.17419078946113586\n",
      "Epoch 1003 of 2000\n",
      "Train Loss: 0.17313580214977264\n",
      "Epoch 1004 of 2000\n",
      "Train Loss: 0.17327719926834106\n",
      "Epoch 1005 of 2000\n",
      "Train Loss: 0.17362777888774872\n",
      "Epoch 1006 of 2000\n",
      "Train Loss: 0.1725262850522995\n",
      "Epoch 1007 of 2000\n",
      "Train Loss: 0.17209234833717346\n",
      "Epoch 1008 of 2000\n",
      "Train Loss: 0.17255008220672607\n",
      "Epoch 1009 of 2000\n",
      "Train Loss: 0.17207816243171692\n",
      "Epoch 1010 of 2000\n",
      "Train Loss: 0.17206552624702454\n",
      "Epoch 1011 of 2000\n",
      "Train Loss: 0.17088885605335236\n",
      "Epoch 1012 of 2000\n",
      "Train Loss: 0.17089422047138214\n",
      "Epoch 1013 of 2000\n",
      "Train Loss: 0.17072780430316925\n",
      "Epoch 1014 of 2000\n",
      "Train Loss: 0.17009712755680084\n",
      "Epoch 1015 of 2000\n",
      "Train Loss: 0.17006435990333557\n",
      "Epoch 1016 of 2000\n",
      "Train Loss: 0.17007122933864594\n",
      "Epoch 1017 of 2000\n",
      "Train Loss: 0.169804185628891\n",
      "Epoch 1018 of 2000\n",
      "Train Loss: 0.16939619183540344\n",
      "Epoch 1019 of 2000\n",
      "Train Loss: 0.17044797539710999\n",
      "Epoch 1020 of 2000\n",
      "Train Loss: 0.16893668472766876\n",
      "Epoch 1021 of 2000\n",
      "Train Loss: 0.16913571953773499\n",
      "Epoch 1022 of 2000\n",
      "Train Loss: 0.16900500655174255\n",
      "Epoch 1023 of 2000\n",
      "Train Loss: 0.16910775005817413\n",
      "Epoch 1024 of 2000\n",
      "Train Loss: 0.16888220608234406\n",
      "Epoch 1025 of 2000\n",
      "Train Loss: 0.1686730831861496\n",
      "Epoch 1026 of 2000\n",
      "Train Loss: 0.16842696070671082\n",
      "Epoch 1027 of 2000\n",
      "Train Loss: 0.17019306123256683\n",
      "Epoch 1028 of 2000\n",
      "Train Loss: 0.16850677132606506\n",
      "Epoch 1029 of 2000\n",
      "Train Loss: 0.16883039474487305\n",
      "Epoch 1030 of 2000\n",
      "Train Loss: 0.1687972992658615\n",
      "Epoch 1031 of 2000\n",
      "Train Loss: 0.16829067468643188\n",
      "Epoch 1032 of 2000\n",
      "Train Loss: 0.16846473515033722\n",
      "Epoch 1033 of 2000\n",
      "Train Loss: 0.16873258352279663\n",
      "Epoch 1034 of 2000\n",
      "Train Loss: 0.1688946932554245\n",
      "Epoch 1035 of 2000\n",
      "Train Loss: 0.16830530762672424\n",
      "Epoch 1036 of 2000\n",
      "Train Loss: 0.1680140346288681\n",
      "Epoch 1037 of 2000\n",
      "Train Loss: 0.16821597516536713\n",
      "Epoch 1038 of 2000\n",
      "Train Loss: 0.16875454783439636\n",
      "Epoch 1039 of 2000\n",
      "Train Loss: 0.16820630431175232\n",
      "Epoch 1040 of 2000\n",
      "Train Loss: 0.16780152916908264\n",
      "Epoch 1041 of 2000\n",
      "Train Loss: 0.16805043816566467\n",
      "Epoch 1042 of 2000\n",
      "Train Loss: 0.16779983043670654\n",
      "Epoch 1043 of 2000\n",
      "Train Loss: 0.16779203712940216\n",
      "Epoch 1044 of 2000\n",
      "Train Loss: 0.167667418718338\n",
      "Epoch 1045 of 2000\n",
      "Train Loss: 0.16777460277080536\n",
      "Epoch 1046 of 2000\n",
      "Train Loss: 0.16848373413085938\n",
      "Epoch 1047 of 2000\n",
      "Train Loss: 0.1682761013507843\n",
      "Epoch 1048 of 2000\n",
      "Train Loss: 0.16767188906669617\n",
      "Epoch 1049 of 2000\n",
      "Train Loss: 0.16811636090278625\n",
      "Epoch 1050 of 2000\n",
      "Train Loss: 0.1677904576063156\n",
      "Epoch 1051 of 2000\n",
      "Train Loss: 0.16852323710918427\n",
      "Epoch 1052 of 2000\n",
      "Train Loss: 0.16877290606498718\n",
      "Epoch 1053 of 2000\n",
      "Train Loss: 0.16769234836101532\n",
      "Epoch 1054 of 2000\n",
      "Train Loss: 0.1680537462234497\n",
      "Epoch 1055 of 2000\n",
      "Train Loss: 0.16908594965934753\n",
      "Epoch 1056 of 2000\n",
      "Train Loss: 0.16787764430046082\n",
      "Epoch 1057 of 2000\n",
      "Train Loss: 0.1677585393190384\n",
      "Epoch 1058 of 2000\n",
      "Train Loss: 0.16824860870838165\n",
      "Epoch 1059 of 2000\n",
      "Train Loss: 0.16769449412822723\n",
      "Epoch 1060 of 2000\n",
      "Train Loss: 0.1673356592655182\n",
      "Epoch 1061 of 2000\n",
      "Train Loss: 0.16802026331424713\n",
      "Epoch 1062 of 2000\n",
      "Train Loss: 0.16813388466835022\n",
      "Epoch 1063 of 2000\n",
      "Train Loss: 0.1671694666147232\n",
      "Epoch 1064 of 2000\n",
      "Train Loss: 0.16734418272972107\n",
      "Epoch 1065 of 2000\n",
      "Train Loss: 0.16771236062049866\n",
      "Epoch 1066 of 2000\n",
      "Train Loss: 0.16723445057868958\n",
      "Epoch 1067 of 2000\n",
      "Train Loss: 0.16722211241722107\n",
      "Epoch 1068 of 2000\n",
      "Train Loss: 0.16745734214782715\n",
      "Epoch 1069 of 2000\n",
      "Train Loss: 0.1672365516424179\n",
      "Epoch 1070 of 2000\n",
      "Train Loss: 0.16724331676959991\n",
      "Epoch 1071 of 2000\n",
      "Train Loss: 0.16724887490272522\n",
      "Epoch 1072 of 2000\n",
      "Train Loss: 0.16740143299102783\n",
      "Epoch 1073 of 2000\n",
      "Train Loss: 0.16769029200077057\n",
      "Epoch 1074 of 2000\n",
      "Train Loss: 0.16706331074237823\n",
      "Epoch 1075 of 2000\n",
      "Train Loss: 0.1671798974275589\n",
      "Epoch 1076 of 2000\n",
      "Train Loss: 0.16737216711044312\n",
      "Epoch 1077 of 2000\n",
      "Train Loss: 0.16718891263008118\n",
      "Epoch 1078 of 2000\n",
      "Train Loss: 0.16703744232654572\n",
      "Epoch 1079 of 2000\n",
      "Train Loss: 0.16700243949890137\n",
      "Epoch 1080 of 2000\n",
      "Train Loss: 0.16872592270374298\n",
      "Epoch 1081 of 2000\n",
      "Train Loss: 0.1685761660337448\n",
      "Epoch 1082 of 2000\n",
      "Train Loss: 0.16720044612884521\n",
      "Epoch 1083 of 2000\n",
      "Train Loss: 0.16791538894176483\n",
      "Epoch 1084 of 2000\n",
      "Train Loss: 0.16756194829940796\n",
      "Epoch 1085 of 2000\n",
      "Train Loss: 0.16715823113918304\n",
      "Epoch 1086 of 2000\n",
      "Train Loss: 0.1678866744041443\n",
      "Epoch 1087 of 2000\n",
      "Train Loss: 0.16706565022468567\n",
      "Epoch 1088 of 2000\n",
      "Train Loss: 0.1678648293018341\n",
      "Epoch 1089 of 2000\n",
      "Train Loss: 0.16808298230171204\n",
      "Epoch 1090 of 2000\n",
      "Train Loss: 0.1674666702747345\n",
      "Epoch 1091 of 2000\n",
      "Train Loss: 0.1673053652048111\n",
      "Epoch 1092 of 2000\n",
      "Train Loss: 0.1674615889787674\n",
      "Epoch 1093 of 2000\n",
      "Train Loss: 0.16760806739330292\n",
      "Epoch 1094 of 2000\n",
      "Train Loss: 0.16672664880752563\n",
      "Epoch 1095 of 2000\n",
      "Train Loss: 0.16690900921821594\n",
      "Epoch 1096 of 2000\n",
      "Train Loss: 0.16777269542217255\n",
      "Epoch 1097 of 2000\n",
      "Train Loss: 0.17030823230743408\n",
      "Epoch 1098 of 2000\n",
      "Train Loss: 0.16900479793548584\n",
      "Epoch 1099 of 2000\n",
      "Train Loss: 0.17070026695728302\n",
      "Epoch 1100 of 2000\n",
      "Train Loss: 0.16862143576145172\n",
      "Epoch 1101 of 2000\n",
      "Train Loss: 0.16799302399158478\n",
      "Epoch 1102 of 2000\n",
      "Train Loss: 0.16796286404132843\n",
      "Epoch 1103 of 2000\n",
      "Train Loss: 0.16779504716396332\n",
      "Epoch 1104 of 2000\n",
      "Train Loss: 0.16882863640785217\n",
      "Epoch 1105 of 2000\n",
      "Train Loss: 0.1676807999610901\n",
      "Epoch 1106 of 2000\n",
      "Train Loss: 0.16879060864448547\n",
      "Epoch 1107 of 2000\n",
      "Train Loss: 0.1686416119337082\n",
      "Epoch 1108 of 2000\n",
      "Train Loss: 0.1672772467136383\n",
      "Epoch 1109 of 2000\n",
      "Train Loss: 0.16753031313419342\n",
      "Epoch 1110 of 2000\n",
      "Train Loss: 0.1677566021680832\n",
      "Epoch 1111 of 2000\n",
      "Train Loss: 0.16816756129264832\n",
      "Epoch 1112 of 2000\n",
      "Train Loss: 0.16740480065345764\n",
      "Epoch 1113 of 2000\n",
      "Train Loss: 0.16708996891975403\n",
      "Epoch 1114 of 2000\n",
      "Train Loss: 0.16701486706733704\n",
      "Epoch 1115 of 2000\n",
      "Train Loss: 0.1673111766576767\n",
      "Epoch 1116 of 2000\n",
      "Train Loss: 0.16686956584453583\n",
      "Epoch 1117 of 2000\n",
      "Train Loss: 0.16696783900260925\n",
      "Epoch 1118 of 2000\n",
      "Train Loss: 0.16678623855113983\n",
      "Epoch 1119 of 2000\n",
      "Train Loss: 0.16694223880767822\n",
      "Epoch 1120 of 2000\n",
      "Train Loss: 0.16682922840118408\n",
      "Epoch 1121 of 2000\n",
      "Train Loss: 0.16677597165107727\n",
      "Epoch 1122 of 2000\n",
      "Train Loss: 0.16687041521072388\n",
      "Epoch 1123 of 2000\n",
      "Train Loss: 0.16670410335063934\n",
      "Epoch 1124 of 2000\n",
      "Train Loss: 0.16672007739543915\n",
      "Epoch 1125 of 2000\n",
      "Train Loss: 0.16678449511528015\n",
      "Epoch 1126 of 2000\n",
      "Train Loss: 0.16678282618522644\n",
      "Epoch 1127 of 2000\n",
      "Train Loss: 0.1667046993970871\n",
      "Epoch 1128 of 2000\n",
      "Train Loss: 0.16687984764575958\n",
      "Epoch 1129 of 2000\n",
      "Train Loss: 0.16698889434337616\n",
      "Epoch 1130 of 2000\n",
      "Train Loss: 0.16656778752803802\n",
      "Epoch 1131 of 2000\n",
      "Train Loss: 0.1669258326292038\n",
      "Epoch 1132 of 2000\n",
      "Train Loss: 0.16670243442058563\n",
      "Epoch 1133 of 2000\n",
      "Train Loss: 0.1667689085006714\n",
      "Epoch 1134 of 2000\n",
      "Train Loss: 0.16682887077331543\n",
      "Epoch 1135 of 2000\n",
      "Train Loss: 0.16668011248111725\n",
      "Epoch 1136 of 2000\n",
      "Train Loss: 0.1666618138551712\n",
      "Epoch 1137 of 2000\n",
      "Train Loss: 0.1665947437286377\n",
      "Epoch 1138 of 2000\n",
      "Train Loss: 0.1666659712791443\n",
      "Epoch 1139 of 2000\n",
      "Train Loss: 0.16658714413642883\n",
      "Epoch 1140 of 2000\n",
      "Train Loss: 0.16697700321674347\n",
      "Epoch 1141 of 2000\n",
      "Train Loss: 0.1667667180299759\n",
      "Epoch 1142 of 2000\n",
      "Train Loss: 0.16649004817008972\n",
      "Epoch 1143 of 2000\n",
      "Train Loss: 0.1670219749212265\n",
      "Epoch 1144 of 2000\n",
      "Train Loss: 0.16675366461277008\n",
      "Epoch 1145 of 2000\n",
      "Train Loss: 0.16686920821666718\n",
      "Epoch 1146 of 2000\n",
      "Train Loss: 0.16681066155433655\n",
      "Epoch 1147 of 2000\n",
      "Train Loss: 0.1667684018611908\n",
      "Epoch 1148 of 2000\n",
      "Train Loss: 0.1668701469898224\n",
      "Epoch 1149 of 2000\n",
      "Train Loss: 0.16671137511730194\n",
      "Epoch 1150 of 2000\n",
      "Train Loss: 0.1668734848499298\n",
      "Epoch 1151 of 2000\n",
      "Train Loss: 0.16681694984436035\n",
      "Epoch 1152 of 2000\n",
      "Train Loss: 0.16749480366706848\n",
      "Epoch 1153 of 2000\n",
      "Train Loss: 0.16645804047584534\n",
      "Epoch 1154 of 2000\n",
      "Train Loss: 0.16784828901290894\n",
      "Epoch 1155 of 2000\n",
      "Train Loss: 0.16701170802116394\n",
      "Epoch 1156 of 2000\n",
      "Train Loss: 0.16668425500392914\n",
      "Epoch 1157 of 2000\n",
      "Train Loss: 0.16746871173381805\n",
      "Epoch 1158 of 2000\n",
      "Train Loss: 0.1668885499238968\n",
      "Epoch 1159 of 2000\n",
      "Train Loss: 0.16727545857429504\n",
      "Epoch 1160 of 2000\n",
      "Train Loss: 0.16682341694831848\n",
      "Epoch 1161 of 2000\n",
      "Train Loss: 0.16663400828838348\n",
      "Epoch 1162 of 2000\n",
      "Train Loss: 0.16714349389076233\n",
      "Epoch 1163 of 2000\n",
      "Train Loss: 0.1668725311756134\n",
      "Epoch 1164 of 2000\n",
      "Train Loss: 0.16665834188461304\n",
      "Epoch 1165 of 2000\n",
      "Train Loss: 0.16696475446224213\n",
      "Epoch 1166 of 2000\n",
      "Train Loss: 0.16736295819282532\n",
      "Epoch 1167 of 2000\n",
      "Train Loss: 0.16670402884483337\n",
      "Epoch 1168 of 2000\n",
      "Train Loss: 0.166819229722023\n",
      "Epoch 1169 of 2000\n",
      "Train Loss: 0.16657516360282898\n",
      "Epoch 1170 of 2000\n",
      "Train Loss: 0.16681542992591858\n",
      "Epoch 1171 of 2000\n",
      "Train Loss: 0.1667267382144928\n",
      "Epoch 1172 of 2000\n",
      "Train Loss: 0.16664347052574158\n",
      "Epoch 1173 of 2000\n",
      "Train Loss: 0.16706454753875732\n",
      "Epoch 1174 of 2000\n",
      "Train Loss: 0.16674773395061493\n",
      "Epoch 1175 of 2000\n",
      "Train Loss: 0.16678257286548615\n",
      "Epoch 1176 of 2000\n",
      "Train Loss: 0.16719678044319153\n",
      "Epoch 1177 of 2000\n",
      "Train Loss: 0.16676108539104462\n",
      "Epoch 1178 of 2000\n",
      "Train Loss: 0.16663530468940735\n",
      "Epoch 1179 of 2000\n",
      "Train Loss: 0.16697216033935547\n",
      "Epoch 1180 of 2000\n",
      "Train Loss: 0.16695384681224823\n",
      "Epoch 1181 of 2000\n",
      "Train Loss: 0.1666247695684433\n",
      "Epoch 1182 of 2000\n",
      "Train Loss: 0.16669276356697083\n",
      "Epoch 1183 of 2000\n",
      "Train Loss: 0.16671445965766907\n",
      "Epoch 1184 of 2000\n",
      "Train Loss: 0.1668001413345337\n",
      "Epoch 1185 of 2000\n",
      "Train Loss: 0.1665346324443817\n",
      "Epoch 1186 of 2000\n",
      "Train Loss: 0.16648630797863007\n",
      "Epoch 1187 of 2000\n",
      "Train Loss: 0.16692042350769043\n",
      "Epoch 1188 of 2000\n",
      "Train Loss: 0.1664862036705017\n",
      "Epoch 1189 of 2000\n",
      "Train Loss: 0.1673106849193573\n",
      "Epoch 1190 of 2000\n",
      "Train Loss: 0.16661103069782257\n",
      "Epoch 1191 of 2000\n",
      "Train Loss: 0.1667594462633133\n",
      "Epoch 1192 of 2000\n",
      "Train Loss: 0.16693897545337677\n",
      "Epoch 1193 of 2000\n",
      "Train Loss: 0.1666446328163147\n",
      "Epoch 1194 of 2000\n",
      "Train Loss: 0.16657190024852753\n",
      "Epoch 1195 of 2000\n",
      "Train Loss: 0.1669328212738037\n",
      "Epoch 1196 of 2000\n",
      "Train Loss: 0.16666805744171143\n",
      "Epoch 1197 of 2000\n",
      "Train Loss: 0.16674773395061493\n",
      "Epoch 1198 of 2000\n",
      "Train Loss: 0.16682633757591248\n",
      "Epoch 1199 of 2000\n",
      "Train Loss: 0.16658596694469452\n",
      "Epoch 1200 of 2000\n",
      "Train Loss: 0.16677619516849518\n",
      "Epoch 1201 of 2000\n",
      "Train Loss: 0.16660568118095398\n",
      "Epoch 1202 of 2000\n",
      "Train Loss: 0.16650669276714325\n",
      "Epoch 1203 of 2000\n",
      "Train Loss: 0.1668306291103363\n",
      "Epoch 1204 of 2000\n",
      "Train Loss: 0.16640795767307281\n",
      "Epoch 1205 of 2000\n",
      "Train Loss: 0.16668109595775604\n",
      "Epoch 1206 of 2000\n",
      "Train Loss: 0.1666293889284134\n",
      "Epoch 1207 of 2000\n",
      "Train Loss: 0.16639545559883118\n",
      "Epoch 1208 of 2000\n",
      "Train Loss: 0.16691386699676514\n",
      "Epoch 1209 of 2000\n",
      "Train Loss: 0.16666226089000702\n",
      "Epoch 1210 of 2000\n",
      "Train Loss: 0.16654351353645325\n",
      "Epoch 1211 of 2000\n",
      "Train Loss: 0.16686582565307617\n",
      "Epoch 1212 of 2000\n",
      "Train Loss: 0.16639544069766998\n",
      "Epoch 1213 of 2000\n",
      "Train Loss: 0.16646155714988708\n",
      "Epoch 1214 of 2000\n",
      "Train Loss: 0.1666611135005951\n",
      "Epoch 1215 of 2000\n",
      "Train Loss: 0.1667475551366806\n",
      "Epoch 1216 of 2000\n",
      "Train Loss: 0.16659173369407654\n",
      "Epoch 1217 of 2000\n",
      "Train Loss: 0.16653667390346527\n",
      "Epoch 1218 of 2000\n",
      "Train Loss: 0.16653935611248016\n",
      "Epoch 1219 of 2000\n",
      "Train Loss: 0.1666284054517746\n",
      "Epoch 1220 of 2000\n",
      "Train Loss: 0.16659489274024963\n",
      "Epoch 1221 of 2000\n",
      "Train Loss: 0.1665850132703781\n",
      "Epoch 1222 of 2000\n",
      "Train Loss: 0.1665189266204834\n",
      "Epoch 1223 of 2000\n",
      "Train Loss: 0.16660550236701965\n",
      "Epoch 1224 of 2000\n",
      "Train Loss: 0.16654297709465027\n",
      "Epoch 1225 of 2000\n",
      "Train Loss: 0.16648368537425995\n",
      "Epoch 1226 of 2000\n",
      "Train Loss: 0.16676238179206848\n",
      "Epoch 1227 of 2000\n",
      "Train Loss: 0.16643476486206055\n",
      "Epoch 1228 of 2000\n",
      "Train Loss: 0.16651323437690735\n",
      "Epoch 1229 of 2000\n",
      "Train Loss: 0.16676203906536102\n",
      "Epoch 1230 of 2000\n",
      "Train Loss: 0.1664924919605255\n",
      "Epoch 1231 of 2000\n",
      "Train Loss: 0.1667763888835907\n",
      "Epoch 1232 of 2000\n",
      "Train Loss: 0.16660302877426147\n",
      "Epoch 1233 of 2000\n",
      "Train Loss: 0.1665782630443573\n",
      "Epoch 1234 of 2000\n",
      "Train Loss: 0.16691672801971436\n",
      "Epoch 1235 of 2000\n",
      "Train Loss: 0.16660156846046448\n",
      "Epoch 1236 of 2000\n",
      "Train Loss: 0.16636867821216583\n",
      "Epoch 1237 of 2000\n",
      "Train Loss: 0.1666790246963501\n",
      "Epoch 1238 of 2000\n",
      "Train Loss: 0.16653364896774292\n",
      "Epoch 1239 of 2000\n",
      "Train Loss: 0.166385680437088\n",
      "Epoch 1240 of 2000\n",
      "Train Loss: 0.1665264368057251\n",
      "Epoch 1241 of 2000\n",
      "Train Loss: 0.16646535694599152\n",
      "Epoch 1242 of 2000\n",
      "Train Loss: 0.1664748191833496\n",
      "Epoch 1243 of 2000\n",
      "Train Loss: 0.16654615104198456\n",
      "Epoch 1244 of 2000\n",
      "Train Loss: 0.166396826505661\n",
      "Epoch 1245 of 2000\n",
      "Train Loss: 0.16652598977088928\n",
      "Epoch 1246 of 2000\n",
      "Train Loss: 0.16661010682582855\n",
      "Epoch 1247 of 2000\n",
      "Train Loss: 0.16631701588630676\n",
      "Epoch 1248 of 2000\n",
      "Train Loss: 0.1666196882724762\n",
      "Epoch 1249 of 2000\n",
      "Train Loss: 0.16641367971897125\n",
      "Epoch 1250 of 2000\n",
      "Train Loss: 0.16645529866218567\n",
      "Epoch 1251 of 2000\n",
      "Train Loss: 0.16642937064170837\n",
      "Epoch 1252 of 2000\n",
      "Train Loss: 0.166380375623703\n",
      "Epoch 1253 of 2000\n",
      "Train Loss: 0.1663808524608612\n",
      "Epoch 1254 of 2000\n",
      "Train Loss: 0.16634885966777802\n",
      "Epoch 1255 of 2000\n",
      "Train Loss: 0.16640329360961914\n",
      "Epoch 1256 of 2000\n",
      "Train Loss: 0.16641120612621307\n",
      "Epoch 1257 of 2000\n",
      "Train Loss: 0.1662696897983551\n",
      "Epoch 1258 of 2000\n",
      "Train Loss: 0.16650334000587463\n",
      "Epoch 1259 of 2000\n",
      "Train Loss: 0.1663002371788025\n",
      "Epoch 1260 of 2000\n",
      "Train Loss: 0.16633674502372742\n",
      "Epoch 1261 of 2000\n",
      "Train Loss: 0.16650111973285675\n",
      "Epoch 1262 of 2000\n",
      "Train Loss: 0.16666461527347565\n",
      "Epoch 1263 of 2000\n",
      "Train Loss: 0.16641557216644287\n",
      "Epoch 1264 of 2000\n",
      "Train Loss: 0.16633105278015137\n",
      "Epoch 1265 of 2000\n",
      "Train Loss: 0.16657239198684692\n",
      "Epoch 1266 of 2000\n",
      "Train Loss: 0.1664588749408722\n",
      "Epoch 1267 of 2000\n",
      "Train Loss: 0.16652455925941467\n",
      "Epoch 1268 of 2000\n",
      "Train Loss: 0.16667167842388153\n",
      "Epoch 1269 of 2000\n",
      "Train Loss: 0.1666196882724762\n",
      "Epoch 1270 of 2000\n",
      "Train Loss: 0.16630172729492188\n",
      "Epoch 1271 of 2000\n",
      "Train Loss: 0.1664619743824005\n",
      "Epoch 1272 of 2000\n",
      "Train Loss: 0.1665273904800415\n",
      "Epoch 1273 of 2000\n",
      "Train Loss: 0.16636238992214203\n",
      "Epoch 1274 of 2000\n",
      "Train Loss: 0.16637027263641357\n",
      "Epoch 1275 of 2000\n",
      "Train Loss: 0.16651764512062073\n",
      "Epoch 1276 of 2000\n",
      "Train Loss: 0.1664249747991562\n",
      "Epoch 1277 of 2000\n",
      "Train Loss: 0.16634635627269745\n",
      "Epoch 1278 of 2000\n",
      "Train Loss: 0.16643337905406952\n",
      "Epoch 1279 of 2000\n",
      "Train Loss: 0.16643667221069336\n",
      "Epoch 1280 of 2000\n",
      "Train Loss: 0.16633263230323792\n",
      "Epoch 1281 of 2000\n",
      "Train Loss: 0.166639044880867\n",
      "Epoch 1282 of 2000\n",
      "Train Loss: 0.1664724051952362\n",
      "Epoch 1283 of 2000\n",
      "Train Loss: 0.16635377705097198\n",
      "Epoch 1284 of 2000\n",
      "Train Loss: 0.16653509438037872\n",
      "Epoch 1285 of 2000\n",
      "Train Loss: 0.16647036373615265\n",
      "Epoch 1286 of 2000\n",
      "Train Loss: 0.1663527637720108\n",
      "Epoch 1287 of 2000\n",
      "Train Loss: 0.1664564609527588\n",
      "Epoch 1288 of 2000\n",
      "Train Loss: 0.16660286486148834\n",
      "Epoch 1289 of 2000\n",
      "Train Loss: 0.1663173884153366\n",
      "Epoch 1290 of 2000\n",
      "Train Loss: 0.16634559631347656\n",
      "Epoch 1291 of 2000\n",
      "Train Loss: 0.1665162742137909\n",
      "Epoch 1292 of 2000\n",
      "Train Loss: 0.16629141569137573\n",
      "Epoch 1293 of 2000\n",
      "Train Loss: 0.16628512740135193\n",
      "Epoch 1294 of 2000\n",
      "Train Loss: 0.1664089560508728\n",
      "Epoch 1295 of 2000\n",
      "Train Loss: 0.16629621386528015\n",
      "Epoch 1296 of 2000\n",
      "Train Loss: 0.16634811460971832\n",
      "Epoch 1297 of 2000\n",
      "Train Loss: 0.16630610823631287\n",
      "Epoch 1298 of 2000\n",
      "Train Loss: 0.1664092242717743\n",
      "Epoch 1299 of 2000\n",
      "Train Loss: 0.1663862019777298\n",
      "Epoch 1300 of 2000\n",
      "Train Loss: 0.16639117896556854\n",
      "Epoch 1301 of 2000\n",
      "Train Loss: 0.16640609502792358\n",
      "Epoch 1302 of 2000\n",
      "Train Loss: 0.1666969358921051\n",
      "Epoch 1303 of 2000\n",
      "Train Loss: 0.16645848751068115\n",
      "Epoch 1304 of 2000\n",
      "Train Loss: 0.1663300096988678\n",
      "Epoch 1305 of 2000\n",
      "Train Loss: 0.16675706207752228\n",
      "Epoch 1306 of 2000\n",
      "Train Loss: 0.16679243743419647\n",
      "Epoch 1307 of 2000\n",
      "Train Loss: 0.1664814054965973\n",
      "Epoch 1308 of 2000\n",
      "Train Loss: 0.16631782054901123\n",
      "Epoch 1309 of 2000\n",
      "Train Loss: 0.16658607125282288\n",
      "Epoch 1310 of 2000\n",
      "Train Loss: 0.16666118800640106\n",
      "Epoch 1311 of 2000\n",
      "Train Loss: 0.16634760797023773\n",
      "Epoch 1312 of 2000\n",
      "Train Loss: 0.16649605333805084\n",
      "Epoch 1313 of 2000\n",
      "Train Loss: 0.16659745573997498\n",
      "Epoch 1314 of 2000\n",
      "Train Loss: 0.16637837886810303\n",
      "Epoch 1315 of 2000\n",
      "Train Loss: 0.16646656394004822\n",
      "Epoch 1316 of 2000\n",
      "Train Loss: 0.1665409952402115\n",
      "Epoch 1317 of 2000\n",
      "Train Loss: 0.16635215282440186\n",
      "Epoch 1318 of 2000\n",
      "Train Loss: 0.16683340072631836\n",
      "Epoch 1319 of 2000\n",
      "Train Loss: 0.1664242297410965\n",
      "Epoch 1320 of 2000\n",
      "Train Loss: 0.16647513210773468\n",
      "Epoch 1321 of 2000\n",
      "Train Loss: 0.16647422313690186\n",
      "Epoch 1322 of 2000\n",
      "Train Loss: 0.1664203256368637\n",
      "Epoch 1323 of 2000\n",
      "Train Loss: 0.1664232760667801\n",
      "Epoch 1324 of 2000\n",
      "Train Loss: 0.1667010337114334\n",
      "Epoch 1325 of 2000\n",
      "Train Loss: 0.1663351058959961\n",
      "Epoch 1326 of 2000\n",
      "Train Loss: 0.34821879863739014\n",
      "Epoch 1327 of 2000\n",
      "Train Loss: 0.2387632578611374\n",
      "Epoch 1328 of 2000\n",
      "Train Loss: 0.23936323821544647\n",
      "Epoch 1329 of 2000\n",
      "Train Loss: 0.21896430850028992\n",
      "Epoch 1330 of 2000\n",
      "Train Loss: 0.20272421836853027\n",
      "Epoch 1331 of 2000\n",
      "Train Loss: 0.1946822851896286\n",
      "Epoch 1332 of 2000\n",
      "Train Loss: 0.2097233235836029\n",
      "Epoch 1333 of 2000\n",
      "Train Loss: 0.20210270583629608\n",
      "Epoch 1334 of 2000\n",
      "Train Loss: 0.18998436629772186\n",
      "Epoch 1335 of 2000\n",
      "Train Loss: 0.18441471457481384\n",
      "Epoch 1336 of 2000\n",
      "Train Loss: 0.1821373850107193\n",
      "Epoch 1337 of 2000\n",
      "Train Loss: 0.17696896195411682\n",
      "Epoch 1338 of 2000\n",
      "Train Loss: 0.178263321518898\n",
      "Epoch 1339 of 2000\n",
      "Train Loss: 0.1787428855895996\n",
      "Epoch 1340 of 2000\n",
      "Train Loss: 0.1783229559659958\n",
      "Epoch 1341 of 2000\n",
      "Train Loss: 0.1765911877155304\n",
      "Epoch 1342 of 2000\n",
      "Train Loss: 0.175993874669075\n",
      "Epoch 1343 of 2000\n",
      "Train Loss: 0.17799867689609528\n",
      "Epoch 1344 of 2000\n",
      "Train Loss: 0.17687995731830597\n",
      "Epoch 1345 of 2000\n",
      "Train Loss: 0.1747969686985016\n",
      "Epoch 1346 of 2000\n",
      "Train Loss: 0.17444086074829102\n",
      "Epoch 1347 of 2000\n",
      "Train Loss: 0.17488233745098114\n",
      "Epoch 1348 of 2000\n",
      "Train Loss: 0.17439064383506775\n",
      "Epoch 1349 of 2000\n",
      "Train Loss: 0.1737622767686844\n",
      "Epoch 1350 of 2000\n",
      "Train Loss: 0.17355474829673767\n",
      "Epoch 1351 of 2000\n",
      "Train Loss: 0.17264828085899353\n",
      "Epoch 1352 of 2000\n",
      "Train Loss: 0.17246928811073303\n",
      "Epoch 1353 of 2000\n",
      "Train Loss: 0.17238786816596985\n",
      "Epoch 1354 of 2000\n",
      "Train Loss: 0.17253145575523376\n",
      "Epoch 1355 of 2000\n",
      "Train Loss: 0.17097333073616028\n",
      "Epoch 1356 of 2000\n",
      "Train Loss: 0.1718822717666626\n",
      "Epoch 1357 of 2000\n",
      "Train Loss: 0.17118552327156067\n",
      "Epoch 1358 of 2000\n",
      "Train Loss: 0.1708327829837799\n",
      "Epoch 1359 of 2000\n",
      "Train Loss: 0.17069344222545624\n",
      "Epoch 1360 of 2000\n",
      "Train Loss: 0.17002704739570618\n",
      "Epoch 1361 of 2000\n",
      "Train Loss: 0.16982030868530273\n",
      "Epoch 1362 of 2000\n",
      "Train Loss: 0.16992536187171936\n",
      "Epoch 1363 of 2000\n",
      "Train Loss: 0.17024990916252136\n",
      "Epoch 1364 of 2000\n",
      "Train Loss: 0.16922646760940552\n",
      "Epoch 1365 of 2000\n",
      "Train Loss: 0.168985977768898\n",
      "Epoch 1366 of 2000\n",
      "Train Loss: 0.16879265010356903\n",
      "Epoch 1367 of 2000\n",
      "Train Loss: 0.16867566108703613\n",
      "Epoch 1368 of 2000\n",
      "Train Loss: 0.16850173473358154\n",
      "Epoch 1369 of 2000\n",
      "Train Loss: 0.1684238612651825\n",
      "Epoch 1370 of 2000\n",
      "Train Loss: 0.16838259994983673\n",
      "Epoch 1371 of 2000\n",
      "Train Loss: 0.16820372641086578\n",
      "Epoch 1372 of 2000\n",
      "Train Loss: 0.16804750263690948\n",
      "Epoch 1373 of 2000\n",
      "Train Loss: 0.16792595386505127\n",
      "Epoch 1374 of 2000\n",
      "Train Loss: 0.16798439621925354\n",
      "Epoch 1375 of 2000\n",
      "Train Loss: 0.16770243644714355\n",
      "Epoch 1376 of 2000\n",
      "Train Loss: 0.16773851215839386\n",
      "Epoch 1377 of 2000\n",
      "Train Loss: 0.16777852177619934\n",
      "Epoch 1378 of 2000\n",
      "Train Loss: 0.16746407747268677\n",
      "Epoch 1379 of 2000\n",
      "Train Loss: 0.1674896478652954\n",
      "Epoch 1380 of 2000\n",
      "Train Loss: 0.16753162443637848\n",
      "Epoch 1381 of 2000\n",
      "Train Loss: 0.16746219992637634\n",
      "Epoch 1382 of 2000\n",
      "Train Loss: 0.1674133837223053\n",
      "Epoch 1383 of 2000\n",
      "Train Loss: 0.16728083789348602\n",
      "Epoch 1384 of 2000\n",
      "Train Loss: 0.16734416782855988\n",
      "Epoch 1385 of 2000\n",
      "Train Loss: 0.16730996966362\n",
      "Epoch 1386 of 2000\n",
      "Train Loss: 0.16719500720500946\n",
      "Epoch 1387 of 2000\n",
      "Train Loss: 0.16725903749465942\n",
      "Epoch 1388 of 2000\n",
      "Train Loss: 0.16732792556285858\n",
      "Epoch 1389 of 2000\n",
      "Train Loss: 0.16721007227897644\n",
      "Epoch 1390 of 2000\n",
      "Train Loss: 0.1670580506324768\n",
      "Epoch 1391 of 2000\n",
      "Train Loss: 0.16731134057044983\n",
      "Epoch 1392 of 2000\n",
      "Train Loss: 0.1669306457042694\n",
      "Epoch 1393 of 2000\n",
      "Train Loss: 0.1671002209186554\n",
      "Epoch 1394 of 2000\n",
      "Train Loss: 0.167288139462471\n",
      "Epoch 1395 of 2000\n",
      "Train Loss: 0.1669708788394928\n",
      "Epoch 1396 of 2000\n",
      "Train Loss: 0.16714812815189362\n",
      "Epoch 1397 of 2000\n",
      "Train Loss: 0.16703703999519348\n",
      "Epoch 1398 of 2000\n",
      "Train Loss: 0.16690905392169952\n",
      "Epoch 1399 of 2000\n",
      "Train Loss: 0.16687233746051788\n",
      "Epoch 1400 of 2000\n",
      "Train Loss: 0.16744796931743622\n",
      "Epoch 1401 of 2000\n",
      "Train Loss: 0.16678929328918457\n",
      "Epoch 1402 of 2000\n",
      "Train Loss: 0.16809844970703125\n",
      "Epoch 1403 of 2000\n",
      "Train Loss: 0.16691704094409943\n",
      "Epoch 1404 of 2000\n",
      "Train Loss: 0.16748517751693726\n",
      "Epoch 1405 of 2000\n",
      "Train Loss: 0.16708539426326752\n",
      "Epoch 1406 of 2000\n",
      "Train Loss: 0.16729289293289185\n",
      "Epoch 1407 of 2000\n",
      "Train Loss: 0.16726545989513397\n",
      "Epoch 1408 of 2000\n",
      "Train Loss: 0.16682173311710358\n",
      "Epoch 1409 of 2000\n",
      "Train Loss: 0.16736359894275665\n",
      "Epoch 1410 of 2000\n",
      "Train Loss: 0.16682219505310059\n",
      "Epoch 1411 of 2000\n",
      "Train Loss: 0.16685912013053894\n",
      "Epoch 1412 of 2000\n",
      "Train Loss: 0.16716769337654114\n",
      "Epoch 1413 of 2000\n",
      "Train Loss: 0.16675126552581787\n",
      "Epoch 1414 of 2000\n",
      "Train Loss: 0.16691794991493225\n",
      "Epoch 1415 of 2000\n",
      "Train Loss: 0.16683430969715118\n",
      "Epoch 1416 of 2000\n",
      "Train Loss: 0.1668628603219986\n",
      "Epoch 1417 of 2000\n",
      "Train Loss: 0.1668187826871872\n",
      "Epoch 1418 of 2000\n",
      "Train Loss: 0.166631817817688\n",
      "Epoch 1419 of 2000\n",
      "Train Loss: 0.16680565476417542\n",
      "Epoch 1420 of 2000\n",
      "Train Loss: 0.16669616103172302\n",
      "Epoch 1421 of 2000\n",
      "Train Loss: 0.16673389077186584\n",
      "Epoch 1422 of 2000\n",
      "Train Loss: 0.16677235066890717\n",
      "Epoch 1423 of 2000\n",
      "Train Loss: 0.1666114777326584\n",
      "Epoch 1424 of 2000\n",
      "Train Loss: 0.1666523516178131\n",
      "Epoch 1425 of 2000\n",
      "Train Loss: 0.1667514592409134\n",
      "Epoch 1426 of 2000\n",
      "Train Loss: 0.16659343242645264\n",
      "Epoch 1427 of 2000\n",
      "Train Loss: 0.16668811440467834\n",
      "Epoch 1428 of 2000\n",
      "Train Loss: 0.16680917143821716\n",
      "Epoch 1429 of 2000\n",
      "Train Loss: 0.16660276055335999\n",
      "Epoch 1430 of 2000\n",
      "Train Loss: 0.16657845675945282\n",
      "Epoch 1431 of 2000\n",
      "Train Loss: 0.16661863029003143\n",
      "Epoch 1432 of 2000\n",
      "Train Loss: 0.1667015254497528\n",
      "Epoch 1433 of 2000\n",
      "Train Loss: 0.1667066365480423\n",
      "Epoch 1434 of 2000\n",
      "Train Loss: 0.16655895113945007\n",
      "Epoch 1435 of 2000\n",
      "Train Loss: 0.166656494140625\n",
      "Epoch 1436 of 2000\n",
      "Train Loss: 0.1665465533733368\n",
      "Epoch 1437 of 2000\n",
      "Train Loss: 0.16664579510688782\n",
      "Epoch 1438 of 2000\n",
      "Train Loss: 0.16655439138412476\n",
      "Epoch 1439 of 2000\n",
      "Train Loss: 0.166587233543396\n",
      "Epoch 1440 of 2000\n",
      "Train Loss: 0.16652549803256989\n",
      "Epoch 1441 of 2000\n",
      "Train Loss: 0.1666383445262909\n",
      "Epoch 1442 of 2000\n",
      "Train Loss: 0.1665254384279251\n",
      "Epoch 1443 of 2000\n",
      "Train Loss: 0.16649354994297028\n",
      "Epoch 1444 of 2000\n",
      "Train Loss: 0.1665061116218567\n",
      "Epoch 1445 of 2000\n",
      "Train Loss: 0.16670441627502441\n",
      "Epoch 1446 of 2000\n",
      "Train Loss: 0.16669535636901855\n",
      "Epoch 1447 of 2000\n",
      "Train Loss: 0.16654567420482635\n",
      "Epoch 1448 of 2000\n",
      "Train Loss: 0.16656741499900818\n",
      "Epoch 1449 of 2000\n",
      "Train Loss: 0.16660436987876892\n",
      "Epoch 1450 of 2000\n",
      "Train Loss: 0.1666148155927658\n",
      "Epoch 1451 of 2000\n",
      "Train Loss: 0.16657397150993347\n",
      "Epoch 1452 of 2000\n",
      "Train Loss: 0.166518434882164\n",
      "Epoch 1453 of 2000\n",
      "Train Loss: 0.16649146378040314\n",
      "Epoch 1454 of 2000\n",
      "Train Loss: 0.16651815176010132\n",
      "Epoch 1455 of 2000\n",
      "Train Loss: 0.16649113595485687\n",
      "Epoch 1456 of 2000\n",
      "Train Loss: 0.16653650999069214\n",
      "Epoch 1457 of 2000\n",
      "Train Loss: 0.16656652092933655\n",
      "Epoch 1458 of 2000\n",
      "Train Loss: 0.16652043163776398\n",
      "Epoch 1459 of 2000\n",
      "Train Loss: 0.16646048426628113\n",
      "Epoch 1460 of 2000\n",
      "Train Loss: 0.1664610356092453\n",
      "Epoch 1461 of 2000\n",
      "Train Loss: 0.16651295125484467\n",
      "Epoch 1462 of 2000\n",
      "Train Loss: 0.16648505628108978\n",
      "Epoch 1463 of 2000\n",
      "Train Loss: 0.16655144095420837\n",
      "Epoch 1464 of 2000\n",
      "Train Loss: 0.16643884778022766\n",
      "Epoch 1465 of 2000\n",
      "Train Loss: 0.16650068759918213\n",
      "Epoch 1466 of 2000\n",
      "Train Loss: 0.1664767563343048\n",
      "Epoch 1467 of 2000\n",
      "Train Loss: 0.1663985252380371\n",
      "Epoch 1468 of 2000\n",
      "Train Loss: 0.16661228239536285\n",
      "Epoch 1469 of 2000\n",
      "Train Loss: 0.16646067798137665\n",
      "Epoch 1470 of 2000\n",
      "Train Loss: 0.1665143221616745\n",
      "Epoch 1471 of 2000\n",
      "Train Loss: 0.16643136739730835\n",
      "Epoch 1472 of 2000\n",
      "Train Loss: 0.1666358858346939\n",
      "Epoch 1473 of 2000\n",
      "Train Loss: 0.16644248366355896\n",
      "Epoch 1474 of 2000\n",
      "Train Loss: 0.1664527952671051\n",
      "Epoch 1475 of 2000\n",
      "Train Loss: 0.16648037731647491\n",
      "Epoch 1476 of 2000\n",
      "Train Loss: 0.16650192439556122\n",
      "Epoch 1477 of 2000\n",
      "Train Loss: 0.16642163693904877\n",
      "Epoch 1478 of 2000\n",
      "Train Loss: 0.16648133099079132\n",
      "Epoch 1479 of 2000\n",
      "Train Loss: 0.166438028216362\n",
      "Epoch 1480 of 2000\n",
      "Train Loss: 0.16641239821910858\n",
      "Epoch 1481 of 2000\n",
      "Train Loss: 0.16638918220996857\n",
      "Epoch 1482 of 2000\n",
      "Train Loss: 0.16637563705444336\n",
      "Epoch 1483 of 2000\n",
      "Train Loss: 0.16639967262744904\n",
      "Epoch 1484 of 2000\n",
      "Train Loss: 0.16640254855155945\n",
      "Epoch 1485 of 2000\n",
      "Train Loss: 0.16639651358127594\n",
      "Epoch 1486 of 2000\n",
      "Train Loss: 0.1663540154695511\n",
      "Epoch 1487 of 2000\n",
      "Train Loss: 0.1663409173488617\n",
      "Epoch 1488 of 2000\n",
      "Train Loss: 0.1663869321346283\n",
      "Epoch 1489 of 2000\n",
      "Train Loss: 0.16640333831310272\n",
      "Epoch 1490 of 2000\n",
      "Train Loss: 0.16636550426483154\n",
      "Epoch 1491 of 2000\n",
      "Train Loss: 0.16638733446598053\n",
      "Epoch 1492 of 2000\n",
      "Train Loss: 0.1664493978023529\n",
      "Epoch 1493 of 2000\n",
      "Train Loss: 0.16642014682292938\n",
      "Epoch 1494 of 2000\n",
      "Train Loss: 0.1663910299539566\n",
      "Epoch 1495 of 2000\n",
      "Train Loss: 0.16636621952056885\n",
      "Epoch 1496 of 2000\n",
      "Train Loss: 0.16634111106395721\n",
      "Epoch 1497 of 2000\n",
      "Train Loss: 0.1663782000541687\n",
      "Epoch 1498 of 2000\n",
      "Train Loss: 0.1664009988307953\n",
      "Epoch 1499 of 2000\n",
      "Train Loss: 0.1663353443145752\n",
      "Epoch 1500 of 2000\n",
      "Train Loss: 0.1663292944431305\n",
      "Epoch 1501 of 2000\n",
      "Train Loss: 0.16635757684707642\n",
      "Epoch 1502 of 2000\n",
      "Train Loss: 0.16637971997261047\n",
      "Epoch 1503 of 2000\n",
      "Train Loss: 0.16633231937885284\n",
      "Epoch 1504 of 2000\n",
      "Train Loss: 0.16638433933258057\n",
      "Epoch 1505 of 2000\n",
      "Train Loss: 0.16635994613170624\n",
      "Epoch 1506 of 2000\n",
      "Train Loss: 0.16635756194591522\n",
      "Epoch 1507 of 2000\n",
      "Train Loss: 0.16633525490760803\n",
      "Epoch 1508 of 2000\n",
      "Train Loss: 0.16649793088436127\n",
      "Epoch 1509 of 2000\n",
      "Train Loss: 0.1663898378610611\n",
      "Epoch 1510 of 2000\n",
      "Train Loss: 0.16642549633979797\n",
      "Epoch 1511 of 2000\n",
      "Train Loss: 0.16638638079166412\n",
      "Epoch 1512 of 2000\n",
      "Train Loss: 0.16634301841259003\n",
      "Epoch 1513 of 2000\n",
      "Train Loss: 0.1663810759782791\n",
      "Epoch 1514 of 2000\n",
      "Train Loss: 0.16638828814029694\n",
      "Epoch 1515 of 2000\n",
      "Train Loss: 0.16642960906028748\n",
      "Epoch 1516 of 2000\n",
      "Train Loss: 0.16636057198047638\n",
      "Epoch 1517 of 2000\n",
      "Train Loss: 0.16634489595890045\n",
      "Epoch 1518 of 2000\n",
      "Train Loss: 0.16636709868907928\n",
      "Epoch 1519 of 2000\n",
      "Train Loss: 0.16630040109157562\n",
      "Epoch 1520 of 2000\n",
      "Train Loss: 0.16647236049175262\n",
      "Epoch 1521 of 2000\n",
      "Train Loss: 0.16652913391590118\n",
      "Epoch 1522 of 2000\n",
      "Train Loss: 0.16647174954414368\n",
      "Epoch 1523 of 2000\n",
      "Train Loss: 0.16646511852741241\n",
      "Epoch 1524 of 2000\n",
      "Train Loss: 0.16640198230743408\n",
      "Epoch 1525 of 2000\n",
      "Train Loss: 0.1663355827331543\n",
      "Epoch 1526 of 2000\n",
      "Train Loss: 0.16643989086151123\n",
      "Epoch 1527 of 2000\n",
      "Train Loss: 0.1663772612810135\n",
      "Epoch 1528 of 2000\n",
      "Train Loss: 0.16639478504657745\n",
      "Epoch 1529 of 2000\n",
      "Train Loss: 0.16644947230815887\n",
      "Epoch 1530 of 2000\n",
      "Train Loss: 0.16649410128593445\n",
      "Epoch 1531 of 2000\n",
      "Train Loss: 0.1663099080324173\n",
      "Epoch 1532 of 2000\n",
      "Train Loss: 0.1663551926612854\n",
      "Epoch 1533 of 2000\n",
      "Train Loss: 0.1663368195295334\n",
      "Epoch 1534 of 2000\n",
      "Train Loss: 0.16648398339748383\n",
      "Epoch 1535 of 2000\n",
      "Train Loss: 0.16639728844165802\n",
      "Epoch 1536 of 2000\n",
      "Train Loss: 0.16637852787971497\n",
      "Epoch 1537 of 2000\n",
      "Train Loss: 0.1664646565914154\n",
      "Epoch 1538 of 2000\n",
      "Train Loss: 0.16638202965259552\n",
      "Epoch 1539 of 2000\n",
      "Train Loss: 0.1663263589143753\n",
      "Epoch 1540 of 2000\n",
      "Train Loss: 0.16626663506031036\n",
      "Epoch 1541 of 2000\n",
      "Train Loss: 0.16635863482952118\n",
      "Epoch 1542 of 2000\n",
      "Train Loss: 0.1663312166929245\n",
      "Epoch 1543 of 2000\n",
      "Train Loss: 0.16636696457862854\n",
      "Epoch 1544 of 2000\n",
      "Train Loss: 0.16634729504585266\n",
      "Epoch 1545 of 2000\n",
      "Train Loss: 0.16641680896282196\n",
      "Epoch 1546 of 2000\n",
      "Train Loss: 0.16640010476112366\n",
      "Epoch 1547 of 2000\n",
      "Train Loss: 0.16632041335105896\n",
      "Epoch 1548 of 2000\n",
      "Train Loss: 0.16631527245044708\n",
      "Epoch 1549 of 2000\n",
      "Train Loss: 0.16634388267993927\n",
      "Epoch 1550 of 2000\n",
      "Train Loss: 0.166375070810318\n",
      "Epoch 1551 of 2000\n",
      "Train Loss: 0.16630306839942932\n",
      "Epoch 1552 of 2000\n",
      "Train Loss: 0.16628125309944153\n",
      "Epoch 1553 of 2000\n",
      "Train Loss: 0.16642829775810242\n",
      "Epoch 1554 of 2000\n",
      "Train Loss: 0.16632072627544403\n",
      "Epoch 1555 of 2000\n",
      "Train Loss: 0.16631004214286804\n",
      "Epoch 1556 of 2000\n",
      "Train Loss: 0.16630513966083527\n",
      "Epoch 1557 of 2000\n",
      "Train Loss: 0.16633810102939606\n",
      "Epoch 1558 of 2000\n",
      "Train Loss: 0.16632965207099915\n",
      "Epoch 1559 of 2000\n",
      "Train Loss: 0.16634804010391235\n",
      "Epoch 1560 of 2000\n",
      "Train Loss: 0.16633188724517822\n",
      "Epoch 1561 of 2000\n",
      "Train Loss: 0.16638866066932678\n",
      "Epoch 1562 of 2000\n",
      "Train Loss: 0.16638676822185516\n",
      "Epoch 1563 of 2000\n",
      "Train Loss: 0.16629016399383545\n",
      "Epoch 1564 of 2000\n",
      "Train Loss: 0.16644105315208435\n",
      "Epoch 1565 of 2000\n",
      "Train Loss: 0.16643205285072327\n",
      "Epoch 1566 of 2000\n",
      "Train Loss: 0.1664583384990692\n",
      "Epoch 1567 of 2000\n",
      "Train Loss: 0.16628248989582062\n",
      "Epoch 1568 of 2000\n",
      "Train Loss: 0.16645321249961853\n",
      "Epoch 1569 of 2000\n",
      "Train Loss: 0.16628332436084747\n",
      "Epoch 1570 of 2000\n",
      "Train Loss: 0.16634705662727356\n",
      "Epoch 1571 of 2000\n",
      "Train Loss: 0.1664050817489624\n",
      "Epoch 1572 of 2000\n",
      "Train Loss: 0.16630277037620544\n",
      "Epoch 1573 of 2000\n",
      "Train Loss: 0.16631989181041718\n",
      "Epoch 1574 of 2000\n",
      "Train Loss: 0.1663205921649933\n",
      "Epoch 1575 of 2000\n",
      "Train Loss: 0.16639213263988495\n",
      "Epoch 1576 of 2000\n",
      "Train Loss: 0.1663151979446411\n",
      "Epoch 1577 of 2000\n",
      "Train Loss: 0.16636916995048523\n",
      "Epoch 1578 of 2000\n",
      "Train Loss: 0.16625627875328064\n",
      "Epoch 1579 of 2000\n",
      "Train Loss: 0.166495680809021\n",
      "Epoch 1580 of 2000\n",
      "Train Loss: 0.16629688441753387\n",
      "Epoch 1581 of 2000\n",
      "Train Loss: 0.16632357239723206\n",
      "Epoch 1582 of 2000\n",
      "Train Loss: 0.16633553802967072\n",
      "Epoch 1583 of 2000\n",
      "Train Loss: 0.1663943976163864\n",
      "Epoch 1584 of 2000\n",
      "Train Loss: 0.16629812121391296\n",
      "Epoch 1585 of 2000\n",
      "Train Loss: 0.16628021001815796\n",
      "Epoch 1586 of 2000\n",
      "Train Loss: 0.16626635193824768\n",
      "Epoch 1587 of 2000\n",
      "Train Loss: 0.16626973450183868\n",
      "Epoch 1588 of 2000\n",
      "Train Loss: 0.16632701456546783\n",
      "Epoch 1589 of 2000\n",
      "Train Loss: 0.166263610124588\n",
      "Epoch 1590 of 2000\n",
      "Train Loss: 0.16628043353557587\n",
      "Epoch 1591 of 2000\n",
      "Train Loss: 0.1662987768650055\n",
      "Epoch 1592 of 2000\n",
      "Train Loss: 0.16632318496704102\n",
      "Epoch 1593 of 2000\n",
      "Train Loss: 0.16642209887504578\n",
      "Epoch 1594 of 2000\n",
      "Train Loss: 0.1662672907114029\n",
      "Epoch 1595 of 2000\n",
      "Train Loss: 0.16632914543151855\n",
      "Epoch 1596 of 2000\n",
      "Train Loss: 0.16622614860534668\n",
      "Epoch 1597 of 2000\n",
      "Train Loss: 0.16637174785137177\n",
      "Epoch 1598 of 2000\n",
      "Train Loss: 0.16625818610191345\n",
      "Epoch 1599 of 2000\n",
      "Train Loss: 0.16622129082679749\n",
      "Epoch 1600 of 2000\n",
      "Train Loss: 0.16638454794883728\n",
      "Epoch 1601 of 2000\n",
      "Train Loss: 0.16632020473480225\n",
      "Epoch 1602 of 2000\n",
      "Train Loss: 0.16632066667079926\n",
      "Epoch 1603 of 2000\n",
      "Train Loss: 0.1662289798259735\n",
      "Epoch 1604 of 2000\n",
      "Train Loss: 0.16627998650074005\n",
      "Epoch 1605 of 2000\n",
      "Train Loss: 0.16630662977695465\n",
      "Epoch 1606 of 2000\n",
      "Train Loss: 0.16626127064228058\n",
      "Epoch 1607 of 2000\n",
      "Train Loss: 0.16625866293907166\n",
      "Epoch 1608 of 2000\n",
      "Train Loss: 0.1662289947271347\n",
      "Epoch 1609 of 2000\n",
      "Train Loss: 0.16621991991996765\n",
      "Epoch 1610 of 2000\n",
      "Train Loss: 0.16625306010246277\n",
      "Epoch 1611 of 2000\n",
      "Train Loss: 0.16623196005821228\n",
      "Epoch 1612 of 2000\n",
      "Train Loss: 0.16627955436706543\n",
      "Epoch 1613 of 2000\n",
      "Train Loss: 0.16624274849891663\n",
      "Epoch 1614 of 2000\n",
      "Train Loss: 0.16624346375465393\n",
      "Epoch 1615 of 2000\n",
      "Train Loss: 0.1662619411945343\n",
      "Epoch 1616 of 2000\n",
      "Train Loss: 0.16620653867721558\n",
      "Epoch 1617 of 2000\n",
      "Train Loss: 0.16624359786510468\n",
      "Epoch 1618 of 2000\n",
      "Train Loss: 0.1662418097257614\n",
      "Epoch 1619 of 2000\n",
      "Train Loss: 0.16622087359428406\n",
      "Epoch 1620 of 2000\n",
      "Train Loss: 0.16627614200115204\n",
      "Epoch 1621 of 2000\n",
      "Train Loss: 0.1662328541278839\n",
      "Epoch 1622 of 2000\n",
      "Train Loss: 0.1663835495710373\n",
      "Epoch 1623 of 2000\n",
      "Train Loss: 0.16627885401248932\n",
      "Epoch 1624 of 2000\n",
      "Train Loss: 0.1663321703672409\n",
      "Epoch 1625 of 2000\n",
      "Train Loss: 0.16625192761421204\n",
      "Epoch 1626 of 2000\n",
      "Train Loss: 0.16629736125469208\n",
      "Epoch 1627 of 2000\n",
      "Train Loss: 0.16630756855010986\n",
      "Epoch 1628 of 2000\n",
      "Train Loss: 0.16626615822315216\n",
      "Epoch 1629 of 2000\n",
      "Train Loss: 0.16629910469055176\n",
      "Epoch 1630 of 2000\n",
      "Train Loss: 0.16630040109157562\n",
      "Epoch 1631 of 2000\n",
      "Train Loss: 0.1664135605096817\n",
      "Epoch 1632 of 2000\n",
      "Train Loss: 0.16628362238407135\n",
      "Epoch 1633 of 2000\n",
      "Train Loss: 0.16627837717533112\n",
      "Epoch 1634 of 2000\n",
      "Train Loss: 0.16624929010868073\n",
      "Epoch 1635 of 2000\n",
      "Train Loss: 0.1662529706954956\n",
      "Epoch 1636 of 2000\n",
      "Train Loss: 0.16628117859363556\n",
      "Epoch 1637 of 2000\n",
      "Train Loss: 0.1662978231906891\n",
      "Epoch 1638 of 2000\n",
      "Train Loss: 0.1662214994430542\n",
      "Epoch 1639 of 2000\n",
      "Train Loss: 0.16626785695552826\n",
      "Epoch 1640 of 2000\n",
      "Train Loss: 0.1662272810935974\n",
      "Epoch 1641 of 2000\n",
      "Train Loss: 0.16628912091255188\n",
      "Epoch 1642 of 2000\n",
      "Train Loss: 0.16620205342769623\n",
      "Epoch 1643 of 2000\n",
      "Train Loss: 0.1662476360797882\n",
      "Epoch 1644 of 2000\n",
      "Train Loss: 0.16621096432209015\n",
      "Epoch 1645 of 2000\n",
      "Train Loss: 0.1662708967924118\n",
      "Epoch 1646 of 2000\n",
      "Train Loss: 0.16620269417762756\n",
      "Epoch 1647 of 2000\n",
      "Train Loss: 0.1662474274635315\n",
      "Epoch 1648 of 2000\n",
      "Train Loss: 0.16620348393917084\n",
      "Epoch 1649 of 2000\n",
      "Train Loss: 0.16624628007411957\n",
      "Epoch 1650 of 2000\n",
      "Train Loss: 0.1662224978208542\n",
      "Epoch 1651 of 2000\n",
      "Train Loss: 0.16623620688915253\n",
      "Epoch 1652 of 2000\n",
      "Train Loss: 0.16621407866477966\n",
      "Epoch 1653 of 2000\n",
      "Train Loss: 0.16623364388942719\n",
      "Epoch 1654 of 2000\n",
      "Train Loss: 0.16621658205986023\n",
      "Epoch 1655 of 2000\n",
      "Train Loss: 0.1662530153989792\n",
      "Epoch 1656 of 2000\n",
      "Train Loss: 0.1662224382162094\n",
      "Epoch 1657 of 2000\n",
      "Train Loss: 0.16624173521995544\n",
      "Epoch 1658 of 2000\n",
      "Train Loss: 0.1662013977766037\n",
      "Epoch 1659 of 2000\n",
      "Train Loss: 0.16630685329437256\n",
      "Epoch 1660 of 2000\n",
      "Train Loss: 0.16628359258174896\n",
      "Epoch 1661 of 2000\n",
      "Train Loss: 0.1662442684173584\n",
      "Epoch 1662 of 2000\n",
      "Train Loss: 0.1662568747997284\n",
      "Epoch 1663 of 2000\n",
      "Train Loss: 0.1662098616361618\n",
      "Epoch 1664 of 2000\n",
      "Train Loss: 0.16629937291145325\n",
      "Epoch 1665 of 2000\n",
      "Train Loss: 0.16623817384243011\n",
      "Epoch 1666 of 2000\n",
      "Train Loss: 0.16627171635627747\n",
      "Epoch 1667 of 2000\n",
      "Train Loss: 0.16628244519233704\n",
      "Epoch 1668 of 2000\n",
      "Train Loss: 0.16631832718849182\n",
      "Epoch 1669 of 2000\n",
      "Train Loss: 0.16636693477630615\n",
      "Epoch 1670 of 2000\n",
      "Train Loss: 0.16622304916381836\n",
      "Epoch 1671 of 2000\n",
      "Train Loss: 0.1663653552532196\n",
      "Epoch 1672 of 2000\n",
      "Train Loss: 0.1662437617778778\n",
      "Epoch 1673 of 2000\n",
      "Train Loss: 0.1663600206375122\n",
      "Epoch 1674 of 2000\n",
      "Train Loss: 0.16627180576324463\n",
      "Epoch 1675 of 2000\n",
      "Train Loss: 0.16623452305793762\n",
      "Epoch 1676 of 2000\n",
      "Train Loss: 0.16623802483081818\n",
      "Epoch 1677 of 2000\n",
      "Train Loss: 0.16625897586345673\n",
      "Epoch 1678 of 2000\n",
      "Train Loss: 0.1662651002407074\n",
      "Epoch 1679 of 2000\n",
      "Train Loss: 0.16621845960617065\n",
      "Epoch 1680 of 2000\n",
      "Train Loss: 0.16628487408161163\n",
      "Epoch 1681 of 2000\n",
      "Train Loss: 0.16626039147377014\n",
      "Epoch 1682 of 2000\n",
      "Train Loss: 0.1662672758102417\n",
      "Epoch 1683 of 2000\n",
      "Train Loss: 0.16620281338691711\n",
      "Epoch 1684 of 2000\n",
      "Train Loss: 0.1662560999393463\n",
      "Epoch 1685 of 2000\n",
      "Train Loss: 0.16625945270061493\n",
      "Epoch 1686 of 2000\n",
      "Train Loss: 0.16631503403186798\n",
      "Epoch 1687 of 2000\n",
      "Train Loss: 0.1662193387746811\n",
      "Epoch 1688 of 2000\n",
      "Train Loss: 0.16633380949497223\n",
      "Epoch 1689 of 2000\n",
      "Train Loss: 0.16622212529182434\n",
      "Epoch 1690 of 2000\n",
      "Train Loss: 0.16622382402420044\n",
      "Epoch 1691 of 2000\n",
      "Train Loss: 0.16621044278144836\n",
      "Epoch 1692 of 2000\n",
      "Train Loss: 0.1662919819355011\n",
      "Epoch 1693 of 2000\n",
      "Train Loss: 0.16627895832061768\n",
      "Epoch 1694 of 2000\n",
      "Train Loss: 0.1661761850118637\n",
      "Epoch 1695 of 2000\n",
      "Train Loss: 0.16621772944927216\n",
      "Epoch 1696 of 2000\n",
      "Train Loss: 0.1663295328617096\n",
      "Epoch 1697 of 2000\n",
      "Train Loss: 0.16636283695697784\n",
      "Epoch 1698 of 2000\n",
      "Train Loss: 0.16620716452598572\n",
      "Epoch 1699 of 2000\n",
      "Train Loss: 0.16624704003334045\n",
      "Epoch 1700 of 2000\n",
      "Train Loss: 0.16632148623466492\n",
      "Epoch 1701 of 2000\n",
      "Train Loss: 0.16628959774971008\n",
      "Epoch 1702 of 2000\n",
      "Train Loss: 0.16618981957435608\n",
      "Epoch 1703 of 2000\n",
      "Train Loss: 0.16623146831989288\n",
      "Epoch 1704 of 2000\n",
      "Train Loss: 0.1663331687450409\n",
      "Epoch 1705 of 2000\n",
      "Train Loss: 0.16629450023174286\n",
      "Epoch 1706 of 2000\n",
      "Train Loss: 0.16622476279735565\n",
      "Epoch 1707 of 2000\n",
      "Train Loss: 0.16631609201431274\n",
      "Epoch 1708 of 2000\n",
      "Train Loss: 0.1662270575761795\n",
      "Epoch 1709 of 2000\n",
      "Train Loss: 0.1662052422761917\n",
      "Epoch 1710 of 2000\n",
      "Train Loss: 0.1661728173494339\n",
      "Epoch 1711 of 2000\n",
      "Train Loss: 0.1662520170211792\n",
      "Epoch 1712 of 2000\n",
      "Train Loss: 0.1662016361951828\n",
      "Epoch 1713 of 2000\n",
      "Train Loss: 0.1661635935306549\n",
      "Epoch 1714 of 2000\n",
      "Train Loss: 0.16618558764457703\n",
      "Epoch 1715 of 2000\n",
      "Train Loss: 0.16618551313877106\n",
      "Epoch 1716 of 2000\n",
      "Train Loss: 0.16623274981975555\n",
      "Epoch 1717 of 2000\n",
      "Train Loss: 0.1662200391292572\n",
      "Epoch 1718 of 2000\n",
      "Train Loss: 0.1661801040172577\n",
      "Epoch 1719 of 2000\n",
      "Train Loss: 0.16624733805656433\n",
      "Epoch 1720 of 2000\n",
      "Train Loss: 0.16622628271579742\n",
      "Epoch 1721 of 2000\n",
      "Train Loss: 0.16625013947486877\n",
      "Epoch 1722 of 2000\n",
      "Train Loss: 0.16620223224163055\n",
      "Epoch 1723 of 2000\n",
      "Train Loss: 0.16619637608528137\n",
      "Epoch 1724 of 2000\n",
      "Train Loss: 0.16621920466423035\n",
      "Epoch 1725 of 2000\n",
      "Train Loss: 0.16628165543079376\n",
      "Epoch 1726 of 2000\n",
      "Train Loss: 0.1662481725215912\n",
      "Epoch 1727 of 2000\n",
      "Train Loss: 0.16623760759830475\n",
      "Epoch 1728 of 2000\n",
      "Train Loss: 0.1662261039018631\n",
      "Epoch 1729 of 2000\n",
      "Train Loss: 0.16619519889354706\n",
      "Epoch 1730 of 2000\n",
      "Train Loss: 0.16618089377880096\n",
      "Epoch 1731 of 2000\n",
      "Train Loss: 0.16620202362537384\n",
      "Epoch 1732 of 2000\n",
      "Train Loss: 0.16618330776691437\n",
      "Epoch 1733 of 2000\n",
      "Train Loss: 0.16622550785541534\n",
      "Epoch 1734 of 2000\n",
      "Train Loss: 0.16620054841041565\n",
      "Epoch 1735 of 2000\n",
      "Train Loss: 0.16618677973747253\n",
      "Epoch 1736 of 2000\n",
      "Train Loss: 0.1661972552537918\n",
      "Epoch 1737 of 2000\n",
      "Train Loss: 0.166212797164917\n",
      "Epoch 1738 of 2000\n",
      "Train Loss: 0.16620402038097382\n",
      "Epoch 1739 of 2000\n",
      "Train Loss: 0.16622993350028992\n",
      "Epoch 1740 of 2000\n",
      "Train Loss: 0.16621480882167816\n",
      "Epoch 1741 of 2000\n",
      "Train Loss: 0.16621685028076172\n",
      "Epoch 1742 of 2000\n",
      "Train Loss: 0.16620200872421265\n",
      "Epoch 1743 of 2000\n",
      "Train Loss: 0.16618216037750244\n",
      "Epoch 1744 of 2000\n",
      "Train Loss: 0.16621586680412292\n",
      "Epoch 1745 of 2000\n",
      "Train Loss: 0.16623012721538544\n",
      "Epoch 1746 of 2000\n",
      "Train Loss: 0.16620449721813202\n",
      "Epoch 1747 of 2000\n",
      "Train Loss: 0.1661701649427414\n",
      "Epoch 1748 of 2000\n",
      "Train Loss: 0.16622017323970795\n",
      "Epoch 1749 of 2000\n",
      "Train Loss: 0.16621915996074677\n",
      "Epoch 1750 of 2000\n",
      "Train Loss: 0.16627103090286255\n",
      "Epoch 1751 of 2000\n",
      "Train Loss: 0.16621041297912598\n",
      "Epoch 1752 of 2000\n",
      "Train Loss: 0.16626563668251038\n",
      "Epoch 1753 of 2000\n",
      "Train Loss: 0.16621029376983643\n",
      "Epoch 1754 of 2000\n",
      "Train Loss: 0.16623708605766296\n",
      "Epoch 1755 of 2000\n",
      "Train Loss: 0.16618379950523376\n",
      "Epoch 1756 of 2000\n",
      "Train Loss: 0.16621018946170807\n",
      "Epoch 1757 of 2000\n",
      "Train Loss: 0.16619117558002472\n",
      "Epoch 1758 of 2000\n",
      "Train Loss: 0.16616089642047882\n",
      "Epoch 1759 of 2000\n",
      "Train Loss: 0.16617096960544586\n",
      "Epoch 1760 of 2000\n",
      "Train Loss: 0.16619332134723663\n",
      "Epoch 1761 of 2000\n",
      "Train Loss: 0.16617920994758606\n",
      "Epoch 1762 of 2000\n",
      "Train Loss: 0.16617652773857117\n",
      "Epoch 1763 of 2000\n",
      "Train Loss: 0.1662190705537796\n",
      "Epoch 1764 of 2000\n",
      "Train Loss: 0.16617293655872345\n",
      "Epoch 1765 of 2000\n",
      "Train Loss: 0.16618797183036804\n",
      "Epoch 1766 of 2000\n",
      "Train Loss: 0.16615045070648193\n",
      "Epoch 1767 of 2000\n",
      "Train Loss: 0.16615352034568787\n",
      "Epoch 1768 of 2000\n",
      "Train Loss: 0.16618958115577698\n",
      "Epoch 1769 of 2000\n",
      "Train Loss: 0.16620035469532013\n",
      "Epoch 1770 of 2000\n",
      "Train Loss: 0.1661732941865921\n",
      "Epoch 1771 of 2000\n",
      "Train Loss: 0.1662069410085678\n",
      "Epoch 1772 of 2000\n",
      "Train Loss: 0.16621185839176178\n",
      "Epoch 1773 of 2000\n",
      "Train Loss: 0.16618461906909943\n",
      "Epoch 1774 of 2000\n",
      "Train Loss: 0.1661892682313919\n",
      "Epoch 1775 of 2000\n",
      "Train Loss: 0.16621141135692596\n",
      "Epoch 1776 of 2000\n",
      "Train Loss: 0.16630029678344727\n",
      "Epoch 1777 of 2000\n",
      "Train Loss: 0.16625644266605377\n",
      "Epoch 1778 of 2000\n",
      "Train Loss: 0.16623827815055847\n",
      "Epoch 1779 of 2000\n",
      "Train Loss: 0.16624292731285095\n",
      "Epoch 1780 of 2000\n",
      "Train Loss: 0.16622042655944824\n",
      "Epoch 1781 of 2000\n",
      "Train Loss: 0.16625414788722992\n",
      "Epoch 1782 of 2000\n",
      "Train Loss: 0.16631262004375458\n",
      "Epoch 1783 of 2000\n",
      "Train Loss: 0.1664198935031891\n",
      "Epoch 1784 of 2000\n",
      "Train Loss: 0.1664046198129654\n",
      "Epoch 1785 of 2000\n",
      "Train Loss: 0.1664033979177475\n",
      "Epoch 1786 of 2000\n",
      "Train Loss: 0.16625776886940002\n",
      "Epoch 1787 of 2000\n",
      "Train Loss: 0.16620315611362457\n",
      "Epoch 1788 of 2000\n",
      "Train Loss: 0.16617465019226074\n",
      "Epoch 1789 of 2000\n",
      "Train Loss: 0.16623243689537048\n",
      "Epoch 1790 of 2000\n",
      "Train Loss: 0.16624775528907776\n",
      "Epoch 1791 of 2000\n",
      "Train Loss: 0.16622307896614075\n",
      "Epoch 1792 of 2000\n",
      "Train Loss: 0.16621902585029602\n",
      "Epoch 1793 of 2000\n",
      "Train Loss: 0.16625864803791046\n",
      "Epoch 1794 of 2000\n",
      "Train Loss: 0.16646020114421844\n",
      "Epoch 1795 of 2000\n",
      "Train Loss: 0.1665506213903427\n",
      "Epoch 1796 of 2000\n",
      "Train Loss: 0.16645756363868713\n",
      "Epoch 1797 of 2000\n",
      "Train Loss: 0.1663151979446411\n",
      "Epoch 1798 of 2000\n",
      "Train Loss: 0.16621141135692596\n",
      "Epoch 1799 of 2000\n",
      "Train Loss: 0.16619926691055298\n",
      "Epoch 1800 of 2000\n",
      "Train Loss: 0.16628746688365936\n",
      "Epoch 1801 of 2000\n",
      "Train Loss: 0.16635973751544952\n",
      "Epoch 1802 of 2000\n",
      "Train Loss: 0.16630372405052185\n",
      "Epoch 1803 of 2000\n",
      "Train Loss: 0.16617190837860107\n",
      "Epoch 1804 of 2000\n",
      "Train Loss: 0.16619546711444855\n",
      "Epoch 1805 of 2000\n",
      "Train Loss: 0.16624419391155243\n",
      "Epoch 1806 of 2000\n",
      "Train Loss: 0.16633296012878418\n",
      "Epoch 1807 of 2000\n",
      "Train Loss: 0.16629138588905334\n",
      "Epoch 1808 of 2000\n",
      "Train Loss: 0.16623584926128387\n",
      "Epoch 1809 of 2000\n",
      "Train Loss: 0.1662740856409073\n",
      "Epoch 1810 of 2000\n",
      "Train Loss: 0.16619478166103363\n",
      "Epoch 1811 of 2000\n",
      "Train Loss: 0.16625215113162994\n",
      "Epoch 1812 of 2000\n",
      "Train Loss: 0.1662731170654297\n",
      "Epoch 1813 of 2000\n",
      "Train Loss: 0.16620132327079773\n",
      "Epoch 1814 of 2000\n",
      "Train Loss: 0.1661739945411682\n",
      "Epoch 1815 of 2000\n",
      "Train Loss: 0.1661907583475113\n",
      "Epoch 1816 of 2000\n",
      "Train Loss: 0.16616180539131165\n",
      "Epoch 1817 of 2000\n",
      "Train Loss: 0.1661936193704605\n",
      "Epoch 1818 of 2000\n",
      "Train Loss: 0.16618892550468445\n",
      "Epoch 1819 of 2000\n",
      "Train Loss: 0.1661907136440277\n",
      "Epoch 1820 of 2000\n",
      "Train Loss: 0.16619853675365448\n",
      "Epoch 1821 of 2000\n",
      "Train Loss: 0.16614930331707\n",
      "Epoch 1822 of 2000\n",
      "Train Loss: 0.16620296239852905\n",
      "Epoch 1823 of 2000\n",
      "Train Loss: 0.1662062257528305\n",
      "Epoch 1824 of 2000\n",
      "Train Loss: 0.16619285941123962\n",
      "Epoch 1825 of 2000\n",
      "Train Loss: 0.16617953777313232\n",
      "Epoch 1826 of 2000\n",
      "Train Loss: 0.16626329720020294\n",
      "Epoch 1827 of 2000\n",
      "Train Loss: 0.16633814573287964\n",
      "Epoch 1828 of 2000\n",
      "Train Loss: 0.1662672758102417\n",
      "Epoch 1829 of 2000\n",
      "Train Loss: 0.16618375480175018\n",
      "Epoch 1830 of 2000\n",
      "Train Loss: 0.16616861522197723\n",
      "Epoch 1831 of 2000\n",
      "Train Loss: 0.16619260609149933\n",
      "Epoch 1832 of 2000\n",
      "Train Loss: 0.16626295447349548\n",
      "Epoch 1833 of 2000\n",
      "Train Loss: 0.1661836802959442\n",
      "Epoch 1834 of 2000\n",
      "Train Loss: 0.16617393493652344\n",
      "Epoch 1835 of 2000\n",
      "Train Loss: 0.1661698818206787\n",
      "Epoch 1836 of 2000\n",
      "Train Loss: 0.1662527322769165\n",
      "Epoch 1837 of 2000\n",
      "Train Loss: 0.16621677577495575\n",
      "Epoch 1838 of 2000\n",
      "Train Loss: 0.16620123386383057\n",
      "Epoch 1839 of 2000\n",
      "Train Loss: 0.16618111729621887\n",
      "Epoch 1840 of 2000\n",
      "Train Loss: 0.16621369123458862\n",
      "Epoch 1841 of 2000\n",
      "Train Loss: 0.16616718471050262\n",
      "Epoch 1842 of 2000\n",
      "Train Loss: 0.16617392003536224\n",
      "Epoch 1843 of 2000\n",
      "Train Loss: 0.16620317101478577\n",
      "Epoch 1844 of 2000\n",
      "Train Loss: 0.16619320213794708\n",
      "Epoch 1845 of 2000\n",
      "Train Loss: 0.16621051728725433\n",
      "Epoch 1846 of 2000\n",
      "Train Loss: 0.16621166467666626\n",
      "Epoch 1847 of 2000\n",
      "Train Loss: 0.16627366840839386\n",
      "Epoch 1848 of 2000\n",
      "Train Loss: 0.16629789769649506\n",
      "Epoch 1849 of 2000\n",
      "Train Loss: 0.16625270247459412\n",
      "Epoch 1850 of 2000\n",
      "Train Loss: 0.16624236106872559\n",
      "Epoch 1851 of 2000\n",
      "Train Loss: 0.16621580719947815\n",
      "Epoch 1852 of 2000\n",
      "Train Loss: 0.16618302464485168\n",
      "Epoch 1853 of 2000\n",
      "Train Loss: 0.16620318591594696\n",
      "Epoch 1854 of 2000\n",
      "Train Loss: 0.16617199778556824\n",
      "Epoch 1855 of 2000\n",
      "Train Loss: 0.1662771850824356\n",
      "Epoch 1856 of 2000\n",
      "Train Loss: 0.1661832332611084\n",
      "Epoch 1857 of 2000\n",
      "Train Loss: 0.16617988049983978\n",
      "Epoch 1858 of 2000\n",
      "Train Loss: 0.1662246733903885\n",
      "Epoch 1859 of 2000\n",
      "Train Loss: 0.16616062819957733\n",
      "Epoch 1860 of 2000\n",
      "Train Loss: 0.166195347905159\n",
      "Epoch 1861 of 2000\n",
      "Train Loss: 0.16616156697273254\n",
      "Epoch 1862 of 2000\n",
      "Train Loss: 0.16623225808143616\n",
      "Epoch 1863 of 2000\n",
      "Train Loss: 0.16622522473335266\n",
      "Epoch 1864 of 2000\n",
      "Train Loss: 0.16625216603279114\n",
      "Epoch 1865 of 2000\n",
      "Train Loss: 0.16627438366413116\n",
      "Epoch 1866 of 2000\n",
      "Train Loss: 0.16626949608325958\n",
      "Epoch 1867 of 2000\n",
      "Train Loss: 0.16618314385414124\n",
      "Epoch 1868 of 2000\n",
      "Train Loss: 0.1661836802959442\n",
      "Epoch 1869 of 2000\n",
      "Train Loss: 0.16626191139221191\n",
      "Epoch 1870 of 2000\n",
      "Train Loss: 0.1661847084760666\n",
      "Epoch 1871 of 2000\n",
      "Train Loss: 0.16619643568992615\n",
      "Epoch 1872 of 2000\n",
      "Train Loss: 0.16619105637073517\n",
      "Epoch 1873 of 2000\n",
      "Train Loss: 0.16630849242210388\n",
      "Epoch 1874 of 2000\n",
      "Train Loss: 0.16621747612953186\n",
      "Epoch 1875 of 2000\n",
      "Train Loss: 0.1661791056394577\n",
      "Epoch 1876 of 2000\n",
      "Train Loss: 0.16616514325141907\n",
      "Epoch 1877 of 2000\n",
      "Train Loss: 0.16625931859016418\n",
      "Epoch 1878 of 2000\n",
      "Train Loss: 0.16623985767364502\n",
      "Epoch 1879 of 2000\n",
      "Train Loss: 0.16629470884799957\n",
      "Epoch 1880 of 2000\n",
      "Train Loss: 0.16631653904914856\n",
      "Epoch 1881 of 2000\n",
      "Train Loss: 0.1662784367799759\n",
      "Epoch 1882 of 2000\n",
      "Train Loss: 0.16628623008728027\n",
      "Epoch 1883 of 2000\n",
      "Train Loss: 0.16615620255470276\n",
      "Epoch 1884 of 2000\n",
      "Train Loss: 0.16619877517223358\n",
      "Epoch 1885 of 2000\n",
      "Train Loss: 0.16623525321483612\n",
      "Epoch 1886 of 2000\n",
      "Train Loss: 0.1662910282611847\n",
      "Epoch 1887 of 2000\n",
      "Train Loss: 0.16633152961730957\n",
      "Epoch 1888 of 2000\n",
      "Train Loss: 0.16622796654701233\n",
      "Epoch 1889 of 2000\n",
      "Train Loss: 0.16619561612606049\n",
      "Epoch 1890 of 2000\n",
      "Train Loss: 0.16614067554473877\n",
      "Epoch 1891 of 2000\n",
      "Train Loss: 0.1662072092294693\n",
      "Epoch 1892 of 2000\n",
      "Train Loss: 0.16621479392051697\n",
      "Epoch 1893 of 2000\n",
      "Train Loss: 0.16621088981628418\n",
      "Epoch 1894 of 2000\n",
      "Train Loss: 0.1661892831325531\n",
      "Epoch 1895 of 2000\n",
      "Train Loss: 0.16617676615715027\n",
      "Epoch 1896 of 2000\n",
      "Train Loss: 0.16624626517295837\n",
      "Epoch 1897 of 2000\n",
      "Train Loss: 0.16616950929164886\n",
      "Epoch 1898 of 2000\n",
      "Train Loss: 0.16622301936149597\n",
      "Epoch 1899 of 2000\n",
      "Train Loss: 0.16625598073005676\n",
      "Epoch 1900 of 2000\n",
      "Train Loss: 0.1661922186613083\n",
      "Epoch 1901 of 2000\n",
      "Train Loss: 0.16621127724647522\n",
      "Epoch 1902 of 2000\n",
      "Train Loss: 0.1661710888147354\n",
      "Epoch 1903 of 2000\n",
      "Train Loss: 0.16628816723823547\n",
      "Epoch 1904 of 2000\n",
      "Train Loss: 0.1661665290594101\n",
      "Epoch 1905 of 2000\n",
      "Train Loss: 0.16623160243034363\n",
      "Epoch 1906 of 2000\n",
      "Train Loss: 0.16626180708408356\n",
      "Epoch 1907 of 2000\n",
      "Train Loss: 0.16630005836486816\n",
      "Epoch 1908 of 2000\n",
      "Train Loss: 0.16627132892608643\n",
      "Epoch 1909 of 2000\n",
      "Train Loss: 0.1662149578332901\n",
      "Epoch 1910 of 2000\n",
      "Train Loss: 0.16617515683174133\n",
      "Epoch 1911 of 2000\n",
      "Train Loss: 0.16621756553649902\n",
      "Epoch 1912 of 2000\n",
      "Train Loss: 0.16633765399456024\n",
      "Epoch 1913 of 2000\n",
      "Train Loss: 0.16621437668800354\n",
      "Epoch 1914 of 2000\n",
      "Train Loss: 0.16627086699008942\n",
      "Epoch 1915 of 2000\n",
      "Train Loss: 0.16625946760177612\n",
      "Epoch 1916 of 2000\n",
      "Train Loss: 0.16626238822937012\n",
      "Epoch 1917 of 2000\n",
      "Train Loss: 0.16620242595672607\n",
      "Epoch 1918 of 2000\n",
      "Train Loss: 0.166266068816185\n",
      "Epoch 1919 of 2000\n",
      "Train Loss: 0.16621124744415283\n",
      "Epoch 1920 of 2000\n",
      "Train Loss: 0.166193425655365\n",
      "Epoch 1921 of 2000\n",
      "Train Loss: 0.16626057028770447\n",
      "Epoch 1922 of 2000\n",
      "Train Loss: 0.16620321571826935\n",
      "Epoch 1923 of 2000\n",
      "Train Loss: 0.1663050502538681\n",
      "Epoch 1924 of 2000\n",
      "Train Loss: 0.166228786110878\n",
      "Epoch 1925 of 2000\n",
      "Train Loss: 0.1662643700838089\n",
      "Epoch 1926 of 2000\n",
      "Train Loss: 0.16618192195892334\n",
      "Epoch 1927 of 2000\n",
      "Train Loss: 0.16618403792381287\n",
      "Epoch 1928 of 2000\n",
      "Train Loss: 0.1661662608385086\n",
      "Epoch 1929 of 2000\n",
      "Train Loss: 0.16619525849819183\n",
      "Epoch 1930 of 2000\n",
      "Train Loss: 0.1662503480911255\n",
      "Epoch 1931 of 2000\n",
      "Train Loss: 0.16623154282569885\n",
      "Epoch 1932 of 2000\n",
      "Train Loss: 0.16622595489025116\n",
      "Epoch 1933 of 2000\n",
      "Train Loss: 0.16626271605491638\n",
      "Epoch 1934 of 2000\n",
      "Train Loss: 0.16616414487361908\n",
      "Epoch 1935 of 2000\n",
      "Train Loss: 0.1661684215068817\n",
      "Epoch 1936 of 2000\n",
      "Train Loss: 0.16620226204395294\n",
      "Epoch 1937 of 2000\n",
      "Train Loss: 0.16617929935455322\n",
      "Epoch 1938 of 2000\n",
      "Train Loss: 0.16620464622974396\n",
      "Epoch 1939 of 2000\n",
      "Train Loss: 0.166228249669075\n",
      "Epoch 1940 of 2000\n",
      "Train Loss: 0.16625136137008667\n",
      "Epoch 1941 of 2000\n",
      "Train Loss: 0.166163831949234\n",
      "Epoch 1942 of 2000\n",
      "Train Loss: 0.16615185141563416\n",
      "Epoch 1943 of 2000\n",
      "Train Loss: 0.16621287167072296\n",
      "Epoch 1944 of 2000\n",
      "Train Loss: 0.16626155376434326\n",
      "Epoch 1945 of 2000\n",
      "Train Loss: 0.16622896492481232\n",
      "Epoch 1946 of 2000\n",
      "Train Loss: 0.1661604791879654\n",
      "Epoch 1947 of 2000\n",
      "Train Loss: 0.16617614030838013\n",
      "Epoch 1948 of 2000\n",
      "Train Loss: 0.16623181104660034\n",
      "Epoch 1949 of 2000\n",
      "Train Loss: 0.16636879742145538\n",
      "Epoch 1950 of 2000\n",
      "Train Loss: 0.16631248593330383\n",
      "Epoch 1951 of 2000\n",
      "Train Loss: 0.16621844470500946\n",
      "Epoch 1952 of 2000\n",
      "Train Loss: 0.16619417071342468\n",
      "Epoch 1953 of 2000\n",
      "Train Loss: 0.16618284583091736\n",
      "Epoch 1954 of 2000\n",
      "Train Loss: 0.16619248688220978\n",
      "Epoch 1955 of 2000\n",
      "Train Loss: 0.16617949306964874\n",
      "Epoch 1956 of 2000\n",
      "Train Loss: 0.16618019342422485\n",
      "Epoch 1957 of 2000\n",
      "Train Loss: 0.16621264815330505\n",
      "Epoch 1958 of 2000\n",
      "Train Loss: 0.1662043333053589\n",
      "Epoch 1959 of 2000\n",
      "Train Loss: 0.16617533564567566\n",
      "Epoch 1960 of 2000\n",
      "Train Loss: 0.166158989071846\n",
      "Epoch 1961 of 2000\n",
      "Train Loss: 0.16639037430286407\n",
      "Epoch 1962 of 2000\n",
      "Train Loss: 0.16630379855632782\n",
      "Epoch 1963 of 2000\n",
      "Train Loss: 0.1662820279598236\n",
      "Epoch 1964 of 2000\n",
      "Train Loss: 0.16629259288311005\n",
      "Epoch 1965 of 2000\n",
      "Train Loss: 0.16625264286994934\n",
      "Epoch 1966 of 2000\n",
      "Train Loss: 0.1661868393421173\n",
      "Epoch 1967 of 2000\n",
      "Train Loss: 0.16617190837860107\n",
      "Epoch 1968 of 2000\n",
      "Train Loss: 0.16627922654151917\n",
      "Epoch 1969 of 2000\n",
      "Train Loss: 0.16627177596092224\n",
      "Epoch 1970 of 2000\n",
      "Train Loss: 0.1662372648715973\n",
      "Epoch 1971 of 2000\n",
      "Train Loss: 0.16623184084892273\n",
      "Epoch 1972 of 2000\n",
      "Train Loss: 0.16615208983421326\n",
      "Epoch 1973 of 2000\n",
      "Train Loss: 0.1662050038576126\n",
      "Epoch 1974 of 2000\n",
      "Train Loss: 0.166266068816185\n",
      "Epoch 1975 of 2000\n",
      "Train Loss: 0.16628246009349823\n",
      "Epoch 1976 of 2000\n",
      "Train Loss: 0.1662483513355255\n",
      "Epoch 1977 of 2000\n",
      "Train Loss: 0.16614802181720734\n",
      "Epoch 1978 of 2000\n",
      "Train Loss: 0.1662011593580246\n",
      "Epoch 1979 of 2000\n",
      "Train Loss: 0.16627457737922668\n",
      "Epoch 1980 of 2000\n",
      "Train Loss: 0.1662287414073944\n",
      "Epoch 1981 of 2000\n",
      "Train Loss: 0.16617806255817413\n",
      "Epoch 1982 of 2000\n",
      "Train Loss: 0.16616138815879822\n",
      "Epoch 1983 of 2000\n",
      "Train Loss: 0.16620197892189026\n",
      "Epoch 1984 of 2000\n",
      "Train Loss: 0.1662716567516327\n",
      "Epoch 1985 of 2000\n",
      "Train Loss: 0.16627219319343567\n",
      "Epoch 1986 of 2000\n",
      "Train Loss: 0.16626863181591034\n",
      "Epoch 1987 of 2000\n",
      "Train Loss: 0.16624602675437927\n",
      "Epoch 1988 of 2000\n",
      "Train Loss: 0.1661839485168457\n",
      "Epoch 1989 of 2000\n",
      "Train Loss: 0.16614195704460144\n",
      "Epoch 1990 of 2000\n",
      "Train Loss: 0.16619466245174408\n",
      "Epoch 1991 of 2000\n",
      "Train Loss: 0.16628503799438477\n",
      "Epoch 1992 of 2000\n",
      "Train Loss: 0.16634565591812134\n",
      "Epoch 1993 of 2000\n",
      "Train Loss: 0.16639050841331482\n",
      "Epoch 1994 of 2000\n",
      "Train Loss: 0.16627278923988342\n",
      "Epoch 1995 of 2000\n",
      "Train Loss: 0.16626256704330444\n",
      "Epoch 1996 of 2000\n",
      "Train Loss: 0.16615250706672668\n",
      "Epoch 1997 of 2000\n",
      "Train Loss: 0.16619588434696198\n",
      "Epoch 1998 of 2000\n",
      "Train Loss: 0.1662011444568634\n",
      "Epoch 1999 of 2000\n",
      "Train Loss: 0.16619166731834412\n",
      "Epoch 2000 of 2000\n",
      "Train Loss: 0.16621267795562744\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train model\n",
    "INPUT_DIM = 20000\n",
    "EMBEDDING_DIM = 512\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 8\n",
    "N_LAYERS = 4\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.6\n",
    "LEARNING_RATE = 0.00075\n",
    "EPOCHS = 2000\n",
    "\n",
    "# #only use 10 datapoints from trainloader for testing\n",
    "# train_loader = torch.utils.data.DataLoader(train[:10], batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid[:10], batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "#only take the first 10 datapoints from trainloader for testing\n",
    "# train = train[:10]\n",
    "# train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "train_net(model, train_loader, valid_loader, optimizer, criterion, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 119, got 103",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m train_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n\u001b[1;32m     24\u001b[0m valid_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(valid_loader)\n\u001b[0;32m---> 25\u001b[0m train(model, optimizer, criterion, train_iter, epoch)\n\u001b[1;32m     26\u001b[0m train_f1, train_loss \u001b[39m=\u001b[39m evaluate(model, train_loader)\n\u001b[1;32m     27\u001b[0m valid_f1, valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_loader)\n",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m      6\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m      7\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m, in \u001b[0;36mRNNMultiLabelClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 10\u001b[0m     _, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(inputs)\n\u001b[1;32m     11\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     12\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msigmoid(out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:472\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    469\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    471\u001b[0m \u001b[39massert\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    473\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_RELU\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:234\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    235\u001b[0m     expected_hidden_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:210\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 119, got 103"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "600ea93296a0ae4f33cffb606658847eff10124d9f52ef16042deecb5dd32ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

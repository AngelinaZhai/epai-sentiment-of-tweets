{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchtext\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from langdetect import detect_langs\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pandarallel import pandarallel \n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  use_cuda = True\n",
    "else:  \n",
    "  use_cuda = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading GLOVE embeddings\n",
    "GLOVE = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=10000)  # use 10k most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ucberkeley-dlab--measuring-hate-speech-c32713cabe528196\n",
      "Found cached dataset parquet (/Users/angelinazhai/.cache/huggingface/datasets/ucberkeley-dlab___parquet/ucberkeley-dlab--measuring-hate-speech-c32713cabe528196/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b364470152648b3b2e674de2b392930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>infitms</th>\n",
       "      <th>outfitms</th>\n",
       "      <th>annotator_severity</th>\n",
       "      <th>std_err</th>\n",
       "      <th>annotator_infitms</th>\n",
       "      <th>annotator_outfitms</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>annotator_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.00000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23530.416138</td>\n",
       "      <td>5567.097812</td>\n",
       "      <td>1.281352</td>\n",
       "      <td>2.954307</td>\n",
       "      <td>2.828875</td>\n",
       "      <td>2.56331</td>\n",
       "      <td>2.278638</td>\n",
       "      <td>2.698575</td>\n",
       "      <td>1.846211</td>\n",
       "      <td>1.052045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744733</td>\n",
       "      <td>-0.567428</td>\n",
       "      <td>1.034322</td>\n",
       "      <td>1.001052</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>0.300588</td>\n",
       "      <td>1.007158</td>\n",
       "      <td>1.011841</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>37.910772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12387.194125</td>\n",
       "      <td>3230.508937</td>\n",
       "      <td>1.023542</td>\n",
       "      <td>1.231552</td>\n",
       "      <td>1.309548</td>\n",
       "      <td>1.38983</td>\n",
       "      <td>1.370876</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>1.402372</td>\n",
       "      <td>1.345706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932260</td>\n",
       "      <td>2.380003</td>\n",
       "      <td>0.496867</td>\n",
       "      <td>0.791943</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>0.236380</td>\n",
       "      <td>0.269876</td>\n",
       "      <td>0.675863</td>\n",
       "      <td>0.613006</td>\n",
       "      <td>11.641276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.340000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-1.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-1.578693</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18148.000000</td>\n",
       "      <td>2719.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.330000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-0.341008</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20052.000000</td>\n",
       "      <td>5602.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32038.250000</td>\n",
       "      <td>8363.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.449555</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50070.000000</td>\n",
       "      <td>11142.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          comment_id   annotator_id       platform      sentiment  \\\n",
       "count  135556.000000  135556.000000  135556.000000  135556.000000   \n",
       "mean    23530.416138    5567.097812       1.281352       2.954307   \n",
       "std     12387.194125    3230.508937       1.023542       1.231552   \n",
       "min         1.000000       1.000000       0.000000       0.000000   \n",
       "25%     18148.000000    2719.000000       0.000000       2.000000   \n",
       "50%     20052.000000    5602.500000       1.000000       3.000000   \n",
       "75%     32038.250000    8363.000000       2.000000       4.000000   \n",
       "max     50070.000000   11142.000000       3.000000       4.000000   \n",
       "\n",
       "             respect        insult      humiliate         status  \\\n",
       "count  135556.000000  135556.00000  135556.000000  135556.000000   \n",
       "mean        2.828875       2.56331       2.278638       2.698575   \n",
       "std         1.309548       1.38983       1.370876       0.898500   \n",
       "min         0.000000       0.00000       0.000000       0.000000   \n",
       "25%         2.000000       2.00000       1.000000       2.000000   \n",
       "50%         3.000000       3.00000       3.000000       3.000000   \n",
       "75%         4.000000       4.00000       3.000000       3.000000   \n",
       "max         4.000000       4.00000       4.000000       4.000000   \n",
       "\n",
       "          dehumanize       violence  ...     hatespeech  hate_speech_score  \\\n",
       "count  135556.000000  135556.000000  ...  135556.000000      135556.000000   \n",
       "mean        1.846211       1.052045  ...       0.744733          -0.567428   \n",
       "std         1.402372       1.345706  ...       0.932260           2.380003   \n",
       "min         0.000000       0.000000  ...       0.000000          -8.340000   \n",
       "25%         1.000000       0.000000  ...       0.000000          -2.330000   \n",
       "50%         2.000000       0.000000  ...       0.000000          -0.340000   \n",
       "75%         3.000000       2.000000  ...       2.000000           1.410000   \n",
       "max         4.000000       4.000000  ...       2.000000           6.300000   \n",
       "\n",
       "             infitms       outfitms  annotator_severity        std_err  \\\n",
       "count  135556.000000  135556.000000       135556.000000  135556.000000   \n",
       "mean        1.034322       1.001052           -0.018817       0.300588   \n",
       "std         0.496867       0.791943            0.487261       0.236380   \n",
       "min         0.100000       0.070000           -1.820000       0.020000   \n",
       "25%         0.710000       0.560000           -0.380000       0.030000   \n",
       "50%         0.960000       0.830000           -0.020000       0.340000   \n",
       "75%         1.300000       1.220000            0.350000       0.420000   \n",
       "max         5.900000       9.000000            1.360000       1.900000   \n",
       "\n",
       "       annotator_infitms  annotator_outfitms     hypothesis  annotator_age  \n",
       "count      135556.000000       135556.000000  135556.000000  135451.000000  \n",
       "mean            1.007158            1.011841       0.014589      37.910772  \n",
       "std             0.269876            0.675863       0.613006      11.641276  \n",
       "min             0.390000            0.280000      -1.578693      18.000000  \n",
       "25%             0.810000            0.670000      -0.341008      29.000000  \n",
       "50%             0.970000            0.850000       0.110405      35.000000  \n",
       "75%             1.170000            1.130000       0.449555      45.000000  \n",
       "max             2.010000            9.000000       0.987511      81.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "df = dataset['train'].to_pandas()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load numpy array from file\n",
    "tmp_np_arr = np.load('hate_speech.npy', allow_pickle=True)\n",
    "\n",
    "#convert to pandas dataframe\n",
    "df.drop(df.iloc[:, 15:131], inplace=True, axis=1)\n",
    "df_tmp = df.drop([\"annotator_id\"], axis=1)\n",
    "df_norm = pd.DataFrame(tmp_np_arr, columns=df_tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spliced = df_norm.drop('comment_id', axis=1)\n",
    "df_spliced = df_spliced.drop('platform', axis=1)\n",
    "df_spliced = df_spliced.drop('sentiment', axis=1)\n",
    "df_spliced = df_spliced.drop('hatespeech', axis=1)\n",
    "df_spliced = df_spliced.drop('hate_speech_score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_spliced.iloc[:,-1:]\n",
    "labels = df_spliced.iloc[:,:-1]\n",
    "labels = labels.to_numpy()\n",
    "label_names = list(df_spliced.iloc[:,:-1].columns)\n",
    "train_size = int(0.7*len(df_spliced))\n",
    "val_size = int((len(df_spliced) - train_size)/2)\n",
    "test_size = len(df_spliced) - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tweet(tweet):\n",
    "    # separate punctuations\n",
    "    tweet = tweet.replace(\".\", \" . \") \\\n",
    "                 .replace(\",\", \" , \") \\\n",
    "                 .replace(\";\", \" ; \") \\\n",
    "                 .replace(\"?\", \" ? \")\n",
    "    return tweet.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_words(glove_vector):\n",
    "    train, valid, test = [], [], []\n",
    "    for index, row in df_spliced.iterrows():\n",
    "        try:\n",
    "            tweet = row[-1]\n",
    "            idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
    "                    for w in split_tweet(tweet)\n",
    "                    if w in glove_vector.stoi] # keep words that has an embedding\n",
    "            if not idxs: # ignore tweets without any word with an embedding\n",
    "                continue\n",
    "            idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
    "            label = np.array(row[:-1].values).astype(np.float32) \n",
    "            label = torch.tensor(label) #storing label information to tensor\n",
    "            #adding tweet to corresponding train/val/test set\n",
    "            if index < train_size:\n",
    "                train.append((idxs, label))\n",
    "            elif index < train_size+val_size:\n",
    "                valid.append((idxs, label))\n",
    "            else:\n",
    "                test.append((idxs, label))\n",
    "        except:\n",
    "            print(\"Error at index: \", index)\n",
    "            continue\n",
    "    return train, valid, test\n",
    "\n",
    "train, valid, test = get_tweet_words(GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "  (xx, yy) = zip(*batch)\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  y_lens = [len(y) for y in yy]\n",
    "\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
    "\n",
    "  return xx_pad, yy_pad\n",
    "  \n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=128, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_labels):\n",
    "        super(RNNMultiLabelClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, hidden = self.rnn(inputs)\n",
    "        out = self.fc1(hidden.squeeze(0))\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "class Tweet_RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The class object for the RNN.\n",
    "    Attributes:\n",
    "    emb: the type of embedding\n",
    "    hidden_size: the number of layers\n",
    "    nn: the actual neural network\n",
    "    fc: the activation layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Tweet_RNN.__init__(self, input_size, hidden_size, num_classes)\n",
    "    # param: self:Tweet_RNN\n",
    "    # param: input_size:int\n",
    "    # param: hidden_size:int\n",
    "    # param: num_classes:int\n",
    "    # param: embedding:str\n",
    "    #    the string should be either: \"glove\", \"word2vec\" or \"none\"\n",
    "    #    and correspond to the desired embedding\n",
    "    # return: void\n",
    "    # initializes the RNN\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_classes: int, embedding: str) -> None:\n",
    "        super(Tweet_RNN, self).__init__()\n",
    "        self.name = 'Tweet_RNN'\n",
    "        if embedding == \"glove\":\n",
    "            self.emb = nn.Embedding.from_pretrained(GLOVE.vectors)\n",
    "        elif embedding == \"word2vec\":\n",
    "            self.emb = nn.Embedding.from_pretrained(WORD2VEC)\n",
    "        else:\n",
    "            self.emb = nn.Embedding(input_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        # self.linear = nn.Linear(input_size, 1);\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # forward(self, x)\n",
    "    # param: self:Tweet_RNN\n",
    "    # param: x:torch.FloatTensor\n",
    "    # initializes the RNN\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.nn(x, h0)\n",
    "\n",
    "        # Pass the output of hidden layer from the last time step to the classifier\n",
    "        out = self.sigmoid(self.linear(out[:, -1, :])) #sigmoid activiation is applyed for all 8 classes\n",
    "\n",
    "        return out #outputs a tensor of dimension 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            y_true += targets.cpu().tolist()\n",
    "            y_pred += (outputs >= 0.5).cpu().tolist()\n",
    "            total_loss += criterion(outputs, targets).item()\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return f1, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 119, got 88",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m train_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n\u001b[1;32m     23\u001b[0m valid_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(valid_loader)\n\u001b[0;32m---> 24\u001b[0m train(model, optimizer, criterion, train_iter, epoch)\n\u001b[1;32m     25\u001b[0m train_f1, train_loss \u001b[39m=\u001b[39m evaluate(model, train_loader)\n\u001b[1;32m     26\u001b[0m valid_f1, valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_loader)\n",
      "Cell \u001b[0;32mIn[53], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m      6\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m      7\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[52], line 10\u001b[0m, in \u001b[0;36mRNNMultiLabelClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 10\u001b[0m     _, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(inputs)\n\u001b[1;32m     11\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\n\u001b[1;32m     12\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msigmoid(out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:472\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    469\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    471\u001b[0m \u001b[39massert\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    473\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_RELU\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:234\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    235\u001b[0m     expected_hidden_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/epai/lib/python3.10/site-packages/torch/nn/modules/rnn.py:210\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 119, got 88"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 50  # Embedding dimension\n",
    "hidden_size = 64\n",
    "num_labels = 8  # Number of classes\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "\n",
    "# train_loader = # Your PyTorch train data loader here\n",
    "# valid_loader = # Your PyTorch validation data loader here\n",
    "\n",
    "#get train and validation data loaders\n",
    "#get first batch of data\n",
    "\n",
    "model = RNNMultiLabelClassifier(119, hidden_size, num_labels)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for multi-label classification\n",
    "\n",
    "train_losses, train_accs = [], []\n",
    "valid_losses, valid_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    valid_iter = iter(valid_loader)\n",
    "    train(model, optimizer, criterion, train_iter, epoch)\n",
    "    train_f1, train_loss = evaluate(model, train_loader)\n",
    "    valid_f1, valid_loss = evaluate(model, valid_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accs.append(train_f1)\n",
    "    valid_accs.append(valid_f1)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train F1={train_f1:.4f}, Valid Loss={valid_loss:.4f}, Valid F1={valid_f1:.4f}\")\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accs, label='Train F1')\n",
    "plt.plot(valid_accs, label='Valid F1')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "600ea93296a0ae4f33cffb606658847eff10124d9f52ef16042deecb5dd32ae8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
